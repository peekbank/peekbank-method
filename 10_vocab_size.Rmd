---
title: "Comparing vocab size and LWL performance"
author: "Hackathon"
date: "`r Sys.Date()`"
output: html_document
---

Goal is to compare child-level CDI vs. LWL performance... and maybe an item-level analysis, too?

```{r setup, message=F}
source(here::here("helper/common.R"))
# load(here::here("cached_intermediates","1_d_trial.Rds")) # ToDo: use Mike's new by-trial df..but where is it?
load(here::here("cached_intermediates","1_cdi_subjects.Rds"))
d_trial <- readRDS(here("..", "peekbank-development", "cached_intermediates", "1_d_trial.Rds"))
d_sub <- readRDS(here("..", "peekbank-development", "cached_intermediates", "1_d_sub.Rds"))

# get IRT item difficulties
en_prod_pars <- read_csv(here("aux_data","EN_production_2PL_params_slopeint.csv")) |> mutate(d = -d)
en_comp_pars <- read_csv(here("aux_data","EN_comprehension_2PL_params_slopeint.csv")) |> mutate(d = -d)
sp_prod_pars <- read_csv(here("aux_data","SP_production_2PL_params_slopeint.csv")) |> mutate(d = -d)
sp_comp_pars <- read_csv(here("aux_data","SP_comprehension_2PL_params_slopeint.csv")) |> mutate(d = -d)
theme_set(theme_bw())

# mismatched items (ToDo: correct distractor_label, too, if desired)
d_trial <- d_trial |> 
  mutate(target_cdi_label = case_when(target_label=="fish" ~ "fish (animal)",
                                      target_label=="chicken" ~ "chicken (animal)",
                                      target_label=="bike" ~ "bicycle",
                                      target_label=="carrot" ~ "carrots",
                                      target_label=="blocks" ~ "block",
                                      target_label=="slide" ~ "slide (object)",
                                      target_label=="glove" ~ "gloves",
                                      target_label=="dress" ~ "dress (object)",
                                      target_label=="can" ~ "can (object)",
                                      target_label=="boot" ~ "boots",
                                      target_label=="toy" ~ "toy (object)",
                                      .default = target_label)) |>
  mutate(distractor_cdi_label = case_when(distractor_label=="fish" ~ "fish (animal)",
                                      distractor_label=="chicken" ~ "chicken (animal)",
                                      distractor_label=="bike" ~ "bicycle",
                                      distractor_label=="carrot" ~ "carrots",
                                      distractor_label=="blocks" ~ "block",
                                      distractor_label=="slide" ~ "slide (object)",
                                      distractor_label=="glove" ~ "gloves",
                                      distractor_label=="dress" ~ "dress (object)",
                                      distractor_label=="can" ~ "can (object)",
                                      distractor_label=="boot" ~ "boots",
                                      distractor_label=="toy" ~ "toy (object)",
                                      .default = distractor_label))

```



```{r}
trial_acc <- d_trial |>
  left_join(en_prod_pars |> select(definition, d), by=c("target_cdi_label"="definition")) |> # FixMe: should use Spanish and comprehension params in relevant datasets...
  rename(target_label_difficulty = d) |>
  left_join(en_prod_pars |> select(definition, d), by=c("distractor_cdi_label"="definition")) |>
  rename(distractor_label_difficulty = d) |>
  mutate(target_weighted_acc = target_label_difficulty * long_window_accuracy,
         weighted_acc = (target_label_difficulty+distractor_label_difficulty) * long_window_accuracy) # harder words more valuable to know
  # ToDo: what to do with distractor label? a trial is easy if you know the target label, OR if you know the distractor label (and use ME)

cor.test(trial_acc$long_window_accuracy, trial_acc$target_label_difficulty) # .06
cor.test(trial_acc$long_window_accuracy, trial_acc$distractor_label_difficulty) # .06
cor.test(trial_acc$target_label_difficulty, trial_acc$distractor_label_difficulty) # .76 <- target and distractor have very similar difficulties

# administration-level accuracy
child_acc <- trial_acc |>
  group_by(dataset_name, subject_id, administration_id, age) |>
  summarize(LWL_accuracy = mean(long_window_accuracy),
            LWL_weighted_acc = mean(weighted_acc),
            LWL_target_weighted_acc = mean(target_weighted_acc),
            mean_target_difficulty = mean(target_label_difficulty),
            sd = sd(long_window_accuracy),
            n = n())
```

Experiments have somewhat different average target label difficulties (and varying overall LWL performance)

```{r}
diff_by_exp <- child_acc |> 
  group_by(dataset_name) |>
  summarise(LWL_accuracy = mean(LWL_accuracy),
            LWL_weighted_acc = mean(LWL_weighted_acc),
            LWL_target_weighted_acc = mean(LWL_target_weighted_acc),
            mean_target_difficulty = mean(mean_target_difficulty),
            mean_age = mean(age),
            n=n())
#cor.test(diff_by_exp$LWL_accuracy, diff_by_exp$mean_target_difficulty)
diff_by_exp |>
  ggplot(aes(x=mean_target_difficulty, y=LWL_weighted_acc, color=mean_age)) +
  geom_point() +
  geom_label_repel(aes(label=dataset_name))
```


IRT Target Label Difficulty vs. Item-level LWL Accuracy by Dataset



```{r}
d <- trial_acc |> 
  group_by(subject_id, age, target_cdi_label, target_label_difficulty) |>
  summarise(LWL_accuracy = mean(long_window_accuracy), 
            sd = sd(long_window_accuracy),
            n = n())

m1short <- lmerTest::lmer(data=trial_acc, short_window_accuracy ~ age * target_label_difficulty * distractor_label_difficulty + 
                       (1|administration_id) + (1|target_cdi_label) + (1|distractor_cdi_label))
summary(m1short)
#sjPlot::r2(m1short)

# different effects
m1long <- lmerTest::lmer(data=trial_acc, long_window_accuracy ~ age * target_label_difficulty * distractor_label_difficulty + 
                       (1|administration_id) + (1|target_cdi_label) + (1|distractor_cdi_label))
summary(m1long)
```

## Examine LWL target difficulties

```{r}
# target and distractor difficulties are strongly related, so let's just average them and label with target

# by-item
by_item <- trial_acc |>
  mutate(mean_difficulty = (target_label_difficulty + distractor_label_difficulty) / 2) |>
  group_by(target_cdi_label) |>
  summarise(target_label_difficulty = first(target_label_difficulty),
            mean_age = mean(age),
            LWL_accuracy = mean(short_window_accuracy, na.rm=T), # why some NAs here?
            n = n()) |>
  arrange(desc(n)) |>
  ungroup()

by_item |> filter(n>4) |>
  ggplot(aes(x=target_label_difficulty, y=LWL_accuracy, color=mean_age)) +
  geom_point() +
  geom_label_repel(aes(label=target_cdi_label)) +
  geom_smooth()
```

# Examine by-subject CDI vs LWL accuracy

```{r by-subject}
# by-subject
by_subj <- trial_acc |>
  # left_join(cdi_data) |> # many-to-many
  group_by(administration_id, age) |>
  summarise(target_label_difficulty = mean(target_label_difficulty),
            distractor_label_difficulty = mean(distractor_label_difficulty),
            LWL_accuracy = mean(short_window_accuracy, na.rm=T), 
            CDI_percent = first(CDI_percent),
            n = n()) |>
  ungroup()


by_subj |> filter(n>4) |>
  select(-administration_id, -n) |>
  GGally::ggpairs()
```

## CDI data

We have CDI data from `r length(unique(cdi_data$subject_id))` participants in `r length(unique(cdi_data$dataset_name))` datasets, comprising a mix of languages, instrument types (WG, WS, WS short), and measurement types (comprehension and production).
Some datasets have percentiles, but for many subjects we just have raw sumscores. 

```{r}
table(cdi_data$dataset_name)
```

```{r}
table(cdi_data$measure, cdi_data$instrument_type)
```

`baumgartner_2014` has comprehension measured with WS short? (asking Heidi)

```{r}
cdi_data |>
  ggplot(aes(x=age, y=rawscore, color=language)) +
  facet_wrap(vars(instrument_type)) +
  geom_point(alpha=.5)
```


```{r}
cdi_acc <- cdi_data |> 
  left_join(child_acc)

cdi_acc |> filter(measure=="prod", 
                  n > 4) |>
  ggplot(aes(x=CDI_percent, y=LWL_accuracy, color=age)) + # , color=dataset_name
  facet_wrap(. ~ instrument_type) + 
  geom_point(alpha=.3) +
  geom_smooth()
```

IRT-difficulty weighted accuracy doesn't work great...

```{r}
cor.test(cdi_acc$LWL_weighted_acc, cdi_acc$CDI_percent)
cor.test(cdi_acc$LWL_target_weighted_acc, cdi_acc$CDI_percent)
```


## Overall correlations of CDI percent scores and LWL accuracy

```{r}
with(cdi_acc |> filter(measure=="prod"),
  cor.test(CDI_percent, LWL_accuracy))

with(cdi_acc |> filter(measure=="comp"),
  cor.test(CDI_percent, LWL_accuracy))
```

## Correlation of CDI sumscores by instrument (vs. LWL accuracy)

```{r}
with(cdi_acc |> filter(instrument_type=="wsshort"),
  cor.test(rawscore, LWL_accuracy))

with(cdi_acc |> filter(instrument_type=="ws"),
  cor.test(rawscore, LWL_accuracy))

with(cdi_acc |> filter(instrument_type=="wg" & measure=="comp"),
  cor.test(rawscore, LWL_accuracy))

with(cdi_acc |> filter(instrument_type=="wg" & measure=="prod"),
  cor.test(rawscore, LWL_accuracy))
```

Overall significant association between CDI and LWL score!

## Extensions

- use IRT difficulty of each word to get a weighted accuracy LWL score
- Growth model to model children over age 


## Regression

use age, LWL RT

