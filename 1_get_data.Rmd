---
title: "Trial analysis 1: Data loading"
author: "Mike"
date: "2/19/2021, updated 8/27/24"
output: 
  html_document:
    toc: true
    toc_depth: 2
    toc_float: true
    code_folding: hide

---

```{r setup, echo = FALSE}
suppressPackageStartupMessages(library(here))
suppressPackageStartupMessages(library(tidyverse))
suppressPackageStartupMessages(library(peekbankr))
suppressPackageStartupMessages(library(lme4))
# suppressPackageStartupMessages(library(ggpmisc))
suppressPackageStartupMessages(library(ggrepel))
suppressPackageStartupMessages(library(ggthemes))

# Seed for random number generation
set.seed(42)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed, cache = TRUE, 
                      message=FALSE, warning=FALSE, error=FALSE)

```


This markdown documents a new way of thinking about modeling variation in LWL data. The idea is to try to:

1. extract a summary statistic for each trial
2. model these summaries with LMMs of various types

The trouble is, what's the right summary statistic? It might be that there's not just one! But let's assume there is one and we just need to sum it up right. 

So we'll start by trying to figure out what the best measure is. We're going to make decisions to maximize within-experiment reliability via ICCs.

We're focused on familiar words here. 

## Get data

```{r, eval=TRUE}
con <- connect_to_peekbank(db_version = "workshop_2024_dev")
all_aoi_timepoints <- get_aoi_timepoints(connection = con, rle=FALSE)

# reload connection in case it is stale
con <- connect_to_peekbank(db_version = "workshop_2024_dev")
all_stimuli <- collect(get_stimuli(connection = con))
all_administrations <- collect(get_administrations(connection = con))
all_subjects <- collect(get_subjects(connection = con))
all_trial_types <- collect(get_trial_types(connection = con))
all_trials <- collect(get_trials(connection = con))
```

Now do the joins. 

```{r}
aoi_data_joined <- all_aoi_timepoints |>
  right_join(all_administrations) |>
  right_join(all_subjects) |>
  right_join(all_trials) |>
  right_join(all_trial_types) |>
  mutate(stimulus_id = target_id) |>
  left_join(all_stimuli) |>
  select(dataset_name, subject_id, administration_id, trial_id, trial_order, dataset_id, 
         stimulus_id, distractor_id, t_norm, age, aoi, english_stimulus_label, 
         stimulus_novelty, target_side, vanilla_trial) %>%
  rename(target_label = english_stimulus_label, 
         target_id = stimulus_id) %>%
  left_join(all_stimuli %>%
              select(stimulus_id, dataset_id, 
                     stimulus_novelty, english_stimulus_label) %>%
              rename(distractor_id = stimulus_id, 
                     distractor_novelty = stimulus_novelty,
                     distractor_label = english_stimulus_label))

save(aoi_data_joined, file= here("cached_intermediates","1_aoi_data_joined.Rds"))
```

Check on number of datasets. 

```{r}
load(here("cached_intermediates", "1_aoi_data_joined.Rds"))
length(unique(aoi_data_joined$dataset_name))
unique(aoi_data_joined$dataset_name)
```

Age distribution. 

```{r}
ggplot(aoi_data_joined, aes(x = age, fill = dataset_name)) + 
  geom_histogram() 
```

We now have a `vanilla_trial` flag that we can use for filtering. 


Take only English familiar word data and also remove datasets that aren't ready for primetime.

```{r}
d_joined <- aoi_data_joined |>
  filter(!(dataset_name %in% c("casillas_tseltal_2015", 
                               "byers-heinlein_2017", 
                               "hurtado_2008", 
                               "kartushina_2019",
                               "weisleder_stl",
                               "xsectional_2007",
                               "garrison_bergelson_2020", 
                               "moore_bergelson_2022_verb"))) |>
  filter(age <= 60, 
         vanilla_trial == TRUE,
         stimulus_novelty == "familiar",
         distractor_novelty == "familiar") |> 
  select(dataset_name, subject_id, administration_id, trial_id, trial_order,
         dataset_id, target_id, t_norm, age, aoi, target_label, distractor_label, target_side) |>
  mutate(correct = ifelse(aoi == "target", 1, 
                          ifelse(aoi == "distractor", 0, NA)), 
         target_label = str_to_lower(target_label),
         distractor_label = str_to_lower(distractor_label))
```

# Item cleanup

Do some further checks/cleanup of items. 

Output targets and distractors. 

```{r}
all_items <- tibble(item = sort(unique(c(d_joined$target_label, d_joined$distractor_label))))
write_csv(all_items, here("metadata", "all_items.csv"))
```

Rename and exclude in a more general way. 

```{r}
included_items <- read_csv(here("metadata","included_items.csv"))

d_aoi <- d_joined |>
  left_join(included_items, by = join_by(target_label == item)) |>
  left_join(included_items, by = join_by(distractor_label == item), 
            suffix = c("_target","_distractor")) |>
  filter(include_target == 1, include_distractor == 1) |>
  mutate(target_label = ifelse(is.na(rename_to_target), 
                               target_label, rename_to_target), 
         distractor_label = ifelse(is.na(rename_to_distractor), 
                                   distractor_label, rename_to_distractor)) |>
  select(-include_target, -include_distractor, -rename_to_distractor, -rename_to_target)
```

# Timecourse cleanup

So datasets vary in missingness of data at the trial level because of:

1. internal exclusion decisions (e.g., fmw_2013, which excluded trials <50%)
2. amount of data given (adams_marchman_2018, which didn't provide times before -500ms)
3. padding of trials with NAs (e.g., pomper_saffran_2016, which has some padding later in the trials). 

```{r}
d_missing <- d_aoi |>
  group_by(dataset_name, t_norm) |>
  summarise(prop_data = mean(!is.na(correct)), 
            n = n()) 

ggplot(d_missing,
       aes(x = t_norm, y = prop_data)) + 
  facet_wrap(~dataset_name) + 
  ylab("Proportion trials that are not missing") + 
  geom_line() + 
  geom_vline(xintercept = -4000, lty = 3) + 
  geom_vline(xintercept = 4000, lty = 3)
```

Note - there is a bit of a ramp down in terms of amount of data for many datasets towards the end. Here we decide to keep this for now, but maybe we want to clean up starts and ends at some point per dataset? 


```{r}
d_missing_clipped <- d_missing |>
  filter(t_norm >= -4000, t_norm <= 4000)

ggplot(d_missing_clipped,
       aes(x = t_norm, y = prop_data)) + 
  facet_wrap(~dataset_name) + 
  ylab("Proportion trials that are not missing") + 
  geom_line() + 
  geom_hline(yintercept = .5, lty = 2)
```

Previously we filtered by hand, but for several of our analyses, we want all the data. 

```{r}
d_aoi <- d_aoi |>
  filter(t_norm >= -4000, t_norm <= 4000)

save(d_aoi, file = here("cached_intermediates","1_d_aoi.Rds"))
```

