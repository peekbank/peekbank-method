---
title: "Trial analysis 1: Data loading"
author: "Mike"
date: "2/19/2021"
output: 
  html_document:
    toc: true
    toc_depth: 2
    toc_float: true
    code_folding: hide

---

```{r setup, echo = FALSE}
suppressPackageStartupMessages(library(here))
suppressPackageStartupMessages(library(tidyverse))
suppressPackageStartupMessages(library(peekbankr))
suppressPackageStartupMessages(library(lme4))
# suppressPackageStartupMessages(library(ggpmisc))
suppressPackageStartupMessages(library(ggrepel))
suppressPackageStartupMessages(library(ggthemes))

# Seed for random number generation
set.seed(42)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed, cache = TRUE, 
                      message=FALSE, warning=FALSE, error=FALSE)

```


This markdown documents a new way of thinking about modeling variation in LWL data. The idea is to try to:

1. extract a summary statistic for each trial
2. model these summaries with LMMs of various types

The trouble is, what's the right summary statistic? It might be that there's not just one! But let's assume there is one and we just need to sum it up right. 

So we'll start by trying to figure out what the best measure is. We're going to make decisions to maximize within-experiment reliability via ICCs.

We're focused on familiar words here. 

## Get data

```{r, eval=TRUE}
con <- connect_to_peekbank(db_version = "workshop_2024_dev")
all_aoi_timepoints <- get_aoi_timepoints(connection = con, rle=FALSE)

# reload connection in case it is stale
con <- connect_to_peekbank(db_version = "workshop_2024_dev")
all_stimuli <- collect(get_stimuli(connection = con))
all_administrations <- collect(get_administrations(connection = con))
all_subjects <- collect(get_subjects(connection = con))
all_trial_types <- collect(get_trial_types(connection = con))
all_trials <- collect(get_trials(connection = con))
```

Now do the joins. 

```{r}
aoi_data_joined <- all_aoi_timepoints |>
  right_join(all_administrations) |>
  right_join(all_subjects) |>
  right_join(all_trials) |>
  right_join(all_trial_types) |>
  mutate(stimulus_id = target_id) |>
  right_join(all_stimuli) |>
  select(dataset_name, subject_id, administration_id, trial_id, trial_order, dataset_id, 
         stimulus_id, distractor_id, t_norm, age, aoi, english_stimulus_label, 
         stimulus_novelty, target_side, vanilla_trial) %>%
  rename(target_label = english_stimulus_label, 
         target_id = stimulus_id) %>%
  left_join(all_stimuli %>%
              select(stimulus_id, dataset_id, 
                     stimulus_novelty, english_stimulus_label) %>%
              rename(distractor_id = stimulus_id, 
                     distractor_novelty = stimulus_novelty,
                     distractor_label = english_stimulus_label))

save(aoi_data_joined, file= here("cached_intermediates","1_aoi_data_joined.Rds"))
```


Looks like a very small number of rows get added in the join, probably a sign of an issue/ ambiguity somewhere. These are due to NA subject IDs, so we don't figure it out and just filter. 

```{r}
aoi_data_joined <- filter(aoi_data_joined, !is.na(subject_id))
```

Check on number of datasets. 

```{r}
load(here("cached_intermediates", "1_aoi_data_joined.Rds"))
length(unique(aoi_data_joined$dataset_name))
unique(aoi_data_joined$dataset_name)
```

We now have a `vanilla_trial` flag that we can use for filtering. 


Take only English familiar word data and also remove datasets that aren't ready for primetime.

```{r}
d_trial <- aoi_data_joined |>
  filter(!(dataset_name %in% c("casillas_tseltal_2015", 
                               "byers-heinlein_2017", 
                               "hurtado_2008", 
                               "kartushina_2019",
                               "weisleder_stl",
                               "xsectional_2007",
                               "pomper_dimy"))) |>
  filter(age > 12, age <= 60, 
         stimulus_novelty == "familiar",
         distractor_novelty == "familiar") |> 
  select(dataset_name, subject_id, administration_id, trial_id, trial_order,
         dataset_id, target_id, t_norm, age, aoi, target_label, distractor_label, target_side) |>
  mutate(correct = ifelse(aoi == "target", 1, 
                          ifelse(aoi == "distractor", 0, NA)))
```

Do some further checks/cleanup of items. 

```{r}
sort(unique(d_trial$target_label))
```


```{r}
colors <- c("red","orange","yellow","green","blue","purple","brown","white","black","pink")
d_trial <- filter(d_trial, !(target_label %in% 
                     c(colors, "baba", "cur","gall","kier","mog", "opal",
                       "raby","tog","vaby","yitty", "shawl",
                       "\'file\' must be specified", "opple","pity")))

d_trial$target_label[d_trial$target_label %in% c("birdy","birdie")] <- "bird"
d_trial$target_label[d_trial$target_label %in% c("blocks")] <- "block"
d_trial$target_label[d_trial$target_label %in% c("carrots")] <- "carrot"
d_trial$target_label[d_trial$target_label %in% c("diapey")] <- "diaper"
d_trial$target_label[d_trial$target_label %in% c("doggy")] <- "dog"
d_trial$target_label[d_trial$target_label %in% c("kitty","kittycat")] <- "cat"
d_trial$target_label[d_trial$target_label %in% c("paci")] <- "pacifier"
d_trial$target_label[d_trial$target_label %in% c("shoes")] <- "shoe"
d_trial$target_label[d_trial$target_label %in% c("sippycup")] <- "sippy"
d_trial$target_label[d_trial$target_label %in% c("socks")] <- "sock"
```


```{r}
sort(unique(d_trial$distractor_label))

```


```{r}
d_trial <- filter(d_trial, !(distractor_label %in% 
                     c(colors, "baba", "cur","gall","kier","mog", "opal",
                       "raby","tog","vaby","yitty", "shawl",
                       "\'file\' must be specified", "opple","pity")))


d_trial$distractor_label[d_trial$distractor_label %in% c("birdy","birdie")] <- "bird"
d_trial$distractor_label[d_trial$distractor_label %in% c("blocks")] <- "block"
d_trial$distractor_label[d_trial$distractor_label %in% c("carrots")] <- "carrot"
d_trial$distractor_label[d_trial$distractor_label %in% c("diapey")] <- "diaper"
d_trial$distractor_label[d_trial$distractor_label %in% c("doggy")] <- "dog"
d_trial$distractor_label[d_trial$distractor_label %in% c("kitty","kittycat")] <- "cat"
d_trial$distractor_label[d_trial$distractor_label %in% c("paci")] <- "pacifier"
d_trial$distractor_label[d_trial$distractor_label %in% c("shoes")] <- "shoe"
d_trial$distractor_label[d_trial$distractor_label %in% c("sippycup")] <- "sippy"
d_trial$distractor_label[d_trial$distractor_label %in% c("socks")] <- "sock"
```


# Get rid of useless timecourse info


So datasets vary in missingness of data at the trial level because of:

1. internal exclusion decisions (e.g., fmw_2013, which excluded trials <50%)
2. amount of data given (adams_marchman_2018, which didn't provide times before -500ms)
3. padding of trials with NAs (e.g., pomper_saffran_2016, which has some padding later in the trials). 

```{r}
d_missing <- d_trial |>
  group_by(dataset_name, t_norm) |>
  summarise(prop_data = mean(!is.na(correct)), 
            n = n()) 

ggplot(d_missing,
       aes(x = t_norm, y = prop_data)) + 
  facet_wrap(~dataset_name) + 
  ylab("Proportion trials that are not missing") + 
  geom_line()
```

```{r}
d_missing_clipped <- d_missing |>
  filter(t_norm >= -4000, t_norm <= 4000, 
         !(dataset_name == "adams_marchman_2018" & t_norm > 3750),
         !(dataset_name == "fmw_2013" & (t_norm < 500)),
         !(dataset_name == "potter-canine" & (t_norm > 3500)))

ggplot(d_missing_clipped,
       aes(x = t_norm, y = prop_data)) + 
  facet_wrap(~dataset_name) + 
  ylab("Proportion trials that are not missing") + 
  geom_line()
```

Previously we filtered by hand, but for several of our analyses, we want all the data. 

```{r}
d_trial <- d_trial |>
  filter(t_norm >= -4000, t_norm <= 4000)

         # !(dataset_name == "adams_marchman_2018" & t_norm > 3750),
         # !(dataset_name == "fmw_2013" & (t_norm < 500)),
         # !(dataset_name == "potter-canine" & (t_norm > 3500))
save(d_trial, file = here("cached_intermediates","1_d_trial.Rds"))
```

