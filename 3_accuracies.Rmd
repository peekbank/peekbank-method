---
title: "Accuracies"
author: "Mike Frank"
date: "2022-1-19 updated 2025-04-16"
output: html_document
---

```{r}
source(here::here("helper/common.R"))

d_aoi <- readRDS(here("cached_intermediates", "1_d_aoi.Rds"))
t_increment <- 200
```
# Cached icc runs
```{r}
icc_window_sim <- function(t_start = -500, t_end = 4000, object) {
  print(paste(t_start, t_end))

  df <- d_aoi |>
    filter(t_norm > t_start, t_norm < t_end) |>
    group_by(dataset_name, dataset_id, administration_id, target_label, trial_id) |>
    summarise(
      accuracy = mean(correct, na.rm = TRUE),
      prop_data = mean(!is.na(correct))
    ) |>
    group_by(dataset_name, dataset_id, administration_id, target_label) |>
    mutate(repetition = row_number())

  # compute ICCs
  df |>
    group_by(dataset_name) |>
    nest() |>
    mutate(icc = unlist(map(data, ~ get_icc(., "accuracy", object)))) |>
    select(-data) |>
    unnest(cols = c())
}
```

```{r}
acc_params <- expand_grid(
  t_start = seq(-1000, 1500, t_increment),
  t_end = seq(2000, 4000, t_increment),
  object = c("administration")
)

# multidyplr attempt
cluster <- new_cluster(14)
cluster_library(cluster, "tidyverse")
cluster_library(cluster, "agreement")
cluster_copy(cluster, "icc_window_sim")
cluster_copy(cluster, "get_icc")
cluster_copy(cluster, "d_aoi")

tic()
accs <- acc_params |>
  # partition(cluster) |>
  mutate(icc = pmap(list(t_start, t_end, object), icc_window_sim)) |>
  # collect() |>
  unnest(col = icc)
toc()

saveRDS(accs, here("cached_intermediates", "3_accs.Rds"))
```

# Reliability

These simulations use ICCs as a way to understand how we summarize accuracy data. In particular, we're going to look at how ICCs change as a function of window size. 

```{r}
accs <- readRDS(here("cached_intermediates/3_accs.Rds"))
```

Looks like for stimulus and administration you get consistent but modest gains if you take the longest window. BUT for stimuli, the early part of the trial adds reliability (probably because of bias due to stimulus-level preferences?). In contrast, for administrations, the early part of the trial is less informative. 500ms seems like a pretty good compromise. 

Current thinking is that we care about administration-level icc (per kid reliability) much more than per-item reliability. 

## Visualizations

Summary data frame. 

```{r}
accs_summary <- accs |>
  filter(object == "administration") |>
  group_by(t_start, t_end, object) |>
  summarize(
    N = n(),
    mean_icc = mean(icc, na.rm = TRUE)
  ) |>
  mutate(window_size = t_end - t_start)
```

Descriptive heatmap. 


```{r}
ggplot(accs_summary, aes(x = t_start, y = t_end, fill = mean_icc)) +
  geom_tile(color = "white") +
  scale_fill_viridis(name = "Mean ICC", option = "inferno") +
  scale_x_continuous(breaks = c(-1000, -500, 0, 500, 1000, 1500)) +
  xlab("Window Start Time (in ms)") +
  ylab("Window End Time (in ms)")
```

Window-by-window. 


```{r}
ggplot(
  accs_summary,
  aes(color = mean_icc)
) +
  geom_vline(xintercept = 0, linetype = "dashed") +
  geom_segment(aes(x = t_start, xend = t_end, y = mean_icc, yend = mean_icc)) +
  geom_segment(aes(x = t_start, xend = t_start, y = mean_icc - 0.005, yend = mean_icc + 0.005)) +
  geom_segment(aes(x = t_end, xend = t_end, y = mean_icc - 0.005, yend = mean_icc + 0.005)) +
  scale_color_viridis(name = "Mean ICC", direction = -1) +
  theme(legend.position = "none") +
  ylab("Mean ICC") +
  xlab("Analysis Window (in ms)")
```



# Window size by age

Let's do one more simulation where we check if this result holds across two ages. We'll break down age into > 24 months and < 24 months, which roughly splits the dataset. There are `r  length(unique(d_aoi$administration_id[d_aoi$age < 24]))` younger kids and `r length(unique(d_aoi$administration_id[d_aoi$age >= 24]))` older kids. 


```{r}
icc_window_sim_age <- function(t_start = -1000, t_end = 4000, object) {
  df <- d_aoi |>
    mutate(younger = age < 24) |>
    filter(t_norm > t_start, t_norm < t_end) |>
    group_by(
      dataset_name, dataset_id, younger, administration_id,
      target_label, trial_id
    ) |>
    summarise(
      accuracy = mean(correct, na.rm = TRUE),
      prop_data = mean(!is.na(correct))
    )

  # compute ICCs
  df |>
    group_by(dataset_name, younger) |>
    nest() |>
    mutate(icc = unlist(map(data, ~ get_icc(., "accuracy", object)))) |>
    select(-data) |>
    unnest(cols = c())
}

t_increment <- 200

acc_params <- expand_grid(
  t_start = seq(-1000, 1500, t_increment),
  t_end = seq(2000, 4000, t_increment),
  object = c("administration")
)

library(multidplyr)
cluster <- new_cluster(16)
cluster_library(cluster, "dplyr")
cluster_library(cluster, "tidyr")
cluster_library(cluster, "purrr")
cluster_library(cluster, "agreement")
cluster_copy(cluster, "get_icc")
cluster_copy(cluster, "d_aoi")
cluster_copy(cluster, "icc_window_sim_age")

accs_byage <- acc_params |>
  partition(cluster) |>
  mutate(icc = pmap(list(t_start, t_end, object), icc_window_sim_age)) |>
  collect() |>
  unnest(col = icc)

saveRDS(accs_byage, here("cached_intermediates", "3_accs_byage.rds"))
```

## Visualizations

Now plot. 

```{r}
accs_byage <- readRDS(here("cached_intermediates", "3_accs_byage.rds"))
accs_byage_summary <- accs_byage |>
  group_by(younger, t_start, t_end, object) |>
  summarize(
    N = n(),
    mean_icc = mean(icc, na.rm = TRUE)
  ) |>
  mutate(window_size = t_end - t_start) |>
  mutate(age = ifelse(younger, ">=24 months", "<24 months"))
```


Here we see that the younger kids lose more reliability when the window is short, but otherwise the conclusions remain unchanged. 

```{r}
ggplot(
  accs_byage_summary,
  aes(color = mean_icc)
) +
  geom_vline(xintercept = 0, linetype = "dashed") +
  geom_segment(aes(x = t_start, xend = t_end, y = mean_icc, yend = mean_icc)) +
  geom_segment(aes(x = t_start, xend = t_start, y = mean_icc - 0.005, yend = mean_icc + 0.005)) +
  geom_segment(aes(x = t_end, xend = t_end, y = mean_icc - 0.005, yend = mean_icc + 0.005)) +
  scale_color_viridis(name = "Mean ICC", direction = -1) +
  theme(legend.position = "none") +
  ylab("Mean ICC") +
  xlab("Analysis Window (in ms)") +
  facet_grid(age ~ object)

ggplot(accs_byage_summary, aes(x = t_start, y = t_end, fill = mean_icc)) +
  geom_tile(color = "white") +
  scale_fill_viridis(name = "Mean ICC", option = "inferno") +
  scale_x_continuous(breaks = c(-1000, -500, 0, 500, 1000, 1500)) +
  xlab("Window Start Time (in ms)") +
  ylab("Window End Time (in ms)") +
  facet_wrap(~age)
```
# Fine grained reliability

```{r}
icc_window_sim <- function(t_start = -500, t_end = 4000, object) {
  print(paste(t_start, t_end))

  df <- d_aoi |>
    filter(t_norm > t_start, t_norm < t_end) |>
    group_by(dataset_name, dataset_id, administration_id, target_label, trial_id) |>
    summarise(
      accuracy = mean(correct, na.rm = TRUE),
      prop_data = mean(!is.na(correct))
    ) |>
    group_by(dataset_name, dataset_id, administration_id, target_label) |>
    mutate(repetition = row_number())

  # compute ICCs
  df |>
    group_by(dataset_name) |>
    nest() |>
    mutate(icc = unlist(map(data, ~ get_icc(., "accuracy", object)))) |>
    select(-data) |>
    unnest(cols = c())
}
acc_params <- expand_grid(
  t_start = c(200, 250, 300, 350, 400, 450, 500, 550, 600),
  t_end = 4000,
  object = c("administration")
)
library(multidplyr)
# multidyplr attempt
cluster <- new_cluster(14)
cluster_library(cluster, "dplyr")
cluster_library(cluster, "tidyr")
cluster_library(cluster, "purrr")
cluster_library(cluster, "agreement")
cluster_copy(cluster, "icc_window_sim")
cluster_copy(cluster, "get_icc")
cluster_copy(cluster, "d_aoi")


accs <- acc_params |>
  partition(cluster) |>
  mutate(icc = pmap(list(t_start, t_end, object), icc_window_sim)) |>
  collect() |>
  unnest(col = icc)


saveRDS(accs, here("cached_intermediates", "3_acc_icc_finegrain.rds"))
```


## Visualizations

Now plot. 

Not sure what was wrong with reflook and yurovsky?

```{r}
accs_finegrain <- readRDS(here("cached_intermediates", "3_acc_icc_finegrain.rds"))



accs_finegrain |> ggplot(aes(x = t_start, y = icc)) +
  geom_point(aes(col = dataset_name)) +
  stat_summary()

accs_finegrain |> ggplot(aes(x = t_start, y = icc)) +
  geom_point(aes(col = dataset_name)) +
  facet_wrap(~dataset_name) +
  theme(legend.position = "none")
```
trends vary and we can't really tell!


# Validity via MB-CDI

We have MB-CDI WS data for a number of datasets. 


```{r}
cdi_data <- readRDS(here("cached_intermediates", "1_cdi_subjects.Rds"))
```

Now let's get our sim function. 

```{r}
cdi_sim <- function(t_start = -500, t_end = 4000) {
  # first average by trial then by subject
  by_trial_accuracies <- d_aoi |>
    filter(t_norm >= t_start, t_norm <= t_end) |>
    group_by(dataset_name, subject_id, administration_id, trial_id) |>
    summarize(correct = mean(correct, na.rm = TRUE))

  by_subject_accuracies <- by_trial_accuracies |>
    group_by(dataset_name, subject_id, administration_id) |>
    summarize(mean_correct = mean(correct, na.rm = TRUE)) |>
    left_join(cdi_data) |>
    filter(!(is.na(prod) & is.na(comp)) & !is.na(mean_correct))

  by_subject_accuracies |>
    group_by(dataset_name) |>
    summarise(
      cor_comp = ifelse(sum(!is.na(comp)) > 0, cor.test(mean_correct, comp)$estimate, NA),
      cor_prod = ifelse(sum(!is.na(prod)) > 0, cor.test(mean_correct, prod)$estimate, NA),
      n_comp = sum(!is.na(comp)),
      n_prod = sum(!is.na(prod)),
      n_total = sum(!is.na(mean_correct))
    )
}
```

```{r}
cdi_params <- expand_grid(
  t_start = seq(-1000, 1500, 250),
  t_end = seq(2000, 4000, 250)
)

cluster_copy(cluster, "cdi_sim")
cluster_copy(cluster, "cdi_data")

tic()
cdi_corrs <- cdi_params |>
  partition(cluster) |>
  mutate(icc = pmap(list(t_start, t_end), cdi_sim)) |>
  collect() |>
  unnest(col = icc)
toc()

saveRDS(cdi_corrs, here("cached_intermediates", "3_cdi_corrs.Rds"))
```

```{r}
cdi_corrs <- readRDS(here("cached_intermediates", "3_cdi_corrs.Rds"))


cdi_corrs_long <- cdi_corrs |>
  pivot_longer(names_to = "measure", values_to = "r", starts_with("cor"))
```


Visualize!


```{r}
ggplot(
  filter(cdi_corrs_long, measure == "cor_prod", n_prod > 50),
  aes(col = r)
) +
  geom_vline(xintercept = 0, linetype = "dashed") +
  geom_hline(yintercept = 0.05, lty = 3) +
  geom_segment(aes(x = t_start, xend = t_end, y = r, yend = r)) +
  scale_color_viridis(name = "Mean ICC", direction = -1) +
  theme(legend.position = "none") +
  ylab("Correlation with MB-CDI sumscore") +
  xlab("Analysis Window (in ms)") +
  facet_wrap(~dataset_name) +
  ggtitle("Correlation with CDI production, n > 50")
```
```{r}
ggplot(
  filter(cdi_corrs_long, measure == "cor_comp", n_prod > 50),
  aes(col = r)
) +
  geom_vline(xintercept = 0, linetype = "dashed") +
  geom_hline(yintercept = 0.05, lty = 3) +
  geom_segment(aes(x = t_start, xend = t_end, y = r, yend = r)) +
  scale_color_viridis(name = "Mean ICC", direction = -1) +
  theme(legend.position = "none") +
  ylab("Correlation with MB-CDI sumscore") +
  xlab("Analysis Window (in ms)") +
  facet_wrap(~dataset_name) +
  ggtitle("Correlation with CDI comprehension, n > 50")
```
```{r}
ms_corr <- cdi_corrs_long |>
  filter(measure == "cor_prod", n_prod > 50) |>
  group_by(t_start, t_end) |>
  summarise(r = mean(r, na.rm = TRUE))

ggplot(ms_corr, aes(col = r)) +
  geom_vline(xintercept = 0, linetype = "dashed") +
  geom_segment(aes(x = t_start, xend = t_end, y = r, yend = r)) +
  scale_color_viridis(name = "Mean ICC", direction = -1) +
  theme(legend.position = "none") +
  ylab("Correlation with MB-CDI sumscore") +
  xlab("Analysis Window (in ms)") +
  ggtitle("Correlation with CDI production, n > 50")
```

is something weird about comprehension?

```{r}
ms_corr_comp <- cdi_corrs_long |>
  filter(measure == "cor_comp", n_comp > 50) |>
  group_by(t_start, t_end) |>
  summarise(r = mean(r, na.rm = TRUE))

ggplot(ms_corr_comp, aes(col = r)) +
  geom_vline(xintercept = 0, linetype = "dashed") +
  geom_segment(aes(x = t_start, xend = t_end, y = r, yend = r)) +
  scale_color_viridis(name = "Mean ICC", direction = -1) +
  theme(legend.position = "none") +
  ylab("Correlation with MB-CDI sumscore") +
  xlab("Analysis Window (in ms)") +
  ggtitle("Correlation with CDI comprehension, n > 50")
```

## Across datasets

```{r}
cdi_sim_alldata <- function(t_start = -500, t_end = 4000) {
  # first average by trial then by subject
  by_trial_accuracies <- d_aoi |>
    filter(t_norm >= t_start, t_norm <= t_end) |>
    group_by(dataset_name, subject_id, administration_id, trial_id) |>
    summarize(correct = mean(correct, na.rm = TRUE))

  by_subject_accuracies <- by_trial_accuracies |>
    group_by(dataset_name, subject_id, administration_id) |>
    summarize(mean_correct = mean(correct, na.rm = TRUE)) |>
    left_join(cdi_data) |>
    filter(!(is.na(prod) & is.na(comp)) & !is.na(mean_correct))

  by_subject_accuracies |>
    ungroup() |>
    summarise(
      cor_comp = ifelse(sum(!is.na(comp)) > 0, cor.test(mean_correct, comp)$estimate, NA),
      cor_prod = ifelse(sum(!is.na(prod)) > 0, cor.test(mean_correct, prod)$estimate, NA),
      n_comp = sum(!is.na(comp)),
      n_prod = sum(!is.na(prod)),
      n_total = sum(!is.na(mean_correct))
    )
}

cluster_copy(cluster, "cdi_sim_alldata")
cluster_copy(cluster, "cdi_data")

tic()
cdi_corrs_alldata <- cdi_params |>
  partition(cluster) |>
  mutate(icc = pmap(list(t_start, t_end), cdi_sim_alldata)) |>
  collect() |>
  unnest(col = icc)
toc()


saveRDS(cdi_corrs_alldata, here("cached_intermediates", "3_cdi_corrs_alldata.Rds"))
```

```{r}
cdi_corrs_alldata <- readRDS(here("cached_intermediates", "3_cdi_corrs_alldata.Rds"))

ggplot(cdi_corrs_alldata, aes(col = cor_prod)) +
  geom_vline(xintercept = 0, linetype = "dashed") +
  geom_segment(aes(x = t_start, xend = t_end, y = cor_prod, yend = cor_prod)) +
  scale_color_viridis(name = "Mean ICC", direction = -1) +
  theme(legend.position = "none") +
  ylab("Correlation with MB-CDI sumscore") +
  xlab("Analysis Window (in ms)") +
  ggtitle("Accuracy/CDI production correlation across all datasets")
```


```{r}
ggplot(cdi_corrs_alldata, aes(col = cor_comp)) +
  geom_vline(xintercept = 0, linetype = "dashed") +
  geom_segment(aes(x = t_start, xend = t_end, y = cor_prod, yend = cor_prod)) +
  scale_color_viridis(name = "Mean ICC", direction = -1) +
  theme(legend.position = "none") +
  ylab("Correlation with MB-CDI sumscore") +
  xlab("Analysis Window (in ms)") +
  ggtitle("Accuracy/CDI comprehension correlation across all datasets")
```

on a per-dataset level, it's noisy, but at least across all the datasets, there's not much signal from comp -- the correlations are all pretty clustered (and correlations aren't high -- is this a bunch of little kids?). But given that even though numerically some short windows are best 500-4000 isn't actually much lower (so presumably within measurement error). 

For production, long window (going to 3000+ seems to be important)

# Bootstrap

```{r}
icc_window_sim_bootstrap <- function(t_start = -500, t_end = 4000, object) {
  print(paste(t_start, t_end))

  df <- d_aoi |>
    filter(t_norm > t_start, t_norm < t_end) |>
    group_by(dataset_name, dataset_id, administration_id, target_label, trial_id) |>
    summarise(
      accuracy = mean(correct, na.rm = TRUE),
      prop_data = mean(!is.na(correct))
    ) |>
    filter(!is.na(accuracy)) |>
    group_by(dataset_name, dataset_id, administration_id, target_label) |>
    mutate(repetition = row_number())

  # compute ICCs
  df |>
    group_by(dataset_name) |>
    nest() |>
    mutate(icc = map(data, \(d) bootstrap_icc(d, "accuracy", 2000))) |>
    select(-data) |>
    unnest(icc)
}


acc_params <- expand_grid(
  t_start = c(0, 200, 400, 500),
  t_end = c(2000, 4000),
  object = c("administration")
)
library(multidplyr)
# multidyplr attempt
cluster <- new_cluster(16)
cluster_library(cluster, "dplyr")
cluster_library(cluster, "tidyr")
cluster_library(cluster, "purrr")
cluster_library(cluster, "agreement")
cluster_copy(cluster, "icc_window_sim_bootstrap")
cluster_copy(cluster, "bootstrap_icc")
cluster_copy(cluster, "d_aoi")


accs_boot <- acc_params |>
  partition(cluster) |>
  # head(1) |>
  mutate(icc = pmap(list(t_start, t_end, object), \(t_s, t_e, o) icc_window_sim_bootstrap(t_s, t_e, o))) |>
  collect() |>
  unnest(col = icc)


saveRDS(here("cached_intermediates", "3_acc_icc_boot.rds"))
```

```{r}
acc_boot <- readRDS(here("cached_intermediates", "3_acc_icc_boot.rds"))
```

```{r}
acc_boot |> ggplot(aes(x = t_start, y = est, ymin = lower, ymax = upper, color = as.character(t_end))) +
  geom_pointrange(position = position_dodge(width = 25)) +
  facet_wrap(~dataset_name)
```

so we're going to need to figure out what to do here, since this doesn't easily represent the how much which is better and more the "this is very noisy"

# Summary

* ICCs show high reliability for LONG time windows (500 - 4000) for subjects. 
* For administrations, the baseline period actually is quite reliable (indicating visual salience). 
* For experimental effects, there is a shorter window when the curves pull apart, but we don't have an a priori guess about when that is. Would be interesting to do this with more datasets. 
* For correlations with CDI, it looks like shorter more traditional windows are more correlated (at least for the youngest kids). 

So longer windows apparently index information about the child beyond what's captured by parent report. This could be language-related but not captured by CDIs - for example, late emerging or more fragmentary word knowledge, or it could be something like attention or memory or sticky fixation. Some users might want to get this information, while others might not. 


# (Old -- to update) Validity via experimental effect

We're going to use the size and significance of the Swingley & Aslin (2002) mispronunciation effect as our simulation target instead of ICCs. 

This is with younger kids. 

We have to reload data because our working dataframe is only "vanilla" familiar word trials. 


```{r}
library(peekbankr)
subjects <- get_subjects()
sa_administrations <- get_administrations(dataset_name = "swingley_aslin_2002")
sa_trial_types <- get_trial_types(dataset_name = "swingley_aslin_2002")
sa_trials <- get_trials(dataset_name = "swingley_aslin_2002")
sa_aoi_timepoints <- get_aoi_timepoints(dataset_name = "swingley_aslin_2002")

sa_data <- sa_aoi_timepoints |>
  left_join(sa_administrations) |>
  left_join(sa_trials) |>
  left_join(sa_trial_types) |>
  left_join(subjects) |>
  filter(condition != "filler") |>
  mutate(condition = if_else(condition == "cp", "Correct", "Mispronounced"))
```
visualize the curves:

```{r}
correct_accuracy <- sa_data |>
  group_by(t_norm, condition) |>
  summarise(correct = sum(aoi == "target") /
    sum(aoi %in% c("target", "distractor")))

ggplot(correct_accuracy, aes(x = t_norm, y = correct, col = condition)) +
  geom_point() +
  geom_smooth() +
  geom_hline(yintercept = .5, lty = 2, col = "black")
```


```{r }
sa_sim <- function(t_start = -500, t_end = 4000) {
  by_subject_accuracies <- sa_data |>
    filter(t_norm >= t_start, t_norm <= t_end) |>
    group_by(condition, t_norm, administration_id) |>
    summarize(correct = sum(aoi == "target") /
      sum(aoi %in% c("target", "distractor")))

  mean_accuracies <- by_subject_accuracies |>
    group_by(administration_id, condition) |>
    summarize(mean_correct = mean(correct)) |>
    group_by(administration_id) |>
    summarise(diff = mean_correct[condition == "Correct"] -
      mean_correct[condition == "Mispronounced"])

  tibble(
    acc_diff = mean(mean_accuracies$diff),
    p_val = t.test(mean_accuracies$diff)$p.value
  )
}
```

```{r}
sa_acc_params <- expand_grid(
  t_start = seq(-1000, 1500, 100),
  t_end = seq(2000, 3000, 100)
)
tic()
sa_accs <- sa_acc_params |>
  # partition(cluster) |>
  mutate(icc = pmap(list(t_start, t_end), sa_sim)) |>
  # collect() |>
  unnest(col = icc)
toc()

save(file = "cached_intermediates/3_accs_sa.Rds", sa_accs)
```
Visualize.

```{r}
load(file = "cached_intermediates/3_accs_sa.Rds")

ggplot(sa_accs, aes(col = p_val)) +
  geom_vline(xintercept = 0, linetype = "dashed") +
  geom_hline(yintercept = 0.05, lty = 3) +
  geom_segment(aes(x = t_start, xend = t_end, y = p_val, yend = p_val)) +
  # geom_segment(aes(x=t_start,xend=t_start,y=mean_icc-0.005,yend=mean_icc+0.005))+
  # geom_segment(aes(x=t_end,xend=t_end,y=mean_icc-0.005,yend=mean_icc+0.005))+
  scale_color_viridis(name = "Mean ICC", direction = -1) +
  scale_y_log10() +
  theme(legend.position = "none") +
  ylab("Log p-value on key test") +
  xlab("Analysis Window (in ms)")
```

Conclusion: if you cherry pick the window that has the biggest difference, you will get the lowest p-value. We should have known that before the simulation. 

So really the question is when this window is, A PRIORI. Because of course the issue is that this analysis has a horrible false positive problem. Hence permutation-based analyses. 

