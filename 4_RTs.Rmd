---
title: "RT Computation"
author: "Mike Frank"
date: "2022-12-15"
output: html_document
---


```{r}
source(here::here("helper/common.R"))
```
# Load data and prep intermediates

(no need to run)
```{r}
d_aoi <- readRDS(here("cached_intermediates", "1_d_aoi.Rds"))
```

```{r}
source("helper/rt_helper.R")
```

```{r}
foo <- readRDS(here("cluster/rts.rds")) |>
  filter(shift_type == "D-T") |>
  filter(dataset_name == "adams_marchman_2018") |>
  group_by(dataset_name, time_0, window, time_end, during, frac) |>
  nest()

test <- foo$data[[1]]

test |> select(administration_id, trial_id, target_label)
```

## Prep
looking at
* the 3 options for RT = the first time you leave, the last time you leave before landing, when you land
* raw v log RT
* the option to trim for max shift length of 600ms either calculated from first leaving or final leaving

```{r}
rle_data <- d_aoi %>%
  filter(
    any(t_norm == 0), # must have data at 0
    t_norm >= 0
  ) %>% # only pass data after 0
  group_by(administration_id, trial_id, trial_order, dataset_name) %>%
  reframe(
    lengths = rle(aoi)$lengths,
    values = rle(aoi)$values
  )


grid_options_3 <- expand_grid(window = c(0), time_0 = c(F, T), time_end = F, during = T, frac = 0) |>
  bind_rows(expand_grid(window = c(200, 300, 400, 500), time_0 = T, time_end = T, during = T, frac = 1)) |>
  bind_rows(expand_grid(
    window = c(250, 350, 450, 600, 700, 800, 900, 1000),
    time_0 = T, time_end = T, during = T, frac = 0
  ))


grid_options_1 <- expand_grid(window = c(400, 500), time_0 = c(T, F), time_end = c(T, F), during = T, frac = c(0, .5, .75))

grid_options_2 <- expand_grid(window = c(200, 300), time_0 = c(T, F), time_end = c(T, F), during = T, frac = c(0, .5, .75))

tic()
rts_1 <- rle_data %>%
  group_by(administration_id, trial_id, trial_order) %>%
  nest() |>
  cross_join(grid_options_1) |>
  mutate(rts = pmap(
    list(data, time_0, window, time_end, during, frac),
    \(d, t0, w, te, dur, fr)
    get_rt(d,
      t_0 = t0, window_length = w,
      t_end = te, window_mostly_region = dur, mostly_fraction = fr
    )
  )) |>
  select(-data) |>
  unnest(cols = c(rts))
toc()

saveRDS(rts_1, here("cached_intermediates", "4_rt_1.rds"))

tic()
rts_2 <- rle_data %>%
  group_by(administration_id, trial_id, trial_order) %>%
  nest() |>
  cross_join(grid_options_2) |>
  mutate(rts = pmap(
    list(data, time_0, window, time_end, during, frac),
    \(d, t0, w, te, dur, fr)
    get_rt(d,
      t_0 = t0, window_length = w,
      t_end = te, window_mostly_region = dur, mostly_fraction = fr
    )
  )) |>
  select(-data) |>
  unnest(cols = c(rts))
toc()

saveRDS(rts_2, here("cached_intermediates", "4_rt_2.rds"))

tic()
rts_3 <- rle_data %>%
  group_by(administration_id, trial_id, trial_order) %>%
  nest() |>
  cross_join(grid_options_3) |>
  mutate(rts = pmap(
    list(data, time_0, window, time_end, during, frac),
    \(d, t0, w, te, dur, fr)
    get_rt(d,
      t_0 = t0, window_length = w,
      t_end = te, window_mostly_region = dur, mostly_fraction = fr
    )
  )) |>
  select(-data) |>
  unnest(cols = c(rts))
toc()

saveRDS(rts_3, here("cached_intermediates", "4_rt_3.rds"))
```

```{r}
d_rt_1 <- readRDS(here("cached_intermediates", "4_rt_1.rds"))


d_rt_dt_1 <- d_rt_1 |>
  filter(shift_type == "D-T") |>
  mutate(
    land_rt = rt,
    first_launch_rt = shift_start_rt,
    # last_launch_rt = last_shift_rt
  ) |>
  mutate(
    # across(c("land_rt", "first_launch_rt", "last_launch_rt"), log, .names = "log_{.col}"),
    across(
      c(
        "land_rt", "first_launch_rt",
        # "log_land_rt", "log_first_launch_rt",
      ),
      ~ ifelse(shift_length <= 600, .x, NA),
      .names = "trim_first_{.col}"
    ),
    across(
      c(
        "land_rt", # "last_launch_rt",
        # "log_land_rt", "log_last_launch_rt"
      ),
      ~ ifelse(last_shift_length <= 600, .x, NA),
      .names = "trim_last_{.col}"
    )
  ) |>
  select(-rt, -shift_start_rt, -last_shift_rt) |>
  left_join(d_aoi %>%
    select(administration_id, dataset_name, subject_id, trial_id, trial_order, target_label) %>%
    distinct())

# just because I keep having crashing issues with icc
saveRDS(d_rt_dt_1, here("cached_intermediates", "4_rt_dt_1.Rds"))
```

```{r}
d_rt_dt_1 <- readRDS(here("cached_intermediates", "4_rt_dt_1.Rds"))


# cluster <- new_cluster(4)
# cluster_library(cluster, "tidyverse")
# cluster_library(cluster, "agreement")
# cluster_copy(cluster, "get_icc")
tic()
rt_iccs_1 <- d_rt_dt_1 |>
  group_by(dataset_name, time_0, window, time_end, during, frac) |>
  nest() |>
  filter(dataset_name != "adams_marchman_2018") |> # in case this is the reason it keeps crashing?
  # partition(cluster) |>
  mutate(icc_admin = map(data, \(d) {
    print(dataset_name)
    rt_names <- colnames(d)[str_ends(colnames(d), "rt")]
    n_total <- map_dbl(rt_names, \(rt_col) {
      d |>
        filter(!is.na(d[rt_col])) |>
        nrow()
    })
    n_admin <- map_dbl(rt_names, \(rt_col) {
      d |>
        filter(!is.na(d[rt_col])) |>
        select(administration_id) |>
        unique() |>
        nrow()
    })
    icc_values <- map_dbl(rt_names, \(rt_col) {
      get_icc(d |> filter(!is.na(d[rt_col])), column = rt_col, object = "administration")
    })
    tibble(measure = rt_names, icc = icc_values, n_total = n_total, n_admin = n_admin)
  })) |>
  select(-data) |>
  #   collect() |>
  unnest(icc_admin)
toc()

saveRDS(rt_iccs_1, here("cached_intermediates", "4_rt_iccs_1.rds"))

# try again at adams_marchman

tic()
rt_iccs_1_am <- d_rt_dt_1 |>
  group_by(dataset_name, time_0, window, time_end, during, frac) |>
  nest() |>
  filter(dataset_name == "adams_marchman_2018") |> # try again
  mutate(icc_admin = map(data, \(d) {
    print(dataset_name)
    rt_names <- colnames(d)[str_ends(colnames(d), "rt")]
    n_total <- map_dbl(rt_names, \(rt_col) {
      d |>
        filter(!is.na(d[rt_col])) |>
        nrow()
    })
    n_admin <- map_dbl(rt_names, \(rt_col) {
      d |>
        filter(!is.na(d[rt_col])) |>
        select(administration_id) |>
        unique() |>
        nrow()
    })
    icc_values <- map_dbl(rt_names, \(rt_col) {
      get_icc(d |> filter(!is.na(d[rt_col])), column = rt_col, object = "administration")
    })
    tibble(measure = rt_names, icc = icc_values, n_total = n_total, n_admin = n_admin)
  })) |>
  select(-data) |>
  unnest(icc_admin)
toc()

saveRDS(rt_iccs_1_am, here("cached_intermediates", "4_rt_iccs_1_am.rds"))
```

```{r}
d_rt_2 <- readRDS(here("cached_intermediates", "4_rt_2.rds"))


d_rt_dt_2 <- d_rt_2 |>
  filter(shift_type == "D-T") |>
  mutate(
    land_rt = rt,
    first_launch_rt = shift_start_rt,
    # last_launch_rt = last_shift_rt
  ) |>
  mutate(
    # across(c("land_rt", "first_launch_rt", "last_launch_rt"), log, .names = "log_{.col}"),
    across(
      c(
        "land_rt", "first_launch_rt",
        # "log_land_rt", "log_first_launch_rt",
      ),
      ~ ifelse(shift_length <= 600, .x, NA),
      .names = "trim_first_{.col}"
    ),
    across(
      c(
        "land_rt", # "last_launch_rt",
        # "log_land_rt", "log_last_launch_rt"
      ),
      ~ ifelse(last_shift_length <= 600, .x, NA),
      .names = "trim_last_{.col}"
    )
  ) |>
  select(-rt, -shift_start_rt, -last_shift_rt) |>
  left_join(d_aoi %>%
    select(administration_id, dataset_name, subject_id, trial_id, trial_order, target_label) %>%
    distinct())

# just because I keep having crashing issues with icc
saveRDS(d_rt_dt_2, here("cached_intermediates", "4_rt_dt_2.Rds"))
```

```{r}
d_rt_dt_2 <- readRDS(here("cached_intermediates", "4_rt_dt_2.Rds"))


tic()
rt_iccs_2 <- d_rt_dt_2 |>
  group_by(dataset_name, time_0, window, time_end, during, frac) |>
  nest() |>
  filter(dataset_name != "adams_marchman_2018") |> # in case this is the reason it keeps crashing?
  # partition(cluster) |>
  mutate(icc_admin = map(data, \(d) {
    print(dataset_name)
    rt_names <- colnames(d)[str_ends(colnames(d), "rt")]
    n_total <- map_dbl(rt_names, \(rt_col) {
      d |>
        filter(!is.na(d[rt_col])) |>
        nrow()
    })
    n_admin <- map_dbl(rt_names, \(rt_col) {
      d |>
        filter(!is.na(d[rt_col])) |>
        select(administration_id) |>
        unique() |>
        nrow()
    })
    icc_values <- map_dbl(rt_names, \(rt_col) {
      get_icc(d |> filter(!is.na(d[rt_col])), column = rt_col, object = "administration")
    })
    tibble(measure = rt_names, icc = icc_values, n_total = n_total, n_admin = n_admin)
  })) |>
  select(-data) |>
  #   collect() |>
  unnest(icc_admin)
toc()

saveRDS(rt_iccs_2, here("cached_intermediates", "4_rt_iccs_2.rds"))

# try again at adams_marchman

tic()
rt_iccs_2_am <- d_rt_dt_2 |>
  group_by(dataset_name, time_0, window, time_end, during, frac) |>
  nest() |>
  filter(dataset_name == "adams_marchman_2018") |> # try again
  mutate(icc_admin = map(data, \(d) {
    print(dataset_name)
    rt_names <- colnames(d)[str_ends(colnames(d), "rt")]
    n_total <- map_dbl(rt_names, \(rt_col) {
      d |>
        filter(!is.na(d[rt_col])) |>
        nrow()
    })
    n_admin <- map_dbl(rt_names, \(rt_col) {
      d |>
        filter(!is.na(d[rt_col])) |>
        select(administration_id) |>
        unique() |>
        nrow()
    })
    icc_values <- map_dbl(rt_names, \(rt_col) {
      get_icc(d |> filter(!is.na(d[rt_col])), column = rt_col, object = "administration")
    })
    tibble(measure = rt_names, icc = icc_values, n_total = n_total, n_admin = n_admin)
  })) |>
  select(-data) |>
  unnest(icc_admin)
toc()

saveRDS(rt_iccs_2_am, here("cached_intermediates", "4_rt_iccs_2_am.rds"))
```
```{r}
d_rt_3 <- readRDS(here("cached_intermediates", "4_rt_3.rds"))


d_rt_dt_3 <- d_rt_3 |>
  filter(shift_type == "D-T") |>
  mutate(
    land_rt = rt,
    first_launch_rt = shift_start_rt,
    # last_launch_rt = last_shift_rt
  ) |>
  mutate(
    # across(c("land_rt", "first_launch_rt", "last_launch_rt"), log, .names = "log_{.col}"),
    across(
      c(
        "land_rt", "first_launch_rt",
        # "log_land_rt", "log_first_launch_rt",
      ),
      ~ ifelse(shift_length <= 600, .x, NA),
      .names = "trim_first_{.col}"
    ),
    across(
      c(
        "land_rt", # "last_launch_rt",
        # "log_land_rt", "log_last_launch_rt"
      ),
      ~ ifelse(last_shift_length <= 600, .x, NA),
      .names = "trim_last_{.col}"
    )
  ) |>
  select(-rt, -shift_start_rt, -last_shift_rt) |>
  left_join(d_aoi %>%
    select(administration_id, dataset_name, subject_id, trial_id, trial_order, target_label) %>%
    distinct())

# just because I keep having crashing issues with icc
saveRDS(d_rt_dt_3, here("cached_intermediates", "4_rt_dt_3.Rds"))
```
```{r}
d_rt_dt_3 <- readRDS(here("cached_intermediates", "4_rt_dt_3.Rds"))


tic()
rt_iccs_3 <- d_rt_dt_3 |>
  group_by(dataset_name, time_0, window, time_end, during, frac) |>
  nest() |>
  filter(dataset_name != "adams_marchman_2018") |> # in case this is the reason it keeps crashing?
  # partition(cluster) |>
  mutate(icc_admin = map(data, \(d) {
    print(dataset_name)
    rt_names <- colnames(d)[str_ends(colnames(d), "rt")]
    n_total <- map_dbl(rt_names, \(rt_col) {
      d |>
        filter(!is.na(d[rt_col])) |>
        nrow()
    })
    n_admin <- map_dbl(rt_names, \(rt_col) {
      d |>
        filter(!is.na(d[rt_col])) |>
        select(administration_id) |>
        unique() |>
        nrow()
    })
    icc_values <- map_dbl(rt_names, \(rt_col) {
      get_icc(d |> filter(!is.na(d[rt_col])), column = rt_col, object = "administration")
    })
    tibble(measure = rt_names, icc = icc_values, n_total = n_total, n_admin = n_admin)
  })) |>
  select(-data) |>
  #   collect() |>
  unnest(icc_admin)
toc()

saveRDS(rt_iccs_3, here("cached_intermediates", "4_rt_iccs_3.rds"))

# try again at adams_marchman

# too much crashing, it's just too big
tic()
# rt_iccs_3_am <- d_rt_dt_3 |>
#   group_by(dataset_name, time_0, window, time_end, during, frac) |>
#   filter(window>0) |> #otherwise it definitely crashes
#   nest() |>
#   filter(dataset_name == "adams_marchman_2018")  # try again
#   mutate(icc_admin = map(data, \(d) {
#     print(dataset_name)
#     rt_names <- colnames(d)[str_ends(colnames(d), "rt")]
#     n_total <- map_dbl(rt_names, \(rt_col) {
#       d |>
#         filter(!is.na(d[rt_col])) |>
#         nrow()
#     })
#     n_admin <- map_dbl(rt_names, \(rt_col) {
#       d |>
#         filter(!is.na(d[rt_col])) |>
#         select(administration_id) |>
#         unique() |>
#         nrow()
#     })
#     icc_values <- map_dbl(rt_names, \(rt_col) {
#       get_icc(d |> filter(!is.na(d[rt_col])), column = rt_col, object = "administration")
#     })
#     tibble(measure = rt_names, icc = icc_values, n_total = n_total, n_admin = n_admin)
#   })) |>
#   select(-data) |>
#   unnest(icc_admin)
# toc()
#
# saveRDS(rt_iccs_3_am, here("cached_intermediates", "4_rt_iccs_3_am.rds"))
```

### combine

```{r}
icc_combined <- readRDS(here("cached_intermediates", "4_rt_iccs_3.rds")) |>
  bind_rows(readRDS(here("cached_intermediates", "4_rt_iccs_2.rds"))) |>
  bind_rows(readRDS(here("cached_intermediates", "4_rt_iccs_1.rds"))) |>
  bind_rows(readRDS(here("cached_intermediates", "4_rt_iccs_1_am.rds"))) |>
  bind_rows(readRDS(here("cached_intermediates", "4_rt_iccs_2_am.rds")))

# note that what was in 3 was a wider range of lengths and stuff and that kept crashing on AM because of the number of datapoints

saveRDS(icc_combined, here("cached_intermediates", "4_rt_iccs.rds"))
```

### logged follow up

```{r}
d_rt_dt <- readRDS(here("cached_intermediates", "4_rt_dt_3.Rds")) |>
  bind_rows(readRDS(here("cached_intermediates", "4_rt_dt_2.Rds"))) |>
  bind_rows(readRDS(here("cached_intermediates", "4_rt_dt_1.Rds")))


rt_dt_trad <- d_rt_dt |> filter(window == 400, frac == 1)

rt_dt_new <- d_rt_dt |> filter(window == 400, frac == 0, time_0 == T, time_end == T)

rt_dt_for_log <- rt_dt_trad |>
  bind_rows(rt_dt_new) |>
  mutate(across(c("land_rt", "first_launch_rt", "trim_first_land_rt", "trim_first_first_launch_rt"), \(rt) ifelse(is.na(rt), NA, log(rt)), .names = "log_{.col}"))
```

```{r}
tic()
rt_iccs_4 <- rt_dt_for_log |>
  group_by(dataset_name, time_0, window, time_end, during, frac) |>
  nest() |>
  filter(dataset_name != "adams_marchman_2018") |> # again with the crashing
  mutate(icc_admin = map(data, \(d) {
    print(dataset_name)
    rt_names <- colnames(d)[str_ends(colnames(d), "rt")]
    n_total <- map_dbl(rt_names, \(rt_col) {
      d |>
        filter(!is.na(d[rt_col])) |>
        nrow()
    })
    n_admin <- map_dbl(rt_names, \(rt_col) {
      d |>
        filter(!is.na(d[rt_col])) |>
        select(administration_id) |>
        unique() |>
        nrow()
    })
    icc_values <- map_dbl(rt_names, \(rt_col) {
      get_icc(d |> filter(!is.na(d[rt_col])), column = rt_col, object = "administration")
    })
    tibble(measure = rt_names, icc = icc_values, n_total = n_total, n_admin = n_admin)
  })) |>
  select(-data) |>
  unnest(icc_admin)
toc()

saveRDS(rt_iccs_4, here("cached_intermediates", "4_rt_iccs_log.rds"))

tic()
rt_iccs_4_am <- rt_dt_for_log |>
  group_by(dataset_name, time_0, window, time_end, during, frac) |>
  nest() |>
  filter(dataset_name == "adams_marchman_2018") |>
  mutate(icc_admin = map(data, \(d) {
    print(dataset_name)
    rt_names <- colnames(d)[str_ends(colnames(d), "rt")]
    n_total <- map_dbl(rt_names, \(rt_col) {
      d |>
        filter(!is.na(d[rt_col])) |>
        nrow()
    })
    n_admin <- map_dbl(rt_names, \(rt_col) {
      d |>
        filter(!is.na(d[rt_col])) |>
        select(administration_id) |>
        unique() |>
        nrow()
    })
    icc_values <- map_dbl(rt_names, \(rt_col) {
      get_icc(d |> filter(!is.na(d[rt_col])), column = rt_col, object = "administration")
    })
    tibble(measure = rt_names, icc = icc_values, n_total = n_total, n_admin = n_admin)
  })) |>
  select(-data) |>
  unnest(icc_admin)
toc()

saveRDS(rt_iccs_4_am, here("cached_intermediates", "4_rt_iccs_log_am.rds"))

readRDS(here("cached_intermediates", "4_rt_iccs_log.rds")) |>
  bind_rows(rt_iccs_4_am) |>
  saveRDS(here("cached_intermediates", "4_rt_iccs_log.rds"))
```


# Measure 3: Reaction time


IV analysis options:
* what time point to use for RT -- the first time you leave, the last time you leave before landing, when you land
* raw vs log 
* filter by max shift length
* filter min start time
* do we exclude based on pre-min loss (between t_0 and min start time)

outcome measures:
* reliability = ICC
* data loss = how many datapoints are kept

## how often does it matter?

```{r}
d_rt <- readRDS(here("cached_intermediates", "4_rt_3.rds")) |> filter(window == 0, time_0 == T)
```
 
 here we're looking at D-T transitions, where the child was on D at time 0
 
### time point to use for RT + max shift length


```{r}
ggplot(d_rt, aes(x = shift_start_rt, y = rt)) +
  geom_point(alpha = .1) +
  geom_smooth(method = "lm")

ggplot(
  d_rt,
  aes(x = shift_start_rt, y = rt)
) +
  geom_point(alpha = .1) +
  geom_point(
    data = filter(d_rt, shift_length > 600),
    col = "red"
  ) +
  geom_smooth(method = "lm")
```

this is filtering at 600ms based on first leaving distractor

```{r}
# does type of start initiation matter

d_rt |>
  ungroup() |>
  filter(!is.na(shift_start_rt)) |>
  mutate(
    initiation_type_matters = shift_start_rt != last_shift_rt,
    long_shift_occurs = shift_length > 600
  ) |>
  group_by(initiation_type_matters, long_shift_occurs) |>
  tally() |>
  ungroup() |>
  mutate(pct = n / sum(n))
```
for the most part, these combos won't matter because >95% of the data has the kid only leaving once and doesn't have a long shift. 

distribution of shift lengths -- cut off at 1000ms; by far most shifts are <250ms

```{r}
d_rt |> ggplot(aes(x = shift_length)) +
  geom_histogram(binwidth = 50) +
  coord_cartesian(xlim = c(0, 1000))
d_rt |> ggplot(aes(x = last_shift_length)) +
  geom_histogram(binwidth = 50) +
  coord_cartesian(xlim = c(0, 1000))
```
### how important is consistency in 0-400ms window
how much of the time are kids looking at the same thing between 0 and 400 ms -- many studies require looks to distractor + no shifts for 250/300/367 ms; how much data loss is coming from this?

```{r}
d_aoi <- readRDS(here("cached_intermediates", "1_d_aoi.Rds"))

t_0 <- d_aoi |>
  filter(t_norm == 0) |>
  mutate(t_0_aoi = aoi) |>
  select(administration_id, trial_id, dataset_name, t_0_aoi) |>
  mutate(t_0_aoi = ifelse(t_0_aoi %in% c("target", "distractor"), t_0_aoi, "other"))
t_375 <- d_aoi |>
  filter(t_norm == 375) |>
  mutate(t_375_aoi = aoi) |>
  select(administration_id, trial_id, dataset_name, t_375_aoi) |>
  mutate(t_375_aoi = ifelse(t_375_aoi %in% c("target", "distractor"), t_375_aoi, "other"))
early_dist <- d_aoi |>
  filter(t_norm < 400 & t_norm >= 0) |>
  group_by(administration_id, trial_id, dataset_name, aoi) |>
  tally() |>
  left_join(t_0) |>
  left_join(t_375) |>
  pivot_wider(names_from = aoi, values_from = n, values_fill = 0) |>
  mutate(type = case_when(
    target + distractor + missing + other != 16 ~ "some missing ts",
    target == 16 ~ "only target",
    distractor == 16 ~ "only distractor",
    missing + other == 16 ~ "all off",
    target == 0 & distractor > 8 ~ "d+m (mostly d)",
    target == 0 ~ "d+m (mostly m)",
    distractor == 0 & target > 8 ~ "t+m (mostly t)",
    distractor == 0 ~ "t+m (mostly m)",
    T ~ "both"
  ))

ggplot(early_dist, aes(x = t_0_aoi, fill = type)) +
  facet_wrap(~t_375_aoi) +
  geom_bar()

early_dist |>
  group_by(type, t_0_aoi, t_375_aoi) |>
  tally() |>
  ungroup() |>
  mutate(pct = round(n / sum(n), 3)) |>
  arrange(desc(pct))
```
we want to know of the ones that are on t/d/m at start

~33% is on distractor for 0-375
~32% is on target for 0-375

~10% is missing/other for 0-375(this is going to get tossed in all cases!)

~5.5% goes from d to t
~5.5% goes from t to d 

~5.5% is mostly on d during the time, with some being on other (at start/end/middle)
~4.5% is mostly on t during the time, with some being on other (at start/end/middle)

to look at:
* when we have d/other or t/other what's the typical patterns
  * how much is other & when
* for both t/d this probably can't be salvaged but look at patterns

```{r}
early_dist |>
  filter(target == 0) |>
  filter(missing + other < 16) |>
  filter(distractor != 16) |>
  ggplot(aes(x = distractor)) +
  geom_bar()

early_dist |>
  filter(distractor == 0) |>
  filter(missing + other < 16) |>
  filter(target != 16) |>
  ggplot(aes(x = target)) +
  geom_bar()
```
so, most of the d/m or t/m mixes is heavily weighted to d or t with only a little m.
when does this m occur?
```{r}
d_aoi |>
  filter(t_norm < 400 & t_norm >= 0) |>
  inner_join(early_dist |> filter(target == 0) |> filter(missing + other < 16) |> filter(distractor != 16) |> select(administration_id, dataset_name, trial_id, num_distractor = distractor)) |>
  mutate(is_distractor = ifelse(aoi == "distractor", 1, 0)) |>
  group_by(t_norm, num_distractor) |>
  summarize(is_distractor = mean(is_distractor)) |>
  ggplot(aes(x = t_norm, y = is_distractor, col = num_distractor, group = num_distractor)) +
  geom_point() +
  geom_line() +
  scale_color_viridis()
```
so, most of the d/m that is mostly d has the m at the beginning or end (rather than in the middle) -- possibly this is just because of how fixations work? 
this is especially true when d is >11 or so, which is most of this chunk of data. 

```{r}
d_aoi |>
  filter(t_norm < 400 & t_norm >= 0) |>
  inner_join(early_dist |> filter(distractor == 0) |> filter(missing + other < 16) |> filter(target != 16) |> select(administration_id, dataset_name, trial_id, num_target = target)) |>
  mutate(is_target = ifelse(aoi == "target", 1, 0)) |>
  group_by(t_norm, num_target) |>
  summarize(is_target = mean(is_target)) |>
  ggplot(aes(x = t_norm, y = is_target, col = num_target, group = num_target)) +
  geom_point() +
  geom_line() +
  scale_color_viridis()
```
what about when there is both t and d?

I think all we're seeing here is autocorrelation, and also not sure how to deal with 3 way data. 

```{r}
d_aoi |>
  filter(t_norm < 400 & t_norm >= 0) |>
  inner_join(early_dist |> filter(distractor > 0) |> filter(missing + other < 16) |> filter(target > 0) |> select(administration_id, dataset_name, trial_id, num_target = target, num_distractor = distractor)) |>
  mutate(distractor_target = case_when(
    aoi == "target" ~ 1,
    aoi == "distractor" ~ 0,
    T ~ NA
  )) |>
  group_by(t_norm, num_target) |>
  summarize(distractor_target = mean(distractor_target, na.rm = T)) |>
  ggplot(aes(x = t_norm, y = distractor_target, col = num_target, group = num_target)) +
  geom_point() +
  geom_line() +
  scale_color_viridis()

d_aoi |>
  filter(t_norm < 400 & t_norm >= 0) |>
  inner_join(early_dist |> filter(distractor > 0) |> filter(missing + other < 16) |> filter(target > 0) |> select(administration_id, dataset_name, trial_id, num_target = target, num_distractor = distractor)) |>
  mutate(distractor_target = case_when(
    aoi == "target" ~ 1,
    aoi == "distractor" ~ 0,
    T ~ NA
  )) |>
  group_by(t_norm, num_distractor) |>
  summarize(distractor_target = mean(distractor_target, na.rm = T)) |>
  ggplot(aes(x = t_norm, y = distractor_target, col = num_distractor, group = num_distractor)) +
  geom_point() +
  geom_line() +
  scale_color_viridis()
```
Take-aways from data-viz:
* most of the data is reasonably behaved 30% T-T, 30% D-D, we're looking at whether we can save another like 5%ish onto either by relaxing early missing data requirements
* for other exclusions around long-shift, again, not much data is being affected (maybe 5% but some of this is also going to overlap more with data that has other issues)

reasonable options for where looking when:
* at t=0 at D (nothing else)
* during window (of variable window length), only looking at D 
* during window (of variable window length), not looking at T 
* during window (of variable window length), not looking at T and looking at D 3/4+ of the time (or other fraction -- traditional analyses use 1)
* at t=0 at D & during window (of variable window length), not looking at T (window {100, 200, 300, 400})
* at t=0 at D & * during window (of variable window length), not looking at T and looking at D 3/4+ of the time (window {100, 200, 300, 400})

so maybe the combinatorics are:

always: during window no looking at T
* looking at D at t=0 (yes/no)
* window length {100, 200, 300, 400}
* looking at D at t=end of window (yes,no)
* looking at D 3/4+ of the time during the window (or other fraction)

## Analyse

We consider a few different options for how to get RTs
All of these are framed as D-T but same would apply for T-D

* we consider RTs measured either from the first time you leave D (first_launch) or the first time you arrive at T (land) [theoretically, the last time you leave D is another option, that was tried in an earlier iteration, but dropped here for tractability -- see descriptives above for this only mattering in a limited number of cases]
* raw v log RT -- here everything is raw, but we can re-address log for the top few
* the option to trim overly long shifts to 600ms either calculated from first leaving or final leaving

how do we determine that the kid is sufficiently on D at the relevant time:
* "relevant time" = window length in ms where they have to be not on T and potentially meet other constraints (below)
* time_0 -- do they have to be on D or just *not* on T
* window_end -- at the end of the window, do they have to be on D or just *not* on T
* fraction -- the minimum fraction of the time during the window that they have to be on D

```{r}
rt_iccs <- readRDS(here("cached_intermediates", "4_rt_iccs.rds"))

rt_iccs$dataset_name <- fct_reorder(rt_iccs$dataset_name, rt_iccs$icc)

rt_iccs_coded <- rt_iccs |> mutate(
  type = case_when(
    str_detect(measure, "first_launch_rt") ~ "first_launch",
    str_detect(measure, "last_launch_rt") ~ "last_launch",
    str_detect(measure, "land_rt") ~ "land"
  ),
  logged = case_when(
    str_detect(measure, "log") ~ "log",
    T ~ "raw"
  ),
  trimming = case_when(
    str_detect(measure, "trim_first") ~ "trim_first",
    str_detect(measure, "trim_last") ~ "trim_last",
    T ~ "untrimmed"
  )
)


rt_iccs_coded |> View()
```

```{r}
rt_iccs_summ <- rt_iccs_coded |>
  group_by(type, logged, trimming, window, time_0, time_end, during, frac) |>
  summarize(mean_icc = mean(icc, na.rm = T), datapoints = sum(n_total))

rt_iccs_summ |>
  arrange(desc(mean_icc)) |>
  head(20) |>
  knitr::kable()
```

```{r}
rt_iccs_summ |> ggplot(aes(x = str_c(logged, "_", type), y = mean_icc, group = interaction(type, logged, trimming))) +
  geom_point(aes(col = trimming), position = position_dodge(width = .4)) +
  stat_summary(position = position_dodge(width = .4)) +
  coord_flip() +
  geom_hline(yintercept = .65) +
  scale_color_brewer(type = "qual", palette = 1)
```



```{r}
rt_iccs_summ |>
  filter(window %in% c(0, 200, 300, 400, 500)) |>
  ggplot(aes(x = str_c("window_end", time_end), y = mean_icc, group = interaction(time_0, time_end, window))) +
  geom_point(aes(col = window), position = position_dodge(width = .4)) +
  stat_summary(position = position_dodge(width = .4)) +
  coord_flip() +
  facet_wrap(~ str_c("time_0", time_0)) +
  geom_hline(yintercept = .65) +
  scale_color_viridis()
```
so, here we see that 400 is best option (at least of these coarse options) and generally having both window start and end be true is best (I think?)

```{r}
rt_iccs_summ |>
  filter(window %in% c(400)) |>
  ggplot(aes(x = str_c("window_end", time_end), y = mean_icc, group = interaction(time_0, time_end, frac))) +
  geom_point(aes(col = as.character(frac)), position = position_dodge(width = .4)) +
  stat_summary(position = position_dodge(width = .4)) +
  coord_flip() +
  facet_wrap(~ str_c("time_0", time_0)) +
  geom_hline(yintercept = .65)
```
looking within the 400 window, looks like both start and end being true is important, but not the type of during. 

```{r}
rt_iccs_summ |>
  filter(time_0, time_end) |>
  ggplot(aes(x = window, y = mean_icc, group = interaction(time_0, time_end, window))) +
  geom_point(aes(col = window), position = position_dodge(width = .4)) +
  stat_summary(position = position_dodge(width = .4)) +
  geom_hline(yintercept = .65) +
  scale_color_viridis()
```
so, looks like 400 is best, but really up to 500 is okay-ish. 

```{r}
rt_iccs_summ |>
  filter(window == 400) |>
  ggplot(aes(x = str_c(logged, "_", type), y = mean_icc, group = interaction(type, logged, trimming))) +
  geom_point(aes(col = trimming), position = position_dodge(width = .4)) +
  stat_summary(position = position_dodge(width = .4)) +
  coord_flip() +
  geom_hline(yintercept = .65) +
  scale_color_brewer(type = "qual", palette = 1)
```
looking again, at 400, land + untrimmed seems fine!
```{r}
rt_iccs_summ |>
  filter(window == 400) |>
  filter(type == "land") |>
  filter(trimming == "untrimmed") |>
  ggplot(aes(x = str_c("window_end", time_end), y = mean_icc, group = interaction(time_0, time_end, frac))) +
  geom_point(aes(col = as.character(frac)), position = position_dodge(width = .4)) +
  coord_flip() +
  facet_wrap(~ str_c("time_0", time_0)) +
  geom_hline(yintercept = .7)
```

```{r}
rt_iccs_summ |>
  filter(time_0, time_end) |>
  filter(frac == 0) |>
  filter(type == "land") |>
  filter(trimming == "untrimmed") |>
  ggplot(aes(x = window, y = mean_icc, group = interaction(time_0, time_end, window))) +
  geom_point(aes(col = window), position = position_dodge(width = .4)) +
  geom_hline(yintercept = .65) +
  scale_color_viridis()
```
so, best options seem to be 
time_0 = T
time_end = T
window = 400 (or thereabouts)
land
trimmed
don't bother about fraction

remaining question is log v not log!
so should try

land_trad = frac=1
launch_trad = frac=1
window = 400 = T/T/frac=0

```{r}
rt_iccs_log <- readRDS(here("cached_intermediates", "4_rt_iccs_log.rds"))

rt_iccs_log$dataset_name <- fct_reorder(rt_iccs_log$dataset_name, rt_iccs_log$icc)

rt_iccs_log_coded <- rt_iccs_log |> mutate(
  type = case_when(
    str_detect(measure, "first_launch_rt") ~ "first_launch",
    str_detect(measure, "last_launch_rt") ~ "last_launch",
    str_detect(measure, "land_rt") ~ "land"
  ),
  logged = case_when(
    str_detect(measure, "log") ~ "log",
    T ~ "raw"
  ),
  trimming = case_when(
    str_detect(measure, "trim_first") ~ "trim_first",
    str_detect(measure, "trim_last") ~ "trim_last",
    T ~ "untrimmed"
  )
)


rt_iccs_log_coded |> View()
```


```{r}
rt_iccs_log_summ <- rt_iccs_log_coded |>
  group_by(type, logged, trimming, window, time_0, time_end, during, frac) |>
  summarize(mean_icc = mean(icc, na.rm = T), datapoints = sum(n_total))

rt_iccs_log_summ |>
  arrange(desc(mean_icc)) |>
  head(20) |>
  knitr::kable()
```

```{r}
rt_iccs_log_summ |> ggplot(aes(x = str_c(logged, "_", type), y = mean_icc, group = interaction(type, logged, trimming))) +
  geom_point(aes(col = trimming, shape = str_c("fraction", frac)), position = position_dodge(width = .4)) +
  coord_flip() +
  geom_hline(yintercept = .65) +
  scale_color_brewer(type = "qual", palette = 1)
```
looks like log is slightly better, but doesn't really matter. 


```{r}
rt_iccs_log_coded |> ggplot(aes(x = str_c(logged, "_", type), y = icc, group = interaction(type, logged, trimming))) +
  geom_point(aes(col = trimming, shape = str_c("fraction", frac)), position = position_dodge(width = .4)) +
  coord_flip() +
  geom_hline(yintercept = .65) +
  scale_color_brewer(type = "qual", palette = 1) +
  facet_wrap(~dataset_name) +
  theme(legend.position = "none")
```
seems like for most datasets there aren't huge differences, and there aren't datasets where our proposal is awful. log versus raw seems somewhat dataset dependent

## Validity

so we don't have great validity options here because it's less clear what RT should index, but we can check for age effects (very expected, but could index non-linguistic noise) and CDI (not necessarily expected to correlate as much with RT as accuracy)

so we'll go with the 400ms versions of the 3 above x whether they are logged 

```{r}
d_rt_dt <- readRDS(here("cached_intermediates", "4_rt_dt_3.Rds")) |>
  bind_rows(readRDS(here("cached_intermediates", "4_rt_dt_2.Rds"))) |>
  bind_rows(readRDS(here("cached_intermediates", "4_rt_dt_1.Rds")))

trad_launch <- d_rt_dt |>
  filter(frac == 1, window == 400, time_0, time_end) |>
  filter(shift_length <= 600) |>
  mutate(useable_rt = first_launch_rt) |>
  mutate(approach = "trad_launch") |>
  group_by(approach, administration_id, dataset_name, subject_id) |>
  summarize(mean_rt = mean(useable_rt), mean_log_rt = mean(log(useable_rt)), n_rts = n())


trad_land <- d_rt_dt |>
  filter(frac == 1, window == 400, time_0, time_end) |>
  mutate(useable_rt = land_rt) |>
  mutate(approach = "trad_land") |>
  group_by(approach, administration_id, dataset_name, subject_id) |>
  summarize(mean_rt = mean(useable_rt), mean_log_rt = mean(log(useable_rt)), n_rts = n())

new <- d_rt_dt |>
  filter(frac == 0, window == 400, time_0, time_end) |>
  mutate(useable_rt = land_rt) |>
  mutate(approach = "new") |>
  group_by(approach, administration_id, dataset_name, subject_id) |>
  summarize(mean_rt = mean(useable_rt), mean_log_rt = mean(log(useable_rt)), n_rts = n())

cdi_data <- readRDS(here("cached_intermediates", "1_cdi_subjects.Rds"))

for_cdi_corr <- trad_launch |>
  bind_rows(trad_land, new) |>
  pivot_longer(`mean_rt`:`mean_log_rt`, names_to = "logged", values_to = "rt") |>
  mutate(logged = case_when(
    logged == "mean_rt" ~ "raw",
    logged == "mean_log_rt" ~ "log"
  )) |>
  left_join(cdi_data)
```

```{r}
# cdi data
cdi_corr <- for_cdi_corr |>
  filter(!(is.na(prod) & is.na(comp))) |>
  filter(!is.na(rt)) |>
  group_by(dataset_name, approach, logged) |>
  summarise(
    cor_comp = ifelse(sum(!is.na(comp)) > 0, cor.test(rt, comp)$estimate, NA),
    cor_prod = ifelse(sum(!is.na(prod)) > 0, cor.test(rt, prod)$estimate, NA),
    cor_age = ifelse(sum(!is.na(age)) > 0, cor.test(rt, age)$estimate, NA),
    n_comp = sum(!is.na(comp)),
    n_prod = sum(!is.na(prod)),
    n_total = sum(!is.na(rt)),
    mean_trials = mean(n_rts)
  )
```

```{r}
cdi_corr |> ggplot(aes(x = str_c(approach, "_", logged), y = cor_prod)) +
  geom_point(aes(col = dataset_name)) +
  stat_summary() +
  coord_flip() +
  theme(legend.position = "none")

cdi_corr |> ggplot(aes(x = str_c(approach, "_", logged), y = cor_prod)) +
  geom_point(aes(col = dataset_name)) +
  coord_flip() +
  theme(legend.position = "none") +
  facet_wrap(~dataset_name)
```
negative correlations are expected -- kids who know more words are expected to be faster. 

some of these datasets just do not have very many kids or (valid) trials/kid -- that's certainly true of ferguson, which has 4-8 kids depending on condition; all others have more kids (28+)

now let's look at comprehension -- but we have comprehension data on many fewer kids/datasets

```{r}
cdi_corr |> ggplot(aes(x = str_c(approach, "_", logged), y = cor_comp)) +
  geom_point(aes(col = dataset_name)) +
  stat_summary() +
  coord_flip() +
  theme(legend.position = "none")

cdi_corr |> ggplot(aes(x = str_c(approach, "_", logged), y = cor_comp)) +
  geom_point(aes(col = dataset_name)) +
  coord_flip() +
  theme(legend.position = "none") +
  facet_wrap(~dataset_name)
```
and we can also check against age 
```{r}
cdi_corr |> ggplot(aes(x = str_c(approach, "_", logged), y = cor_age)) +
  geom_point(aes(col = dataset_name)) +
  stat_summary() +
  coord_flip() +
  theme(legend.position = "none")

cdi_corr |> ggplot(aes(x = str_c(approach, "_", logged), y = cor_age)) +
  geom_point(aes(col = dataset_name)) +
  coord_flip() +
  theme(legend.position = "none") +
  facet_wrap(~dataset_name)
```

again, ferguson is weird and not trustworthy, but otherwise, we're seeing pretty good consistency 

some by-dataset variability on whether log helps or not


## Compare with accuracy ICC

```{r}
load(file = "cached_intermediates/3_accs.Rds")

accs_best <- accs |>
  filter(t_start == 400, t_end == 4000) |>
  filter(object == "administration") |>
  select(dataset_name, icc) |>
  mutate(type = "acc")

rts_best <- readRDS(here("cached_intermediates", "4_rt_iccs_log.rds")) |>
  filter(frac == 0, measure == "land_rt") |>
  ungroup() |>
  select(dataset_name, icc) |>
  mutate(type = "rt")

accs_best |>
  bind_rows(rts_best) |>
  ggplot(aes(x = dataset_name, y = icc, col = type)) +
  geom_point() +
  coord_flip()

accs_best |>
  bind_rows(rts_best) |>
  ggplot(aes(x = type, y = icc, col = type)) +
  geom_point() +
  stat_summary() +
  coord_flip()
```
RT tends to have higher reliability than accuracy (whether that is *meaningful* reliability is another matter) 

## Summary

for further purposes are doing (raw) RT on a window=400, on distractor (target) at 0 and 400, not on the other one during those 400 ms, no other requirements

```{r}
canonical_rt <- readRDS(here("cached_intermediates", "4_rt_1.rds")) |>
  filter(window == 400, frac == 0, time_0 == T, time_end == T) |>
  left_join(d_aoi %>%
    select(administration_id, dataset_name, subject_id, trial_id, trial_order, target_label) %>%
    distinct())

saveRDS(canonical_rt, here("cached_intermediates", "4_rt_canonical.rds"))
```


# Old
<!--
## Computing RT 
First compute reaction time. 

NOTE 1/31/24 - consider computing RT from LAUNCH not from LANDING - this may make a difference to what we end up finding. 
MZ: 4/29/24 - both launch and landing-based RT computed by rt helper function
* `rt`: landing-time based RT
* `shift_start_rt`: launch-time based RT

We need RLE data, then we use the RT helper from peekbank-shiny. 

Note MCF: 8/25/25 - we cached this function in the current repo to remove dependencies. 



Compute RTs, relying on the RLE workflow from the shiny app. 



How many trials have RTs for them?

Almost every trial makes it through the computation, but what prop do we have RTs for.

```{r}
d_rt <- readRDS(file = here("cached_intermediates", "4_d_rt.Rds"))

# rt_stats <- d_rt %>%
#   ungroup() %>%
#   summarise(
#     nas = mean(is.na(shift_start_rt)),
#     too_fast = mean(shift_start_rt < 300, na.rm = TRUE),
#     d_t = mean(shift_type == "D-T", na.rm = TRUE),
#     t_d = mean(shift_type == "T-D", na.rm = TRUE),
#     other = mean(shift_type == "other", na.rm = TRUE),
#     no_shift = mean(shift_type == "no shift", na.rm = TRUE)
#   )
#
# knitr::kable(rt_stats, digits = 2)
```




## How to measure shift initiation

```{r}
# not sure whether we should count shift initiation as when the child first looks away from D or when they are last looking at D before looking at T

# how often are these distinct?
```
about 5% of the data that has a shift has a difference between these two measures
but only abut 1% of data with a shift has this difference *and* doesn't have a long shift time, so this is probably not of large consequence. 

```{r}
# does it matter which type of shift we use
ggplot(d_rt, aes(x = shift_start_rt, y = last_shift_rt)) +
  geom_point(alpha = .5, aes(col = age)) +
  geom_smooth(method = "lm") +
  viridis::scale_color_viridis()

ggplot(
  d_rt,
  aes(x = shift_start_rt, y = last_shift_rt)
) +
  geom_point(alpha = .5, aes(col = age)) +
  geom_point(
    data = filter(d_rt, shift_length > 600),
    col = "red"
  ) +
  geom_smooth(method = "lm") +
  viridis::scale_color_viridis()
```





## RT distribution & exclusion

Examine RT distribution.

```{r}
ggplot(filter(d_rt), aes(x = shift_start_rt)) +
  geom_histogram()
```

Logs. 

```{r}
ggplot(d_rt, aes(x = shift_start_rt)) +
  geom_histogram() +
  scale_x_log10()
```

Probably should get rid of the RTs < 250ms or so. 

```{r}
mean(d_rt$shift_start_rt < 250, na.rm = TRUE)
```

Filter. 

```{r}
d_rt_all <- filter(
  d_rt,
  !is.na(shift_start_rt)
)
d_rt <- filter(
  d_rt,
  !is.na(shift_start_rt),
  shift_start_rt > 250
)
```

Look by age.

```{r}
ggplot(
  d_rt,
  aes(x = age, y = rt)
) +
  geom_point(alpha = .5) +
  geom_smooth(method = "lm")
```
Add dataset to try to figure out blockiness. 

```{r}
ggplot(
  d_rt,
  aes(x = age, y = rt)
) +
  geom_point(alpha = .1) +
  geom_smooth(method = "lm") +
  facet_wrap(~dataset_name)
```

Histogram by dataset. 

```{r}
ggplot(
  d_rt,
  aes(x = rt)
) +
  geom_histogram(bins = 10) +
  scale_x_log10() +
  facet_wrap(~dataset_name, scales = "free_y")
```



## RT reliabilities




TODO: rank/score them for the different things 
Why are some ICCs zero? Let's look at Pomper Saffran 2016.

```{r}
ps <- d_rt |>
  filter(dataset_name == "pomper_saffran_2016")

ggplot(
  ps,
  aes(x = target_label, y = shift_start_rt)
) +
  geom_jitter(alpha = .5, width = .2) +
  stat_summary(col = "red") +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5))

ggplot(
  ps,
  aes(x = administration_id, y = shift_start_rt)
) +
  geom_jitter(alpha = .5, width = .2) +
  stat_summary(col = "red")
```

Now check ICCs for RTs for ALL trials (not subsetting to D-T trials). They look OK.

```{r}
# disaggregated
get_icc(ps, object = "stimulus", column = "shift_start_rt")
get_icc(ps, object = "administration", column = "shift_start_rt")
```


```{r}
ps_icc <- dim_icc(ps,
  model = "2A",
  type = "agreement",
  unit = "average",
  object = administration_id,
  rater = target_label,
  trial = trial_id,
  score = shift_start_rt,
  bootstrap = 1000
)

summary(ps_icc)
```

This all looks good and ICCs seem reasonably high - but why do we get fewer zeros when we subset to D-T trials? Let's dig into this. 

Pomper SalientMe shows this pattern. 

```{r}
ps_dt <- ps |>
  filter(shift_type == "D-T")

get_icc(ps, object = "stimulus", column = "shift_start_rt")
get_icc(ps_dt, object = "stimulus", column = "shift_start_rt")
```

Stimulus ICC goes to zero for D-T trials. Let's look at the cross between subjects and trials for each. 

```{r}
ps |>
  ungroup() |>
  select(subject_id, target_label, shift_start_rt) |>
  arrange(target_label) |>
  pivot_wider(names_from = "target_label", values_from = "shift_start_rt") |>
  arrange(subject_id) |>
  View()

# summarize by stimulus
ps_stimulus_rt_summarized <- ps |>
  ungroup() |>
  group_by(target_label) |>
  summarize(
    N = n(),
    mean_rt = mean(shift_start_rt, na.rm = TRUE)
  )

ps_stimulus_rt_summarized <- ps |>
  ungroup() |>
  group_by(target_label) |>
  summarize(
    N = n(),
    mean_rt = mean(shift_start_rt, na.rm = TRUE)
  )

ps_admin_rt_summarized <- ps |>
  ungroup() |>
  group_by(administration_id) |>
  summarize(
    N = n(),
    mean_rt = mean(shift_start_rt, na.rm = TRUE)
  )


ps_dt |>
  ungroup() |>
  select(subject_id, target_label, shift_start_rt) |>
  arrange(target_label) |>
  pivot_wider(names_from = "target_label", values_from = "shift_start_rt") |>
  arrange(subject_id) |>
  View()

# summarize by stimulus
ps_dt_stimulus_rt_summarized <- ps_dt |>
  ungroup() |>
  group_by(administration_id, target_label) |>
  summarize(
    N = n(),
    mean_rt = mean(shift_start_rt, na.rm = TRUE),
    unique_participants = length(unique(administration_id))
  )

ps_dt_admin_rt_summarized <- ps_dt |>
  ungroup() |>
  group_by(administration_id) |>
  summarize(
    N = n(),
    mean_rt = mean(shift_start_rt, na.rm = TRUE)
  )
```

So the D-T dataframe is sparser, but looks more consistent. Let's check out the distributions. 

Let's check if removing rare items fixes the ICC issue

```{r}
rare_items <- ps_dt_stimulus_rt_summarized |>
  filter(N < 8) %>%
  pull(target_label)
rare_participants <- ps_dt_admin_rt_summarized |>
  filter(N < 5) %>%
  pull(administration_id)

get_icc(filter(ps_dt, !(target_label %in% rare_items)), object = "stimulus", column = "shift_start_rt")
get_icc(filter(ps_dt, !(administration_id %in% rare_participants)), object = "stimulus", column = "shift_start_rt")
```

filtering out rare participants or stimuli doesn't seem to matter.


```{r}
ggplot(ps, aes(x = rt)) +
  geom_histogram() +
  facet_wrap(~shift_type)
```
This is consistent with the idea that T-D shifts are more random/ uninformative. Let's look at all data. 

```{r}
ggplot(d_rt, aes(x = rt)) +
  geom_histogram() +
  facet_wrap(~shift_type)
```
Looks well-supported that T-D RTs are different. I now feel comfortable moving forward with D-T only. 
Let's compare ICC from RTs to ICCs from accuracy. 

```{r}
d_summary <- d_trial |>
  group_by(
    dataset_name, trial_id, dataset_id, subject_id, administration_id,
    target_label
  ) |>
  summarise(
    accuracy = mean(correct[t_norm > 500], na.rm = TRUE),
    prop_data = mean(!is.na(correct[t_norm > 500]))
  ) |>
  filter(!is.na(accuracy))

trial_ns_acc <- bind_rows(
  d_summary |>
    group_by(dataset_name, subject_id) |>
    count() |>
    group_by(dataset_name) |>
    summarise(
      n = mean(n),
      dimension = "admin"
    ),
  d_summary |>
    group_by(dataset_name, target_label) |>
    count() |>
    group_by(dataset_name) |>
    summarise(
      n = mean(n),
      dimension = "stimulus"
    )
)

trial_ns_rt <- bind_rows(
  d_rt_dt |>
    group_by(dataset_name, subject_id) |>
    count() |>
    group_by(dataset_name) |>
    summarise(
      n = mean(n),
      dimension = "admin"
    ),
  bind_rows(d_rt_dt |>
    group_by(dataset_name, subject_id) |>
    count() |>
    group_by(dataset_name) |>
    summarise(
      n = mean(n),
      dimension = "stimulus"
    ))
)
# load ICCs for accuracy
load(file = here("cached_intermediates", "2_iccs_accuracy.Rds"))

iccs_long <- iccs |>
  pivot_longer(
    names_to = "dimension", values_to = "icc",
    icc_stimulus_acc:icc_admin_acc
  ) |>
  ungroup() |>
  separate(dimension, into = c("type", "dimension", "measure")) |>
  mutate(dataset_name = fct_reorder(dataset_name, icc))


acc_rt_iccs <- bind_rows(
  filter(iccs_long, measure == "acc") |>
    left_join(trial_ns_acc),
  rt_iccs_long |>
    left_join(trial_ns_rt)
) |>
  mutate(dataset_name = fct_reorder(as.factor(dataset_name), icc)) |>
  select(-type)


ggplot(
  acc_rt_iccs,
  aes(x = dataset_name, y = icc, col = measure)
) +
  geom_point(aes(size = n),
    position = position_dodge(width = .5)
  ) +
  geom_line(aes(group = measure)) +
  facet_wrap(~dimension) +
  theme(axis.text.x = element_text(angle = -90))

acc_rt_iccs |>
  arrange(dataset_name) |>
  mutate(
    icc = round(icc, digits = 2),
    n = round(n)
  )
```

Let's plot by N. 

```{r}
ggplot(
  acc_rt_iccs,
  aes(x = n, y = icc, col = dataset_name)
) +
  geom_point() +
  geom_smooth(aes(group = 1), method = "loess", span = 10, se = FALSE) +
  # scale_x_log10() +
  facet_wrap(dimension ~ measure, scales = "free_x") +
  xlab("N trials per child/word") +
  ylab("Intraclass Correlation Coefficient") +
  ylim(0, 1)
```
This is interesting! We are getting a bunch of signal about individual participants from RT, actually higher ICC than accuracies. Not so much for stimulus information, where it seems like we are doing better from accuracy. Also, as predicted the number of trials per child or per word appears to relate across datasets to the ICC (though there's lots of variance at the bottom end that presumably relates to the variation in ability across kids/variation in difficulty across words). If you choose very different words you get high reliability on that dimension (see "reliability paradoxes" idea).

## Compare ICCs for RT and Accuracy

```{r}
# combine
all_iccs <- iccs %>%
  left_join(rt_iccs)

# plot - Participants
ggplot(all_iccs, aes(admin_rt, icc_admin_acc)) +
  geom_point(size = 1.5) +
  geom_text(aes(label = dataset_name)) +
  geom_abline(intercept = 0, slope = 1) +
  xlim(0, 1) +
  ylim(0, 1)
```

plot - Items

```{r}
ggplot(all_iccs, aes(stimulus_rt, icc_stimulus_acc)) +
  geom_point(size = 1.5) +
  geom_text(aes(label = dataset_name)) +
  geom_abline(intercept = 0, slope = 1) +
  xlim(0, 1) +
  ylim(0, 1)
```

## Look at RTs as a function of filtering

```{r}
save(d_rt_dt, file = here("cached_intermediates", "4_d_rt_dt.Rds"))
```
