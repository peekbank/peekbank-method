---
title: "RT Computation"
author: "Mike Frank"
date: "2022-12-15"
output: html_document
---


```{r}
source(here::here("helper/common.R"))
```

# Load data

```{r}
load(file = here("cached_intermediates","1_d_trial.Rds"))
```

# Measure 3: Reaction time

## Computing RT 
First compute reaction time. 

NOTE 1/31/24 - consider computing RT from LAUNCH not from LANDING - this may make a difference to what we end up finding. 
MZ: 4/29/24 - both launch and landing-based RT computed by rt helper function
* `rt`: landing-time based RT
* `shift_start_rt`: launch-time based RT

We need RLE data, then we use the RT helper from peekbank-shiny. 

Note MCF: 8/25/25 - we cached this function in the current repo to remove dependencies. 

```{r}
source("helper/rt_helper.R")
```

Compute RTs, relying on the RLE workflow from the shiny app. 

```{r}
rle_data <- d_trial %>%
  filter(any(t_norm == 0), # must have data at 0
         t_norm >= 0) %>% # only pass data after 0
  group_by(administration_id, trial_id, trial_order) %>%
  reframe(lengths = rle(aoi)$lengths, 
          values = rle(aoi)$values) 

d_rt <- rle_data %>%
  group_by(administration_id, trial_id, trial_order) %>%
  nest() %>%
  mutate(data = lapply(data, get_rt)) %>%
  unnest(cols = c(data)) %>%
  left_join(d_trial %>%
              select(-t_norm, -correct, -aoi) %>%
              distinct())

saveRDS(file = here("cached_intermediates","4_d_rt.Rds"), d_rt)
```

How many trials have RTs for them?

Almost every trial makes it through the computation, but what prop do we have RTs for.

```{r}
d_rt <- readRDS(file = here("cached_intermediates","4_d_rt.Rds"))

rt_stats <- d_rt %>% 
  ungroup() %>%
  summarise(nas = mean(is.na(shift_start_rt)), 
            too_fast = mean(shift_start_rt < 300, na.rm=TRUE), 
            d_t = mean(shift_type == "D-T", na.rm=TRUE), 
            t_d = mean(shift_type == "T-D", na.rm=TRUE),
            other = mean(shift_type == "other", na.rm=TRUE),
            no_shift = mean(shift_type == "no shift", na.rm=TRUE))

knitr::kable(rt_stats, digits = 2)
```

## Shift-start vs. regular RT

```{r}
ggplot(d_rt, aes(x = shift_start_rt, y = rt)) + 
  geom_point(alpha = .5, aes(col = age)) + 
  geom_smooth(method = "lm") + 
  viridis::scale_color_viridis()
```

What if we took out the too-long shifts (which Virginia says were filtered after 600ms)?

```{r}
ggplot(d_rt, 
       aes(x = shift_start_rt, y = rt)) + 
  geom_point(alpha = .5, aes(col = age)) + 
  geom_point(data = filter(d_rt, shift_length > 600), 
             col = "red") +
  geom_smooth(method = "lm") + 
  viridis::scale_color_viridis()
```



## RT distribution & exclusion

Examine RT distribution.

```{r}
ggplot(filter(d_rt,shift_start_rt>=300), aes(x = shift_start_rt)) + 
  geom_histogram()
```

Logs. 

```{r}
ggplot(d_rt, aes(x = shift_start_rt)) + 
  geom_histogram() +
  scale_x_log10()
```

Probably should get rid of the RTs < 250ms or so. 

```{r}
mean(d_rt$shift_start_rt<250, na.rm=TRUE)
```

Filter. 

```{r}
d_rt_all <- filter(d_rt, 
                   !is.na(shift_start_rt))
d_rt <- filter(d_rt, 
               !is.na(shift_start_rt), 
               shift_start_rt > 250)
```

Look by age.

```{r}
ggplot(d_rt, 
       aes(x = age, y = rt)) + 
  geom_point(alpha = .5) +
  geom_smooth(method="lm") 
```
Add dataset to try to figure out blockiness. 

```{r}
ggplot(d_rt, 
       aes(x = age, y = rt)) + 
  geom_point(alpha = .1) +
  geom_smooth(method="lm") + 
  facet_wrap(~dataset_name)
```

Histogram by dataset. 

```{r}
ggplot(d_rt, 
       aes(x = rt)) + 
  geom_histogram(bins = 10) +
  scale_x_log10() +
  facet_wrap(~dataset_name, scales = "free_y")
```



## RT reliabilities

Let's compute reliabilities now for D-T trials (standard approach, loses half of trials). 

```{r}
d_rt_dt <- d_rt |>
  filter(shift_type == "D-T") |>
  mutate(log_rt = log(rt), 
         shift_start_log_rt = log(shift_start_rt), 
         shift_start_rt_trimmed = ifelse(shift_length <= 600, shift_start_rt, NA)) 

# stimulus_rt = 
#            unlist(map(data, ~get_icc(.x, 
#                                      column = "shift_start_rt",
#                                      object = "stimulus"))),
# landing_stimulus_rt = 
#   unlist(map(data, ~get_icc(.x, 
#                             column = "rt",
#                             object = "stimulus"))),

rt_iccs <- d_rt_dt |>
  group_by(dataset_name) |> 
  nest() |>
  mutate(
    shift_rt = 
      unlist(map(data, ~get_icc(.x, 
                                column = "shift_start_rt",
                                object = "administration"))),
    shift_logrt = 
      unlist(map(data, ~get_icc(.x, 
                                column = "shift_start_log_rt",
                                object = "administration"))),
    shift_trimmed = 
      unlist(map(data, ~get_icc(.x, 
                                column = "shift_start_rt_trimmed",
                                object = "administration"))),
    landing_rt = 
      unlist(map(data, ~get_icc(.x, 
                                column = "rt",
                                object = "administration"))),
    landing_logrt = 
      unlist(map(data, ~get_icc(.x, 
                                column = "log_rt",
                                object = "administration")))) |>
  select(-data) |>
  unnest(cols = c())

rt_iccs_long <- rt_iccs |>
  pivot_longer(names_to = "measure", values_to = "icc", 
               shift_rt:landing_logrt)
```

Plot. 

```{r}
rt_iccs_long$dataset_name <- fct_reorder(rt_iccs_long$dataset_name, rt_iccs_long$icc)

ggplot(rt_iccs_long,
       aes(x = dataset_name, y = icc, col = measure)) +
  geom_point(position = position_dodge(width = .5)) +
  geom_line(aes(group= measure))+ 
  coord_flip() 
```

Why are some ICCs zero? Let's look at Pomper Saffran 2016.

```{r}
ps <- d_rt |> 
  filter(dataset_name == "pomper_saffran_2016")

ggplot(ps, 
       aes(x = target_label, y = shift_start_rt)) +
  geom_jitter(alpha = .5, width = .2) + 
  stat_summary(col = "red")+
  theme(axis.text.x  = element_text(angle=90, vjust=0.5))

ggplot(ps, 
       aes(x = administration_id, y = shift_start_rt)) +
  geom_jitter(alpha = .5, width = .2) + 
  stat_summary(col = "red")
```

Now check ICCs for RTs for ALL trials (not subsetting to D-T trials). They look OK.

```{r}
# disaggregated
get_icc(ps, object = "stimulus", column = "shift_start_rt")
get_icc(ps, object = "administration", column = "shift_start_rt")
```


```{r}
ps_icc <- dim_icc(ps, 
                  model = "2A", 
                  type = "agreement", 
                  unit = "average",
                  object = administration_id, 
                  rater = target_label,
                  trial = trial_id, 
                  score = shift_start_rt, 
                  bootstrap = 1000)

summary(ps_icc)
```

This all looks good and ICCs seem reasonably high - but why do we get fewer zeros when we subset to D-T trials? Let's dig into this. 

Pomper SalientMe shows this pattern. 

```{r}
ps_dt <- ps |>
  filter(shift_type == "D-T")

get_icc(ps, object = "stimulus", column = "shift_start_rt")
get_icc(ps_dt, object = "stimulus", column = "shift_start_rt")
```

Stimulus ICC goes to zero for D-T trials. Let's look at the cross between subjects and trials for each. 

```{r}
ps |> 
  ungroup() |>
  select(subject_id, target_label, shift_start_rt) |>
  arrange(target_label) |>
  pivot_wider(names_from = "target_label", values_from = "shift_start_rt") |>
  arrange(subject_id) |>
  View()

#summarize by stimulus
ps_stimulus_rt_summarized <- ps |>
  ungroup() |>
  group_by(target_label) |>
  summarize(
    N=n(),
    mean_rt=mean(shift_start_rt,na.rm=TRUE)
  )

ps_stimulus_rt_summarized <- ps |>
  ungroup() |>
  group_by(target_label) |>
  summarize(
    N=n(),
    mean_rt=mean(shift_start_rt,na.rm=TRUE)
  )

ps_admin_rt_summarized <- ps |>
  ungroup() |>
  group_by(administration_id) |>
  summarize(
    N=n(),
    mean_rt=mean(shift_start_rt,na.rm=TRUE)
  )


ps_dt |> 
  ungroup() |>
  select(subject_id, target_label, shift_start_rt) |>
  arrange(target_label) |>
  pivot_wider(names_from = "target_label", values_from = "shift_start_rt") |>
  arrange(subject_id) |>
  View()

#summarize by stimulus
ps_dt_stimulus_rt_summarized <- ps_dt |>
  ungroup() |>
  group_by(administration_id,target_label) |>
  summarize(
    N=n(),
    mean_rt=mean(shift_start_rt,na.rm=TRUE),
    unique_participants=length(unique(administration_id))
  )

ps_dt_admin_rt_summarized <- ps_dt |>
  ungroup() |>
  group_by(administration_id) |>
  summarize(
    N=n(),
    mean_rt=mean(shift_start_rt,na.rm=TRUE)
  )
```

So the D-T dataframe is sparser, but looks more consistent. Let's check out the distributions. 

Let's check if removing rare items fixes the ICC issue

```{r}
rare_items <- ps_dt_stimulus_rt_summarized |>
  filter(N<8) %>%
  pull(target_label)
rare_participants <- ps_dt_admin_rt_summarized |>
  filter(N<5) %>%
  pull(administration_id)

get_icc(filter(ps_dt,!(target_label %in% rare_items)), object = "stimulus", column = "shift_start_rt")
get_icc(filter(ps_dt,!(administration_id %in% rare_participants)), object = "stimulus", column = "shift_start_rt")
```

filtering out rare participants or stimuli doesn't seem to matter.


```{r}
ggplot(ps, aes(x = rt)) + 
  geom_histogram() + 
  facet_wrap(~shift_type)
```
This is consistent with the idea that T-D shifts are more random/ uninformative. Let's look at all data. 

```{r}
ggplot(d_rt, aes(x = rt)) + 
  geom_histogram() + 
  facet_wrap(~shift_type)
```
Looks well-supported that T-D RTs are different. I now feel comfortable moving forward with D-T only. 
Let's compare ICC from RTs to ICCs from accuracy. 

```{r}
d_summary <- d_trial |>
  group_by(dataset_name, trial_id, dataset_id, subject_id, administration_id, 
           target_label) |>
  summarise(accuracy = mean(correct[t_norm > 500], na.rm=TRUE),
            prop_data = mean(!is.na(correct[t_norm > 500]))) |>
  filter(!is.na(accuracy))

trial_ns_acc <- bind_rows(d_summary |>
                            group_by(dataset_name, subject_id) |>
                            count() |>
                            group_by(dataset_name) |>
                            summarise(n = mean(n), 
                                      dimension = "admin"),
                          d_summary |>
                            group_by(dataset_name, target_label) |>
                            count() |>
                            group_by(dataset_name) |>
                            summarise(n = mean(n), 
                                      dimension = "stimulus"))

trial_ns_rt <- bind_rows(d_rt_dt |>
                           group_by(dataset_name, subject_id) |>
                           count() |>
                           group_by(dataset_name) |>
                           summarise(n = mean(n), 
                                     dimension = "admin"),
                         bind_rows(d_rt_dt |>
                                     group_by(dataset_name, subject_id) |>
                                     count() |>
                                     group_by(dataset_name) |>
                                     summarise(n = mean(n), 
                                               dimension = "stimulus")))
#load ICCs for accuracy
load(file = here("cached_intermediates","2_iccs_accuracy.Rds"))

iccs_long <- iccs |>
  pivot_longer(names_to = "dimension", values_to = "icc", 
               icc_stimulus_acc:icc_admin_acc) |>
  ungroup() |>
  separate(dimension, into = c("type","dimension","measure")) |>
  mutate(dataset_name = fct_reorder(dataset_name, icc))


acc_rt_iccs <- bind_rows(filter(iccs_long, measure == "acc") |>
                           left_join(trial_ns_acc), 
                         rt_iccs_long |>
                           left_join(trial_ns_rt)) |>
  mutate(dataset_name = fct_reorder(as.factor(dataset_name), icc)) |>
  select(-type)


ggplot(acc_rt_iccs, 
       aes(x = dataset_name, y = icc, col = measure)) +
  geom_point(aes(size = n), 
             position = position_dodge(width = .5)) +
  geom_line(aes(group = measure)) + 
  facet_wrap(~dimension) +
  theme(axis.text.x=element_text(angle=-90))

acc_rt_iccs |> 
  arrange(dataset_name) |>
  mutate(icc = round(icc, digits = 2), 
         n = round(n))
```

Let's plot by N. 

```{r}
ggplot(acc_rt_iccs, 
       aes(x = n, y = icc, col = dataset_name)) + 
  geom_point() + 
  geom_smooth(aes(group = 1), method = "loess", span = 10,  se = FALSE) + 
  # scale_x_log10() +
  facet_wrap(dimension~measure, scales = "free_x") + 
  xlab("N trials per child/word") + 
  ylab("Intraclass Correlation Coefficient") + 
  ylim(0,1)

```
This is interesting! We are getting a bunch of signal about individual participants from RT, actually higher ICC than accuracies. Not so much for stimulus information, where it seems like we are doing better from accuracy. Also, as predicted the number of trials per child or per word appears to relate across datasets to the ICC (though there's lots of variance at the bottom end that presumably relates to the variation in ability across kids/variation in difficulty across words). If you choose very different words you get high reliability on that dimension (see "reliability paradoxes" idea).

## Compare ICCs for RT and Accuracy

```{r}
#combine
all_iccs <- iccs %>%
  left_join(rt_iccs)

#plot - Participants
ggplot(all_iccs, aes(admin_rt,icc_admin_acc))+
  geom_point(size=1.5)+
  geom_text(aes(label=dataset_name))+
  geom_abline(intercept=0,slope=1)+
  xlim(0,1)+
  ylim(0,1)
```

plot - Items

```{r}
ggplot(all_iccs, aes(stimulus_rt,icc_stimulus_acc))+
  geom_point(size=1.5)+
  geom_text(aes(label=dataset_name))+
  geom_abline(intercept=0,slope=1)+
  xlim(0,1)+
  ylim(0,1)
```

## Look at RTs as a function of filtering

```{r}
save(d_rt_dt, file= here("cached_intermediates","4_d_rt_dt.Rds"))
```
