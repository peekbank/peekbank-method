---
title: "Baseline correction"
author: "Mike Frank"
date: "2022-12-15"
output: html_document
---



```{r}
source(here::here("helper/common.R"))

d_aoi <- readRDS(here("cached_intermediates", "1_d_aoi.Rds"))
```




# Baseline-corrected accuracy

the idea with baseline-corrected accuracy is that due to saliency effects (etc etc), accuracy measures might incorporate both kids word knowledge & whatever item properties, and we might be able to get the kids word knowledge more precisely by subtracting out the "accuracy" from the pre-stimulus time. 

* note that there might be a thing where baseline corrected accuracy is especially 

we'll want to look by dataset, since datasets vary widely in how much pre--looking time there is!

```{r}
ggplot(d_aoi, aes(x = t_norm)) +
  geom_histogram() +
  facet_wrap(~dataset_name, scales = "free_y") +
  geom_vline(xintercept = 0) +
  geom_vline(xintercept = 400)
```

let's make a filtered set for datasets that have sub -2000 RTs (long baselines) and start there, since those will be the *most* likely to have useful baseline correction. 

## ICC
we will want to repeat this for other datasets later
```{r}
d_long_baseline_datasets <- d_aoi |>
  filter(t_norm == -3000) |>
  group_by(dataset_name) |>
  tally()

d_long_baseline <- d_aoi |> inner_join(d_long_baseline_datasets |> select(dataset_name))

acc <- d_long_baseline |>
  group_by(
    trial_id, subject_id, administration_id,
    target_label, dataset_name
  ) |>
  summarise(
    baseline_0 = mean(correct[t_norm < 0], na.rm = TRUE),
    baseline_400 = mean(correct[t_norm < 400], na.rm = TRUE),
    accuracy = mean(correct[t_norm > 400], na.rm = TRUE),
    bc_0_accuracy = accuracy - baseline_0,
    bc_400_accuracy = accuracy - baseline_400
  ) |>
  filter(!is.na(accuracy), !is.na(bc_0_accuracy), !is.na(bc_400_accuracy))


icc <- acc |>
  group_by(dataset_name) |>
  nest() |>
  mutate(
    bc_0_acc = map(data, \(d) get_icc(d, column = "bc_0_accuracy", object = "administration")),
    bc_400_acc = map(data, \(d) get_icc(d, column = "bc_400_accuracy", object = "administration")),
    acc = map(data, \(d) get_icc(d, column = "accuracy", object = "administration"))
  ) |>
  unnest(bc_400_acc) |>
  unnest(bc_0_acc) |>
  unnest(acc) |>
  select(-data) |>
  pivot_longer(c("bc_400_acc", "bc_0_acc", "acc"), names_to = "type", values_to = "icc")

ggplot(icc, aes(x = dataset_name, y = icc, col = type)) +
  geom_point() +
  coord_flip()
```
so, on these datasets, we do get higher icc from baseline corrections (on most of the datasets). -it's also unclear if this is *useful* signal or artefact. 

*but* these were specifically selected for long starting windows -- what if we do it to all the datasets?

```{r}
acc <- d_aoi |>
  group_by(
    trial_id, subject_id, administration_id,
    target_label, dataset_name
  ) |>
  summarise(
    baseline_0 = mean(correct[t_norm < 0], na.rm = TRUE),
    baseline_400 = mean(correct[t_norm < 400], na.rm = TRUE),
    accuracy = mean(correct[t_norm > 400], na.rm = TRUE),
    bc_0_accuracy = accuracy - baseline_0,
    bc_400_accuracy = accuracy - baseline_400
  )

icc <- acc |>
  group_by(dataset_name) |>
  nest() |>
  filter(!dataset_name %in% c("adams_marchman_2018", "fernald_marchman_2012")) |> # don't crash, repeat on cluster later
  mutate(
    bc_0_acc = map(data, \(d) get_icc(d |> filter(!is.na("bc_0_accuracy")), column = "bc_0_accuracy", object = "administration")),
    bc_400_acc = map(data, \(d) get_icc(d |> filter(!is.na("bc_400_accuracy")), column = "bc_400_accuracy", object = "administration")),
    acc = map(data, \(d) get_icc(d |> filter(!is.na("accuracy")), column = "accuracy", object = "administration"))
  ) |>
  unnest(bc_400_acc) |>
  unnest(bc_0_acc) |>
  unnest(acc) |>
  select(-data) |>
  pivot_longer(c("bc_400_acc", "bc_0_acc", "acc"), names_to = "type", values_to = "icc")

ggplot(icc, aes(x = type, y = icc, col = type)) +
  geom_point() +
  coord_flip() +
  stat_summary()

ggplot(icc, aes(x = dataset_name, y = icc, col = type)) +
  geom_jitter(width = .3) +
  coord_flip()
```
so, looks like the benefits of baseline correction would only apply to datasets with a lot of baseline. 

```{r}
baseline_amount <- d_aoi |>
  filter(t_norm <= 0, !is.na(correct)) |>
  group_by(dataset_name) |>
  summarize(mean_pre = mean(t_norm, na.rm = T))

icc |>
  left_join(baseline_amount) |>
  ggplot(aes(x = mean_pre, y = icc, col = type)) +
  geom_point() +
  geom_smooth(method = "lm")
```
dataset reliability from other things potentially confounds with how long they chose to window, so we could also look at diffs. 

```{r}
icc |>
  left_join(baseline_amount) |>
  pivot_wider(names_from = "type", values_from = "icc") |>
  mutate(
    benefit_pre_0 = bc_0_acc - acc,
    benefit_pre_400 = bc_400_acc - acc
  ) |>
  pivot_longer(c("benefit_pre_0", "benefit_pre_400"), names_to = "type", values_to = "diff_icc") |>
  ggplot(aes(x = mean_pre, y = diff_icc, col = type)) +
  geom_point() +
  geom_smooth(method = "lm") +
  geom_hline(yintercept = 0)
```
so, basically, if you have a long enough pre-window, such that the mean pre-0 value is -1500ish (so pre-window of 3000+), then baseline correcting might get you *slightly* higher reliability. 

## CDI

so, in addition to caring about reliability (via ICC), we *also* care about whether this matches up with CDI measures.

```{r}
cdi <- readRDS(here("cached_intermediates", "1_cdi_subjects.Rds"))
```

```{r}
cdi_corr <- acc |>
  group_by(dataset_name, administration_id) |>
  summarize(across(c("accuracy", "bc_0_accuracy", "bc_400_accuracy"), \(a) mean(a, na.rm = T))) |>
  left_join(cdi) |>
  group_by(dataset_name) |>
  summarise(
    cor_comp_acc = ifelse(sum(!is.na(comp) & !is.na(accuracy)) > 2, cor.test(accuracy, comp)$estimate, NA),
    cor_comp_bc_0 = ifelse(sum(!is.na(comp) & !is.na(bc_0_accuracy)) > 2, cor.test(bc_0_accuracy, comp)$estimate, NA),
    cor_comp_bc_400 = ifelse(sum(!is.na(comp) & !is.na(bc_400_accuracy)) > 2, cor.test(bc_400_accuracy, comp)$estimate, NA),
    cor_prod_acc = ifelse(sum(!is.na(prod) & !is.na(accuracy)) > 2, cor.test(accuracy, prod)$estimate, NA),
    cor_prod_bc_0 = ifelse(sum(!is.na(prod) & !is.na(bc_0_accuracy)) > 2, cor.test(bc_0_accuracy, prod)$estimate, NA),
    cor_prod_bc_400 = ifelse(sum(!is.na(prod) & !is.na(bc_400_accuracy)) > 2, cor.test(bc_400_accuracy, prod)$estimate, NA),
    cor_age_acc = ifelse(sum(!is.na(prod) & !is.na(accuracy)) > 2, cor.test(accuracy, age)$estimate, NA),
    cor_age_bc_0 = ifelse(sum(!is.na(prod) & !is.na(bc_0_accuracy)) > 2, cor.test(bc_0_accuracy, age)$estimate, NA),
    cor_age_bc_400 = ifelse(sum(!is.na(prod) & !is.na(bc_400_accuracy)) > 2, cor.test(bc_400_accuracy, age)$estimate, NA),
  ) |>
  left_join(baseline_amount)
```

first look across different dataset at how the correlations line up -- generally the unbaseline corrected seems strongest

```{r}
cdi_corr_long |>
  filter(str_detect(type, "prod")) |>
  ggplot(aes(x = reorder(dataset_name, mean_pre), y = cor, col = type)) +
  geom_jitter() +
  geom_hline(yintercept = 0) +
  coord_flip()

cdi_corr_long |>
  filter(str_detect(type, "comp")) |>
  ggplot(aes(x = reorder(dataset_name, mean_pre), y = cor, col = type)) +
  geom_jitter() +
  geom_hline(yintercept = 0) +
  coord_flip()

cdi_corr_long |>
  filter(str_detect(type, "age")) |>
  ggplot(aes(x = reorder(dataset_name, mean_pre), y = cor, col = type)) +
  geom_jitter() +
  geom_hline(yintercept = 0) +
  coord_flip()
```


we can also look specifically at how much better (or worse) the baseline corrected do, which helps visualize across different dataset correlation strengths 
```{r}
cdi_corr |>
  mutate(
    prod_benefit_0 = cor_prod_bc_0 - cor_prod_acc,
    prod_benefit_400 = cor_prod_bc_400 - cor_prod_acc
  ) |>
  pivot_longer(c("prod_benefit_0", "prod_benefit_400"), names_to = "type", values_to = "diff_cdi_prod") |>
  ggplot(aes(x = mean_pre, y = diff_cdi_prod, col = type)) +
  geom_point() +
  geom_hline(yintercept = 0) +
  geom_smooth(method = "lm")

cdi_corr |>
  mutate(
    comp_benefit_0 = cor_comp_bc_0 - cor_comp_acc,
    comp_benefit_400 = cor_comp_bc_400 - cor_comp_acc
  ) |>
  pivot_longer(c("comp_benefit_0", "comp_benefit_400"), names_to = "type", values_to = "diff_cdi_comp") |>
  ggplot(aes(x = mean_pre, y = diff_cdi_comp, col = type)) +
  geom_point() +
  geom_hline(yintercept = 0) +
  geom_smooth(method = "lm")

cdi_corr |>
  mutate(
    age_benefit_0 = cor_age_bc_0 - cor_age_acc,
    age_benefit_400 = cor_age_bc_400 - cor_age_acc
  ) |>
  pivot_longer(c("age_benefit_0", "age_benefit_400"), names_to = "type", values_to = "diff_age") |>
  ggplot(aes(x = mean_pre, y = diff_age, col = type)) +
  geom_point() +
  geom_hline(yintercept = 0) +
  geom_smooth(method = "lm")

cdi_corr_long <- cdi_corr |> pivot_longer(starts_with("cor"), names_to = "type", values_to = "cor")
```

## Summary

For datasets with a lot of pre-onset data (where we have -3000 ms), reliability is slightly increased by doing baseline correction, but there is no corresponding benefit (and potentially a detriment) to validity. 

For datasets with shorter baseline periods, baseline correction is worse for ICC and shows no benefit for validity measures. 

Takeaway: don't baseline correct?

# Old

# (to update/revisit) Baseline-corrected accuracy

## ICC approach

```{r}
icc_bc_window_sim <- function(b_start = -2000, b_end = 0,
                              t_start = 500, t_end = 4000,
                              object) {
  # get trials with some baseline
  baseline_lengths <- d_aoi |>
    group_by(dataset_name, trial_id) |>
    summarise(t_min = min(t_norm))

  # get baseline corrected accuracies for all trials with ANY baseline info
  df <- d_aoi |>
    left_join(baseline_lengths) |>
    filter(t_min < 0) |>
    group_by(dataset_name, dataset_id, administration_id, target_label, trial_id) |>
    summarise(
      window_accuracy = mean(correct[t_norm >= t_start & t_norm <= t_end],
        na.rm = TRUE
      ),
      baseline_accuracy = mean(correct[t_norm >= b_start & t_norm <= b_end],
        na.rm = TRUE
      ),
      bc_accuracy = window_accuracy - baseline_accuracy
    ) |>
    filter(!is.na(bc_accuracy))

  # compute ICCs
  df |>
    group_by(dataset_name) |>
    nest() |>
    mutate(icc = unlist(map(data, ~ get_icc(., "bc_accuracy", object)))) |>
    select(-data) |>
    unnest(cols = c())
}
```

```{r}
bc_acc_params <- expand_grid(
  t_start = seq(500, 1500, 500),
  t_end = seq(2000, 4000, 500),
  b_start = seq(-2000, -1000, 500),
  b_end = seq(-500, 0, 500),
  object = c("stimulus", "administration")
)

# multidyplr attempt
cluster <- new_cluster(14)
cluster_library(cluster, "tidyverse")
cluster_library(cluster, "agreement")
cluster_copy(cluster, "icc_bc_window_sim")
cluster_copy(cluster, "get_icc")
cluster_copy(cluster, "d_aoi")

tic()
bc_accs <- bc_acc_params |>
  partition(cluster) |>
  mutate(icc = pmap(list(b_start, b_end, t_start, t_end, object), icc_bc_window_sim)) |>
  collect() |>
  unnest(col = icc)
toc()

save(file = "cached_intermediates/3_bc_accs.Rds", bc_accs)
```

```{r}
load(file = "cached_intermediates/3_bc_accs.Rds")
```

Summary data frame. 

```{r}
bc_accs_summary <- bc_accs |>
  group_by(b_start, b_end, t_start, t_end, object) |>
  summarize(
    N = n(),
    mean_icc = mean(icc, na.rm = TRUE)
  ) |>
  mutate(
    window_size = t_end - t_start,
    bc_window_size = b_end - b_start
  )
```


```{r}
ggplot(
  bc_accs_summary,
  aes(color = mean_icc)
) +
  geom_vline(xintercept = 0, linetype = "dashed") +
  geom_segment(aes(x = t_start, xend = t_end, y = mean_icc, yend = mean_icc)) +
  geom_segment(aes(x = t_start, xend = t_start, y = mean_icc - 0.005, yend = mean_icc + 0.005)) +
  geom_segment(aes(x = t_end, xend = t_end, y = mean_icc - 0.005, yend = mean_icc + 0.005)) +
  scale_color_viridis(name = "Mean ICC", direction = -1) +
  theme(legend.position = "none") +
  ylab("Mean ICC") +
  xlab("Analysis Window (in ms)") +
  facet_grid(bc_window_size ~ object)
```
These numbers seem low. 

Compare to non-corrected accuracies. 

```{r}
load(file = "cached_intermediates/3_accs.Rds")

accs_summary <- accs |>
  group_by(t_start, t_end, object) |>
  summarize(mean_uncorrected_icc = mean(icc, na.rm = TRUE))

comparison_accs_summary <- left_join(bc_accs_summary, accs_summary)
```

```{r}
ggplot(comparison_accs_summary, aes(x = mean_uncorrected_icc, y = mean_icc, col = window_size)) +
  geom_point() +
  geom_smooth(method = "lm") +
  geom_abline(lty = 2) +
  # xlim(0,.6) + ylim(0,.6) +
  facet_grid(bc_window_size ~ object)
```
Possible interpretation (pace Dan Swingley): baselines are short and noisy. By adding them into the signal, you are diluting what you have. 

Note that we don't have a lot of baseline data here (no more than 2s) so we can't rule out the idea that a longer (4s or so) baseline would be meaningful or useful in a correction analysis. But even with 2s we see substantial reductions in ICC. 

## CDI validity with baseline correction

```{r}
cdi_bc_sim <- function(t_start = 500, t_end = 4000) {
  # get trials with some baseline
  baseline_lengths <- vanilla_cdi_datasets |>
    group_by(dataset_name, trial_id) |>
    summarise(t_min = min(t_norm))

  # get baseline corrected accuracies for all trials with ANY baseline info
  by_subject_accuracies <- vanilla_cdi_datasets |>
    left_join(baseline_lengths) |>
    filter(t_min < 0) |>
    group_by(dataset_name, lab_subject_id, trial_id) |>
    summarize(
      window_accuracy = mean(correct[t_norm >= t_start & t_norm <= t_end],
        na.rm = TRUE
      ),
      baseline_accuracy = mean(correct[t_norm >= -2000 & t_norm <= 0],
        na.rm = TRUE
      ),
      bc_accuracy = window_accuracy - baseline_accuracy
    )

  mean_accuracies <- by_subject_accuracies |>
    group_by(dataset_name, lab_subject_id) |>
    summarize(bc_accuracy = mean(bc_accuracy)) |>
    left_join(cdis)

  mean_accuracies |>
    group_by(dataset_name) |>
    summarise(
      cor_comp = cor.test(bc_accuracy, eng_wg_understood)$estimate,
      cor_prod = cor.test(bc_accuracy, eng_wg_produced)$estimate
    )
}
```

```{r}
cdi_params <- expand_grid(
  t_start = seq(500, 1500, 500),
  t_end = seq(2000, 4000, 500)
)
tic()
cdi_bc_corrs <- cdi_params |>
  mutate(icc = pmap(list(t_start, t_end), cdi_bc_sim)) |>
  unnest(col = icc)
toc()

cdi_bc_corrs <- cdi_bc_corrs |>
  pivot_longer(names_to = "measure", values_to = "r", starts_with("cor"))
```

Visualize!


```{r}
ggplot(cdi_bc_corrs, aes(col = r)) +
  geom_vline(xintercept = 0, linetype = "dashed") +
  geom_hline(yintercept = 0.05, lty = 3) +
  geom_segment(aes(x = t_start, xend = t_end, y = r, yend = r)) +
  # geom_segment(aes(x=t_start,xend=t_start,y=mean_icc-0.005,yend=mean_icc+0.005))+
  # geom_segment(aes(x=t_end,xend=t_end,y=mean_icc-0.005,yend=mean_icc+0.005))+
  scale_color_viridis(name = "Mean ICC", direction = -1) +
  # scale_y_log10() +
  theme(legend.position = "none") +
  ylab("Correlation with MB-CDI sumscore") +
  xlab("Analysis Window (in ms)") +
  facet_grid(dataset_name ~ measure)
```

Compare to uncorrected. 

```{r}
cdi_comparison <- left_join(
  rename(cdi_bc_corrs, r_baseline_corrected = r),
  rename(cdi_corrs, r_uncorrected = r)
) |>
  mutate(window_size = t_end - t_start)

ggplot(cdi_comparison, aes(x = r_uncorrected, y = r_baseline_corrected, col = window_size)) +
  geom_point() +
  geom_smooth(method = "lm") +
  geom_abline(lty = 2) +
  facet_wrap(~dataset_name)
```

This is overall surprising. For Swingley & Aslin, baseline correction makes things look way worse. But for Garrison & Bergelson, it seems like baseline correction helps. 



```{r}
am <- d_aoi |>
  filter(dataset_name == "adams_marchman_2018") |>
  group_by(
    trial_id, subject_id, administration_id,
    target_label
  ) |>
  summarise(
    baseline = mean(correct[t_norm < 400], na.rm = TRUE),
    accuracy = mean(correct[t_norm > 400], na.rm = TRUE),
    bc_accuracy = accuracy - baseline
  ) |>
  filter(!is.na(accuracy), !is.na(bc_accuracy))

ggplot(
  am,
  aes(x = accuracy)
) +
  geom_histogram()

ggplot(
  am,
  aes(x = bc_accuracy)
) +
  geom_histogram()
```

Now check ICCs.

```{r}
# disaggregated
get_icc(am, column = "accuracy", object = "stimulus")
get_icc(am, column = "accuracy", object = "administration")
get_icc(am, column = "bc_accuracy", object = "stimulus")
get_icc(am, column = "bc_accuracy", object = "administration")
```

OK, so for this dataset it seems like within-trial baseline correction is **reducing** reliability for both stimuli and administrations. AM2018 still has relatively high reliability (in contrast to others). Let's try the SA dataset we were looking at before. 

```{r}
sa <- d_trial |>
  filter(dataset_name == "swingley_aslin_2002") |>
  group_by(
    trial_id, subject_id, administration_id,
    target_label
  ) |>
  summarise(
    baseline = mean(correct[t_norm < 500], na.rm = TRUE),
    accuracy = mean(correct[t_norm > 500], na.rm = TRUE),
    bc_accuracy = accuracy - baseline
  ) |>
  filter(!is.na(accuracy))

ggplot(
  am,
  aes(x = accuracy)
) +
  geom_histogram()

ggplot(
  am,
  aes(x = bc_accuracy)
) +
  geom_histogram()

# disaggregated
get_icc(sa, column = "accuracy", object = "stimulus")
get_icc(sa, column = "accuracy", object = "administration")
get_icc(sa, column = "bc_accuracy", object = "stimulus")
get_icc(sa, column = "bc_accuracy", object = "administration")
```

Weirdly it looks like the reverse is happening. Let's get more systematic. 

```{r}
d_summary <- d_trial |>
  group_by(
    dataset_name, trial_id, subject_id, administration_id,
    target_label
  ) |>
  summarise(
    baseline = mean(correct[t_norm < 500], na.rm = TRUE),
    accuracy = mean(correct[t_norm > 500], na.rm = TRUE),
    bc_accuracy = accuracy - baseline,
    target = sum(correct[t_norm > 500], na.rm = TRUE),
    target_baseline = sum(correct[t_norm < 500], na.rm = TRUE),
    distractor = sum(!correct[t_norm > 500], na.rm = TRUE),
    distractor_baseline = sum(!correct[t_norm < 500], na.rm = TRUE),
    elogit_baseline = log((target_baseline + .5) /
      (distractor_baseline + .5)),
    elogit = log((target + .5) /
      (distractor + .5)),
    elogit_bc = elogit - elogit_baseline
  ) |>
  filter(!is.na(accuracy), !is.na(bc_accuracy), !is.na(elogit), !is.na(elogit_bc))

iccs <- d_summary |>
  group_by(dataset_name) |>
  nest() |>
  mutate(
    icc_stimulus_acc = unlist(map(data, ~ get_icc(.x,
      column = "accuracy",
      object = "stimulus"
    ))),
    icc_admin_acc = unlist(map(data, ~ get_icc(.x,
      column = "accuracy",
      object = "administration"
    ))),
    icc_stimulus_bc = unlist(map(data, ~ get_icc(.x,
      column = "bc_accuracy",
      object = "stimulus"
    ))),
    icc_admin_bc = unlist(map(data, ~ get_icc(.x,
      column = "bc_accuracy",
      object = "administration"
    ))),
    icc_stimulus_elogit = unlist(map(data, ~ get_icc(.x,
      column = "elogit",
      object = "stimulus"
    ))),
    icc_admin__elogit = unlist(map(data, ~ get_icc(.x,
      column = "elogit",
      object = "administration"
    ))),
    icc_stimulus_elogitbc = unlist(map(data, ~ get_icc(.x,
      column = "elogit_bc",
      object = "stimulus"
    ))),
    icc_admin_elogitbc = unlist(map(data, ~ get_icc(.x,
      column = "elogit_bc",
      object = "administration"
    )))
  ) |>
  select(-data) |>
  unnest(cols = c())
```

Let's plot these. 

```{r}
iccs_long <- iccs |>
  pivot_longer(-dataset_name, names_to = "measure", values_to = "icc") |>
  separate(measure, into = c("extra", "dimension", "measure")) |>
  select(-extra)

ggplot(
  iccs_long,
  aes(x = measure, y = icc, group = dataset_name)
) +
  geom_point() +
  geom_line(alpha = .5) +
  stat_summary(aes(group = 1), col = "red") +
  facet_wrap(~dimension) +
  ylim(0, 1) +
  ylab("ICC") +
  xlab("Measure")
```


Now trying with different baseline windows
```{r}
baseline_cutoffs <- seq(-500, 500, by = 50)

d_summary_cutoffs <- map(baseline_cutoffs, function(baseline_cutoff) {
  d_trial |>
    group_by(
      dataset_name, trial_id, subject_id, administration_id,
      target_label
    ) |>
    summarise(
      baseline = mean(correct[t_norm < baseline_cutoff], na.rm = TRUE),
      accuracy = mean(correct[t_norm > 500], na.rm = TRUE),
      bc_accuracy = accuracy - baseline,
      target = sum(correct[t_norm > 500], na.rm = TRUE),
      target_baseline = sum(correct[t_norm < baseline_cutoff], na.rm = TRUE),
      distractor = sum(!correct[t_norm > 500], na.rm = TRUE),
      distractor_baseline = sum(!correct[t_norm < baseline_cutoff], na.rm = TRUE),
      elogit_baseline = log((target_baseline + .5) /
        (distractor_baseline + .5)),
      elogit = log((target + .5) /
        (distractor + .5)),
      elogit_bc = elogit - elogit_baseline,
      baseline_cutoff = baseline_cutoff,
      .groups = "drop"
    ) |>
    filter(!is.na(accuracy), !is.na(bc_accuracy), !is.na(elogit), !is.na(elogit_bc))
}) |> list_rbind()
```

```{r eval=F}
iccs_cutoffs <- d_summary_cutoffs |>
  group_by(dataset_name, baseline_cutoff) |>
  nest() |>
  mutate(
    icc_stimulus_acc = unlist(map(data, ~ get_icc(.x,
      column = "accuracy",
      object = "stimulus"
    ))),
    icc_admin_acc = unlist(map(data, ~ get_icc(.x,
      column = "accuracy",
      object = "administration"
    ))),
    icc_stimulus_bc = unlist(map(data, ~ get_icc(.x,
      column = "bc_accuracy",
      object = "stimulus"
    ))),
    icc_admin_bc = unlist(map(data, ~ get_icc(.x,
      column = "bc_accuracy",
      object = "administration"
    ))),
    icc_stimulus_elogit = unlist(map(data, ~ get_icc(.x,
      column = "elogit",
      object = "stimulus"
    ))),
    icc_admin_elogit = unlist(map(data, ~ get_icc(.x,
      column = "elogit",
      object = "administration"
    ))),
    icc_stimulus_elogitbc = unlist(map(data, ~ get_icc(.x,
      column = "elogit_bc",
      object = "stimulus"
    ))),
    icc_admin_elogitbc = unlist(map(data, ~ get_icc(.x,
      column = "elogit_bc",
      object = "administration"
    )))
  ) |>
  select(-data) |>
  unnest(cols = c())

saveRDS(iccs_cutoffs, here("cached_intermediates", "7_iccs_cutoffs.Rds"))
```

```{r}
iccs_cutoffs <- readRDS(here("cached_intermediates", "7_iccs_cutoffs.Rds"))

iccs_long_cutoffs <- iccs_cutoffs |>
  pivot_longer(-c(dataset_name, baseline_cutoff),
    names_to = "measure", values_to = "icc"
  ) |>
  separate_wider_delim(measure,
    delim = "_",
    names = c("extra", "dimension", "measure")
  ) |>
  select(-extra) |>
  mutate(measure = case_when(
    measure == "acc" ~ "accuracy_raw",
    measure == "bc" ~ "accuracy_bc",
    measure == "elogit" ~ "elogit_raw",
    measure == "elogitbc" ~ "elogit_bc"
  )) |>
  separate_wider_delim(measure,
    delim = "_",
    names = c("measure", "correction")
  )

iccs_long_cutoffs |>
  ggplot(aes(
    x = baseline_cutoff, y = icc,
    group = interaction(dataset_name, measure, correction),
    col = measure, lty = correction, shape = correction
  )) +
  geom_line(alpha = .3) +
  geom_vline(xintercept = 0) +
  stat_summary(aes(group = interaction(measure, correction)),
    lty = "solid"
  ) +
  facet_wrap(~dimension) +
  ylim(0, 1) +
  scale_shape_manual(values = c(1, 16)) +
  labs(
    x = "Baseline window cutoff (ms)",
    y = "ICC",
    col = "Measure",
    lty = "Baseline correction",
    shape = "Baseline correction"
  ) +
  theme(legend.position = "bottom")
```


