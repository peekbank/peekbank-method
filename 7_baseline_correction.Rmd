---
title: "Baseline correction"
author: "Mike Frank"
date: "2022-12-15"
output: html_document
---



```{r}
source(here::here("helper/common.R"))

d_aoi <- readRDS(here("cached_intermediates", "1_d_aoi.Rds"))
```




# Baseline-corrected accuracy

the idea with baseline-corrected accuracy is that due to saliency effects (etc etc), accuracy measures might incorporate both kids word knowledge & whatever item properties, and we might be able to get the kids word knowledge more precisely by subtracting out the "accuracy" from the pre-stimulus time. 

* note that there might be a thing where baseline corrected accuracy is especially 

we'll want to look by dataset, since datasets vary widely in how much pre--looking time there is!

```{r}
ggplot(d_aoi, aes(x = t_norm)) +
  geom_histogram() +
  facet_wrap(~dataset_name, scales = "free_y") +
  geom_vline(xintercept = 0) +
  geom_vline(xintercept = 400)

baseline_lengths <- d_aoi |>
  group_by(dataset_name) |>
  summarise(t_min = min(t_norm))
```

let's make a filtered set for datasets that have sub -2000 RTs (long baselines) and start there, since those will be the *most* likely to have useful baseline correction. 

## ICC

```{r}
icc_bc_window_sim <- function(b_start = -2000, b_end = 0,
                              t_start = 500, t_end = 4000,
                              object) {
  # get trials with some baseline
  baseline_lengths <- d_aoi |>
    group_by(dataset_name, trial_id) |>
    summarise(t_min = min(t_norm))

  # get baseline corrected accuracies for all trials with ANY baseline info
  df <- d_aoi |>
    left_join(baseline_lengths) |>
    filter(t_min < 0) |>
    group_by(dataset_name, dataset_id, administration_id, target_label, trial_id) |>
    summarise(
      window_accuracy = mean(correct[t_norm >= t_start & t_norm <= t_end],
        na.rm = TRUE
      ),
      baseline_accuracy = mean(correct[t_norm >= b_start & t_norm <= b_end],
        na.rm = TRUE
      ),
      bc_accuracy = window_accuracy - baseline_accuracy
    ) |>
    filter(!is.na(bc_accuracy)) |>
    group_by(dataset_id, dataset_name, administration_id, target_label) |>
    mutate(repetition = row_number())

  # compute ICCs
  df |>
    group_by(dataset_name) |>
    nest() |>
    mutate(icc = unlist(map(data, ~ get_icc(., "bc_accuracy", object)))) |>
    select(-data) |>
    unnest(cols = c())
}

bc_acc_params <- expand_grid(
  t_start = 500,
  t_end = c(2000, 4000),
  b_start = seq(-4000, -1000, 500),
  b_end = seq(-500, 500, 500),
  object = c("administration")
)

library(multidplyr)
cluster <- new_cluster(16)
cluster_library(cluster, "dplyr")
cluster_library(cluster, "tidyr")
cluster_library(cluster, "purrr")
cluster_library(cluster, "agreement")
cluster_copy(cluster, "get_icc")
cluster_copy(cluster, "d_aoi")
cluster_copy(cluster, "icc_bc_window_sim")

bc_accs <- bc_acc_params |>
  partition(cluster) |>
  mutate(icc = pmap(list(b_start, b_end, t_start, t_end, object), icc_bc_window_sim)) |>
  collect() |>
  unnest(col = icc)

saveRDS(bc_accs, here("cached_intermediates", "bc_accs_icc.rds"))
```

### visualize

```{r}
bc_accs <- readRDS(here("cached_intermediates", "7_bc_accs_icc.rds")) |>
  bind_rows(readRDS(here("cached_intermediates", "3_accs.Rds"))) |>
  filter(t_start %in% c(400, 500), t_end %in% c(2000, 4000)) |>
  mutate(
    b_start = ifelse(is.na(b_start), 0, b_start),
    b_end = ifelse(is.na(b_end), 0, b_end)
  )
```

```{r}
bc_accs_summary <- bc_accs |>
  group_by(t_start, t_end, b_start, b_end) |>
  mutate(mean_icc = mean(icc, na.rm = T))

ggplot(bc_accs_summary, aes(x = b_start, y = b_end, fill = mean_icc)) +
  geom_tile(color = "white") +
  scale_fill_viridis(name = "Mean ICC", option = "inferno") +
  xlab("Window Start Time (in ms)") +
  ylab("Window End Time (in ms)") +
  facet_wrap(~t_end)
```
overall, icc from not doing baseline is >> on icc than window from doing baseline. (note that we have start of 400 versus start of 500 which we are assuming doesn't matter much)

```{r}
bc_accs |>
  filter(t_end == 4000) |>
  ggplot(aes(x = b_start, y = b_end, fill = icc)) +
  geom_tile(color = "white") +
  scale_fill_viridis(name = "Mean ICC", option = "inferno") +
  scale_x_continuous(breaks = c(-1000, -500, 0, 500, 1000, 1500)) +
  xlab("Window Start Time (in ms)") +
  ylab("Window End Time (in ms)") +
  facet_wrap(~dataset_name)
```
```{r}
bc_accs |>
  group_by(dataset_name) |>
  arrange(desc(icc)) |>
  slice(1)
```

so, there exist datasets where a baseline correction (numerically) improves icc (again with this weird 400/500 thing), but they vary based on what the best correction window is.

let's look of the datasets where it's 
```{r}
bc_accs_summary <- bc_accs |>
  inner_join(baseline_lengths |> filter(t_min <= -1000)) |>
  group_by(t_start, t_end, b_start, b_end) |>
  mutate(mean_icc = mean(icc, na.rm = T))
ggplot(bc_accs_summary, aes(x = b_start, y = b_end, fill = mean_icc)) +
  geom_tile(color = "white") +
  scale_fill_viridis(name = "Mean ICC", option = "inferno") +
  scale_x_continuous(breaks = c(-1000, -500, 0, 500, 1000, 1500)) +
  xlab("Window Start Time (in ms)") +
  ylab("Window End Time (in ms)") +
  facet_wrap(~t_end)

bc_accs_summary <- bc_accs |>
  inner_join(baseline_lengths |> filter(t_min <= -2000)) |>
  group_by(t_start, t_end, b_start, b_end) |>
  mutate(mean_icc = mean(icc, na.rm = T))
ggplot(bc_accs_summary, aes(x = b_start, y = b_end, fill = mean_icc)) +
  geom_tile(color = "white") +
  scale_fill_viridis(name = "Mean ICC", option = "inferno") +
  scale_x_continuous(breaks = c(-1000, -500, 0, 500, 1000, 1500)) +
  xlab("Window Start Time (in ms)") +
  ylab("Window End Time (in ms)") +
  facet_wrap(~t_end)

bc_accs_summary <- bc_accs |>
  inner_join(baseline_lengths |> filter(t_min <= -3000)) |>
  group_by(t_start, t_end, b_start, b_end) |>
  mutate(mean_icc = mean(icc, na.rm = T))
ggplot(bc_accs_summary, aes(x = b_start, y = b_end, fill = mean_icc)) +
  geom_tile(color = "white") +
  scale_fill_viridis(name = "Mean ICC", option = "inferno") +
  xlab("Window Start Time (in ms)") +
  ylab("Window End Time (in ms)") +
  facet_wrap(~t_end)
```
so, looks like for the datasets with data before -3000 (5 datasets), then you get higher icc for baselining, but you want your baseline windows to start at -3000 or -4000 and go to -500 or 0.  

### bootstrapping

```{r}
icc_bc_window_bootstrap_sim <- function(b_start = -2000, b_end = 0,
                                        t_start = 500, t_end = 4000,
                                        object) {
  # get trials with some baseline
  baseline_lengths <- d_aoi |>
    group_by(dataset_name, trial_id) |>
    summarise(t_min = min(t_norm))

  # get baseline corrected accuracies for all trials with ANY baseline info
  df <- d_aoi |>
    left_join(baseline_lengths) |>
    filter(t_min < 0) |>
    group_by(dataset_name, dataset_id, administration_id, target_label, trial_id) |>
    summarise(
      window_accuracy = mean(correct[t_norm >= t_start & t_norm <= t_end],
        na.rm = TRUE
      ),
      baseline_accuracy = mean(correct[t_norm >= b_start & t_norm <= b_end],
        na.rm = TRUE
      ),
      bc_accuracy = window_accuracy - baseline_accuracy
    ) |>
    filter(!is.na(bc_accuracy)) |>
    group_by(dataset_id, dataset_name, administration_id, target_label) |>
    mutate(repetition = row_number())

  # compute ICCs
  df |>
    group_by(dataset_name) |>
    nest() |>
    mutate(icc = map(data, \(d) bootstrap_icc(d, "bc_accuracy", 2000))) |>
    select(-data) |>
    unnest(icc)
}


icc_bc_window_age_bootstrap_sim <- function(b_start = -2000, b_end = 0,
                                            t_start = 500, t_end = 4000,
                                            object) {
  # get trials with some baseline
  baseline_lengths <- d_aoi |>
    group_by(dataset_name, trial_id) |>
    summarise(t_min = min(t_norm))

  # get baseline corrected accuracies for all trials with ANY baseline info
  df <- d_aoi |>
    left_join(baseline_lengths) |>
    mutate(younger = age < 24) |>
    filter(t_min < 0) |>
    group_by(dataset_name, dataset_id, administration_id, target_label, trial_id, younger) |>
    summarise(
      window_accuracy = mean(correct[t_norm >= t_start & t_norm <= t_end],
        na.rm = TRUE
      ),
      baseline_accuracy = mean(correct[t_norm >= b_start & t_norm <= b_end],
        na.rm = TRUE
      ),
      bc_accuracy = window_accuracy - baseline_accuracy
    ) |>
    filter(!is.na(bc_accuracy)) |>
    group_by(dataset_id, dataset_name, administration_id, target_label, younger) |>
    mutate(repetition = row_number())

  # compute ICCs
  df |>
    group_by(dataset_name, younger) |>
    nest() |>
    mutate(icc = map(data, \(d) bootstrap_icc(d, "bc_accuracy", 2000))) |>
    select(-data) |>
    unnest(icc)
}

bc_acc_params <- expand_grid(
  t_start = 500,
  t_end = c(4000),
  b_start = seq(-4000, -1000, 1000),
  b_end = c(-500, 0),
  object = c("administration")
)

library(multidplyr)
cluster <- new_cluster(16)
cluster_library(cluster, "dplyr")
cluster_library(cluster, "tidyr")
cluster_library(cluster, "purrr")
cluster_library(cluster, "agreement")
cluster_copy(cluster, "bootstrap_icc")
cluster_copy(cluster, "d_aoi")
cluster_copy(cluster, "icc_bc_window_bootstrap_sim")
cluster_copy(cluster, "icc_bc_window_age_bootstrap_sim")


bc_accs <- bc_acc_params |>
  partition(cluster) |>
  mutate(icc = pmap(list(b_start, b_end, t_start, t_end, object), icc_bc_window_bootstrap_sim)) |>
  collect() |>
  unnest(col = icc)

saveRDS(bc_accs, here("cached_intermediates", "7_bc_accs_icc_boot.rds"))

bc_accs_age <- bc_acc_params |>
  partition(cluster) |>
  mutate(icc = pmap(list(b_start, b_end, t_start, t_end, object), icc_bc_window_age_bootstrap_sim)) |>
  collect() |>
  unnest(col = icc)

saveRDS(bc_accs_age, here("cached_intermediates", "7_bc_accs_icc_boot_age.rds"))
```

```{r, fig.width=10}
bc_boot <- readRDS(here("cached_intermediates", "7_bc_accs_icc_boot.rds")) 
no_bc_boot <- readRDS(here("cached_intermediates", "3_acc_icc_boot.rds")) |> filter(t_start == 500, t_end == 4000)

# age ones waiting on cluster
bc_boot_age <- readRDS(here("cached_intermediates", "7_bc_accs_icc_boot_age.rds"))
no_bc_boot_age <- readRDS(here("cached_intermediates", "3_acc_icc_boot_age.rds")) |> filter(t_start == 500, t_end == 4000)
```


```{r, fig.width=10}
boot_graph <- bc_boot |>
  bind_rows(no_bc_boot) |>
  mutate(b_start = ifelse(is.na(b_start), "none", as.character(b_start)))
boot_graph |> ggplot(aes(x = b_start, color = as.character(b_end), y = est, ymin = lower, ymax = upper)) +
  geom_pointrange(position = position_dodge(width = .25)) +
  facet_wrap(~dataset_name)

boot_graph |> ggplot(aes(x = b_start, color = as.character(b_end), y = est, ymin = lower, ymax = upper)) +
  geom_pointrange(position = position_dodge(width = .25)) +
  facet_wrap(~dataset_name)
```


just for the younger kids (< 24 months)

```{r, fig.width=10}
boot_graph_age <- bc_boot_age|>
  bind_rows(no_bc_boot_age) |>
  mutate(b_start = ifelse(is.na(b_start), "none", as.character(b_start))) |> 
  mutate(younger=ifelse(younger, "<24months", ">=24months"))

boot_graph_age |> filter(younger=="<24months")  |> ggplot(aes(x = b_start, color = as.character(b_end), y = est, ymin = lower, ymax = upper)) +
  geom_pointrange(position = position_dodge(width = .25)) +
  facet_wrap(~dataset_name)

```

just for the older kids

```{r, fig.width=10}

boot_graph_age |> filter(younger==">=24months")  |> ggplot(aes(x = b_start, color = as.character(b_end), y = est, ymin = lower, ymax = upper)) +
  geom_pointrange(position = position_dodge(width = .25)) +
  facet_wrap(~dataset_name)

```
```{r}
baseline_lengths <- d_aoi |>
  group_by(dataset_name) |>
  summarise(t_min = min(t_norm))

bc_boot_summ <- boot_graph |>
  inner_join(baseline_lengths |> filter(t_min <= 0)) |>
  filter(!is.na(lower), !is.na(upper)) |>
  mutate(
    stdev = (upper - lower) / (1.96 * 2),
    var = stdev**2
  ) |>
  group_by(b_start, b_end) |>
  nest() |>
  mutate(icc = map(data, \(d){
    rma(d$est, d$var) |>
      summary() |>
      coef()
  })) |>
  select(-data) |>
  unnest(icc)

bc_boot_summ |> ggplot(aes(x = b_start, color = as.character(b_end), y = estimate, ymin = ci.lb, ymax = ci.ub)) +
  geom_pointrange(position = position_dodge(width = .25))
```


```{r}
bc_boot_early_summ <- boot_graph |>
  inner_join(baseline_lengths |> filter(t_min <= -3000)) |>
  filter(!is.na(lower), !is.na(upper)) |>
  mutate(
    stdev = (upper - lower) / (1.96 * 2),
    var = stdev**2
  ) |>
  group_by(b_start, b_end) |>
  nest() |>
  mutate(icc = map(data, \(d){
    rma(d$est, d$var) |>
      summary() |>
      coef()
  })) |>
  select(-data) |>
  unnest(icc)

bc_boot_early_summ |> ggplot(aes(x = b_start, color = as.character(b_end), y = estimate, ymin = ci.lb, ymax = ci.ub)) +
  geom_pointrange(position = position_dodge(width = .25))
```

```{r}
baseline_lengths <- d_aoi |>
  group_by(dataset_name) |>
  summarise(t_min = min(t_norm))

bc_boot_age_summ <- boot_graph_age |>
  inner_join(baseline_lengths |> filter(t_min <= 0)) |>
  filter(!is.na(lower), !is.na(upper)) |>
  mutate(
    stdev = (upper - lower) / (1.96 * 2),
    var = stdev**2
  ) |>
  group_by(b_start, b_end, younger) |>
  nest() |>
  mutate(icc = map(data, \(d){
    rma(d$est, d$var) |>
      summary() |>
      coef()
  })) |>
  select(-data) |>
  unnest(icc)

bc_boot_age_summ |> ggplot(aes(x = b_start, color = as.character(b_end), y = estimate, ymin = ci.lb, ymax = ci.ub)) + facet_wrap(~younger)+
  geom_pointrange(position = position_dodge(width = .25))
```

```{r}
bc_boot_age_early_summ <- boot_graph_age |>
  inner_join(baseline_lengths |> filter(t_min <= -3000)) |>
  filter(!is.na(lower), !is.na(upper)) |>
  mutate(
    stdev = (upper - lower) / (1.96 * 2),
    var = stdev**2
  ) |>
  group_by(b_start, b_end, younger) |>
  nest() |>
  mutate(icc = map(data, \(d){
    rma(d$est, d$var) |>
      summary() |>
      coef()
  })) |>
  select(-data) |>
  unnest(icc)

bc_boot_age_early_summ |> ggplot(aes(x = b_start, color = as.character(b_end), y = estimate, ymin = ci.lb, ymax = ci.ub)) +facet_wrap(~younger)+
  geom_pointrange(position = position_dodge(width = .25))
```

## CDI

```{r}
source("cl_helper.R")

d_aoi <- readRDS("d_aoi.Rds")
library(tibble)
library(multidplyr)

cdi_data <- readRDS("cdi.rds")




do_cdi <- function(data, indices) {
  summ <- data |>
    slice(indices) |>
    group_by(administration_id) |>
    summarize(mean_var = mean(bc_accuracy, na.rm = T)) |>
    left_join(cdi_data) |>
    ungroup() |>
    summarise(
      cor_comp = ifelse(sum(!is.na(comp) & !is.na(mean_var)) > 2, cor.test(mean_var, comp)$estimate, as.numeric(NA)),
      cor_prod = ifelse(sum(!is.na(prod) & !is.na(mean_var)) > 2, cor.test(mean_var, prod)$estimate, as.numeric(NA)),
      cor_age = ifelse(sum(!is.na(age) & !is.na(mean_var)) > 2, cor.test(mean_var, age)$estimate, as.numeric(NA))
    )
  cors <- c(summ$cor_comp[1], summ$cor_prod[1], summ$cor_age[1])
  names(cors) <- c("cor_comp", "cor_prod", "cor_age")

  return(cors)
}

bc_acc_cdi <- function(b_start, b_end, t_start, t_end) {
  # get trials with some baseline
  baseline_lengths <- d_aoi |>
    group_by(dataset_name, trial_id) |>
    summarise(t_min = min(t_norm))

  # get baseline corrected accuracies for all trials with ANY baseline info
  df <- d_aoi |>
    left_join(baseline_lengths) |>
    filter(t_min < 0) |>
    group_by(dataset_name, dataset_id, administration_id, target_label, trial_id) |>
    summarise(
      window_accuracy = mean(correct[t_norm >= t_start & t_norm <= t_end],
        na.rm = TRUE
      ),
      baseline_accuracy = mean(correct[t_norm >= b_start & t_norm <= b_end],
        na.rm = TRUE
      ),
      bc_accuracy = window_accuracy - baseline_accuracy
    ) |>
    filter(!is.na(bc_accuracy)) |>
    group_by(dataset_id, dataset_name, administration_id, target_label)

  # compute ICCs
  df |>
    group_by(dataset_name) |>
    nest() |>
    mutate(corr = map(data, \(d) {
      b <- boot::boot(d, do_cdi, 2000)
      fold_stats <- b$t |> as_tibble()
      names(fold_stats) <- names(b$t0)
      fold_stats |>
        summarize(across(
          everything(),
          list(
            lower = \(c) ifelse(sum(!is.na(c)) > 1, quantile(c, probs = c(.025), na.rm = T), NA),
            upper = \(c) ifelse(sum(!is.na(c)) > 1, quantile(c, probs = c(.975), na.rm = T), NA)
          )
        )) |>
        bind_cols(b$t0 |> as_tibble_row())
    })) |>
    select(-data) |>
    unnest(corr)
}


cluster <- new_cluster(16)
cluster_library(cluster, "dplyr")
cluster_library(cluster, "stringr")
cluster_library(cluster, "purrr")
cluster_library(cluster, "tidyr")
cluster_library(cluster, "stats")
cluster_library(cluster, "tibble")
cluster_copy(cluster, "do_cdi")
cluster_copy(cluster, "d_aoi")
cluster_copy(cluster, "cdi_data")
cluster_copy(cluster, "bc_acc_cdi")

bc_acc_params <- expand_grid(
  t_start = 500,
  t_end = c(4000),
  b_start = seq(-4000, -1000, 1000),
  b_end = c(-500, 0),
)

bc_accs_boot_cdi <- bc_acc_params |>
  partition(cluster) |>
  mutate(cdi = pmap(list(b_start, b_end, t_start, t_end), \(b_s, b_e, t_s, t_e) bc_acc_cdi(b_s, b_e, t_s, t_e))) |>
  collect() |>
  unnest(cdi)

saveRDS(bc_accs_boot_cdi, here("cached_intermediates", "7_bc_accs_boot_cdi.rds"))

```

```{r}
no_bc_accs_boot <- readRDS(here("cached_intermediates", "3_accs_boot_cdi.rds")) |> filter(t_start==500, t_end==4000)

bc_accs_boot_cdi <- readRDS(here("cached_intermediates", "7_bc_accs_boot_cdi.rds")) |>bind_rows(no_bc_accs_boot) |> 
  mutate(b_start=ifelse(is.na(b_start), "none", as.character(b_start)))

library(metafor)

  baseline_lengths <- d_aoi |>
    group_by(dataset_name) |>
    summarise(t_min = min(t_norm))
  
bc_accs_boot_cdi_summ <-  bc_accs_boot_cdi |>
  left_join(baseline_lengths) |> filter(t_min< 0) |> 
  mutate(
    comp_stdev = (cor_comp_upper - cor_comp_lower) / (1.96 * 2),
    comp_var = comp_stdev**2,
    prod_stdev = (cor_prod_upper - cor_prod_lower) / (1.96 * 2),
    prod_var = prod_stdev**2,
    age_stdev = (cor_age_upper - cor_age_lower) / (1.96 * 2),
    age_var = age_stdev**2
  ) |>
  group_by(t_start, t_end, b_start, b_end) |>
  nest() |>
  mutate(comp= map(data, \(d){
    rma(d$cor_comp, d$comp_var) |>
      summary() |>
      coef()
  }),
  prod= map(data, \(d){
    rma(d$cor_prod, d$prod_var) |>
      summary() |>
      coef()
  }),
  age= map(data, \(d){
    rma(d$cor_age, d$age_var) |>
      summary() |>
      coef()
  })) |>
  select(-data) |>
  unnest(c(comp, prod, age), names_sep="_")

  
  bc_accs_boot_cdi_summ_early <-  bc_accs_boot_cdi |>
    left_join(baseline_lengths) |> filter(t_min< -3000) |> 
  mutate(
    comp_stdev = (cor_comp_upper - cor_comp_lower) / (1.96 * 2),
    comp_var = comp_stdev**2,
    prod_stdev = (cor_prod_upper - cor_prod_lower) / (1.96 * 2),
    prod_var = prod_stdev**2,
    age_stdev = (cor_age_upper - cor_age_lower) / (1.96 * 2),
    age_var = age_stdev**2
  ) |>
  group_by(t_start, t_end, b_start, b_end) |>
  nest() |>
  mutate(comp= map(data, \(d){
    rma(d$cor_comp, d$comp_var) |>
      summary() |>
      coef()
  }),
  prod= map(data, \(d){
    rma(d$cor_prod, d$prod_var) |>
      summary() |>
      coef()
  }),
  age= map(data, \(d){
    rma(d$cor_age, d$age_var) |>
      summary() |>
      coef()
  })) |>
  select(-data) |>
  unnest(c(comp, prod, age), names_sep="_")
```
we expect correlations with production to be positive for accuracy data

```{r}
bc_accs_boot_cdi_summ |> ggplot(aes(x = b_start, y = prod_estimate, ymin=prod_ci.lb, ymax=prod_ci.ub, color=as.character(b_end))) +geom_pointrange(position=position_dodge(width=.5))+coord_flip()


bc_accs_boot_cdi |> left_join(baseline_lengths) |> filter(t_min< 0) |> filter(!is.na(cor_prod)) |>  ggplot(aes(x = b_start, y = cor_prod, ymin=cor_prod_lower, ymax=cor_prod_upper,  col=as.character(b_end))) +
  geom_pointrange() +
  geom_hline(yintercept=0)+
  coord_flip() +
  theme(legend.position = "none") +
  facet_wrap(~dataset_name)

bc_accs_boot_cdi_summ_early |> ggplot(aes(x = b_start, y = prod_estimate, ymin=prod_ci.lb, ymax=prod_ci.ub, color=as.character(b_end))) +geom_pointrange(position=position_dodge(width=.5))+coord_flip()
```
well, no benefit on the production data, and some weak signs of worseness.

correlation with comprehension

```{r}
bc_accs_boot_cdi_summ |> ggplot(aes(x = b_start, y = comp_estimate, ymin=comp_ci.lb, ymax=comp_ci.ub, color=as.character(b_end))) +geom_pointrange(position=position_dodge(width=.5))+coord_flip()


bc_accs_boot_cdi |> left_join(baseline_lengths) |> filter(t_min< 0) |> filter(!is.na(cor_comp)) |>  ggplot(aes(x = b_start, y = cor_comp, ymin=cor_comp_lower, ymax=cor_comp_upper,  col=as.character(b_end))) +
  geom_pointrange() +
  geom_hline(yintercept=0)+
  coord_flip() +
  theme(legend.position = "none") +
  facet_wrap(~dataset_name)

bc_accs_boot_cdi_summ_early |> ggplot(aes(x = b_start, y = comp_estimate, ymin=comp_ci.lb, ymax=comp_ci.ub, color=as.character(b_end))) +geom_pointrange(position=position_dodge(width=.5))+coord_flip()
```

correlation with age 

```{r}
bc_accs_boot_cdi_summ |> ggplot(aes(x = b_start, y = age_estimate, ymin=age_ci.lb, ymax=age_ci.ub, color=as.character(b_end))) +geom_pointrange(position=position_dodge(width=.5))+coord_flip()


bc_accs_boot_cdi |> left_join(baseline_lengths) |> filter(t_min< 0) |> filter(!is.na(cor_age)) |>  ggplot(aes(x = b_start, y = cor_age, ymin=cor_age_lower, ymax=cor_age_upper,  col=as.character(b_end))) +
  geom_pointrange() +
  geom_hline(yintercept=0)+
  coord_flip() +
  theme(legend.position = "none") +
  facet_wrap(~dataset_name)

bc_accs_boot_cdi_summ_early |> ggplot(aes(x = b_start, y = age_estimate, ymin=age_ci.lb, ymax=age_ci.ub, color=as.character(b_end))) +geom_pointrange(position=position_dodge(width=.5))+coord_flip()
```


## Summary

For datasets with a lot of pre-onset data (where we have -3000 ms), reliability is slightly increased by doing baseline correction, but there is no corresponding benefit (and potentially a detriment) to validity. 

For datasets with shorter baseline periods, baseline correction is worse for ICC and shows no benefit for validity measures. 

Takeaway: don't baseline correct?

# Old

# (to update/revisit) Baseline-corrected accuracy

## ICC approach

```{r}
icc_bc_window_sim <- function(b_start = -2000, b_end = 0,
                              t_start = 500, t_end = 4000,
                              object) {
  # get trials with some baseline
  baseline_lengths <- d_aoi |>
    group_by(dataset_name, trial_id) |>
    summarise(t_min = min(t_norm))

  # get baseline corrected accuracies for all trials with ANY baseline info
  df <- d_aoi |>
    left_join(baseline_lengths) |>
    filter(t_min < 0) |>
    group_by(dataset_name, dataset_id, administration_id, target_label, trial_id) |>
    summarise(
      window_accuracy = mean(correct[t_norm >= t_start & t_norm <= t_end],
        na.rm = TRUE
      ),
      baseline_accuracy = mean(correct[t_norm >= b_start & t_norm <= b_end],
        na.rm = TRUE
      ),
      bc_accuracy = window_accuracy - baseline_accuracy
    ) |>
    filter(!is.na(bc_accuracy))

  # compute ICCs
  df |>
    group_by(dataset_name) |>
    nest() |>
    mutate(icc = unlist(map(data, ~ get_icc(., "bc_accuracy", object)))) |>
    select(-data) |>
    unnest(cols = c())
}
```

```{r}
bc_acc_params <- expand_grid(
  t_start = seq(500, 1500, 500),
  t_end = seq(2000, 4000, 500),
  b_start = seq(-2000, -1000, 500),
  b_end = seq(-500, 0, 500),
  object = c("stimulus", "administration")
)

# multidyplr attempt
cluster <- new_cluster(14)
cluster_library(cluster, "tidyverse")
cluster_library(cluster, "agreement")
cluster_copy(cluster, "icc_bc_window_sim")
cluster_copy(cluster, "get_icc")
cluster_copy(cluster, "d_aoi")

tic()
bc_accs <- bc_acc_params |>
  partition(cluster) |>
  mutate(icc = pmap(list(b_start, b_end, t_start, t_end, object), icc_bc_window_sim)) |>
  collect() |>
  unnest(col = icc)
toc()

save(file = "cached_intermediates/3_bc_accs.Rds", bc_accs)
```

```{r}
load(file = "cached_intermediates/3_bc_accs.Rds")
```

Summary data frame. 

```{r}
bc_accs_summary <- bc_accs |>
  group_by(b_start, b_end, t_start, t_end, object) |>
  summarize(
    N = n(),
    mean_icc = mean(icc, na.rm = TRUE)
  ) |>
  mutate(
    window_size = t_end - t_start,
    bc_window_size = b_end - b_start
  )
```


```{r}
ggplot(
  bc_accs_summary,
  aes(color = mean_icc)
) +
  geom_vline(xintercept = 0, linetype = "dashed") +
  geom_segment(aes(x = t_start, xend = t_end, y = mean_icc, yend = mean_icc)) +
  geom_segment(aes(x = t_start, xend = t_start, y = mean_icc - 0.005, yend = mean_icc + 0.005)) +
  geom_segment(aes(x = t_end, xend = t_end, y = mean_icc - 0.005, yend = mean_icc + 0.005)) +
  scale_color_viridis(name = "Mean ICC", direction = -1) +
  theme(legend.position = "none") +
  ylab("Mean ICC") +
  xlab("Analysis Window (in ms)") +
  facet_grid(bc_window_size ~ object)
```
These numbers seem low. 

Compare to non-corrected accuracies. 

```{r}
load(file = "cached_intermediates/3_accs.Rds")

accs_summary <- accs |>
  group_by(t_start, t_end, object) |>
  summarize(mean_uncorrected_icc = mean(icc, na.rm = TRUE))

comparison_accs_summary <- left_join(bc_accs_summary, accs_summary)
```

```{r}
ggplot(comparison_accs_summary, aes(x = mean_uncorrected_icc, y = mean_icc, col = window_size)) +
  geom_point() +
  geom_smooth(method = "lm") +
  geom_abline(lty = 2) +
  # xlim(0,.6) + ylim(0,.6) +
  facet_grid(bc_window_size ~ object)
```
Possible interpretation (pace Dan Swingley): baselines are short and noisy. By adding them into the signal, you are diluting what you have. 

Note that we don't have a lot of baseline data here (no more than 2s) so we can't rule out the idea that a longer (4s or so) baseline would be meaningful or useful in a correction analysis. But even with 2s we see substantial reductions in ICC. 

## CDI validity with baseline correction

```{r}
cdi_bc_sim <- function(t_start = 500, t_end = 4000) {
  # get trials with some baseline
  baseline_lengths <- vanilla_cdi_datasets |>
    group_by(dataset_name, trial_id) |>
    summarise(t_min = min(t_norm))

  # get baseline corrected accuracies for all trials with ANY baseline info
  by_subject_accuracies <- vanilla_cdi_datasets |>
    left_join(baseline_lengths) |>
    filter(t_min < 0) |>
    group_by(dataset_name, lab_subject_id, trial_id) |>
    summarize(
      window_accuracy = mean(correct[t_norm >= t_start & t_norm <= t_end],
        na.rm = TRUE
      ),
      baseline_accuracy = mean(correct[t_norm >= -2000 & t_norm <= 0],
        na.rm = TRUE
      ),
      bc_accuracy = window_accuracy - baseline_accuracy
    )

  mean_accuracies <- by_subject_accuracies |>
    group_by(dataset_name, lab_subject_id) |>
    summarize(bc_accuracy = mean(bc_accuracy)) |>
    left_join(cdis)

  mean_accuracies |>
    group_by(dataset_name) |>
    summarise(
      cor_comp = cor.test(bc_accuracy, eng_wg_understood)$estimate,
      cor_prod = cor.test(bc_accuracy, eng_wg_produced)$estimate
    )
}
```

```{r}
cdi_params <- expand_grid(
  t_start = seq(500, 1500, 500),
  t_end = seq(2000, 4000, 500)
)
tic()
cdi_bc_corrs <- cdi_params |>
  mutate(icc = pmap(list(t_start, t_end), cdi_bc_sim)) |>
  unnest(col = icc)
toc()

cdi_bc_corrs <- cdi_bc_corrs |>
  pivot_longer(names_to = "measure", values_to = "r", starts_with("cor"))
```

Visualize!


```{r}
ggplot(cdi_bc_corrs, aes(col = r)) +
  geom_vline(xintercept = 0, linetype = "dashed") +
  geom_hline(yintercept = 0.05, lty = 3) +
  geom_segment(aes(x = t_start, xend = t_end, y = r, yend = r)) +
  # geom_segment(aes(x=t_start,xend=t_start,y=mean_icc-0.005,yend=mean_icc+0.005))+
  # geom_segment(aes(x=t_end,xend=t_end,y=mean_icc-0.005,yend=mean_icc+0.005))+
  scale_color_viridis(name = "Mean ICC", direction = -1) +
  # scale_y_log10() +
  theme(legend.position = "none") +
  ylab("Correlation with MB-CDI sumscore") +
  xlab("Analysis Window (in ms)") +
  facet_grid(dataset_name ~ measure)
```

Compare to uncorrected. 

```{r}
cdi_comparison <- left_join(
  rename(cdi_bc_corrs, r_baseline_corrected = r),
  rename(cdi_corrs, r_uncorrected = r)
) |>
  mutate(window_size = t_end - t_start)

ggplot(cdi_comparison, aes(x = r_uncorrected, y = r_baseline_corrected, col = window_size)) +
  geom_point() +
  geom_smooth(method = "lm") +
  geom_abline(lty = 2) +
  facet_wrap(~dataset_name)
```

This is overall surprising. For Swingley & Aslin, baseline correction makes things look way worse. But for Garrison & Bergelson, it seems like baseline correction helps. 



```{r}
am <- d_aoi |>
  filter(dataset_name == "adams_marchman_2018") |>
  group_by(
    trial_id, subject_id, administration_id,
    target_label
  ) |>
  summarise(
    baseline = mean(correct[t_norm < 400], na.rm = TRUE),
    accuracy = mean(correct[t_norm > 400], na.rm = TRUE),
    bc_accuracy = accuracy - baseline
  ) |>
  filter(!is.na(accuracy), !is.na(bc_accuracy))

ggplot(
  am,
  aes(x = accuracy)
) +
  geom_histogram()

ggplot(
  am,
  aes(x = bc_accuracy)
) +
  geom_histogram()
```

Now check ICCs.

```{r}
# disaggregated
get_icc(am, column = "accuracy", object = "stimulus")
get_icc(am, column = "accuracy", object = "administration")
get_icc(am, column = "bc_accuracy", object = "stimulus")
get_icc(am, column = "bc_accuracy", object = "administration")
```

OK, so for this dataset it seems like within-trial baseline correction is **reducing** reliability for both stimuli and administrations. AM2018 still has relatively high reliability (in contrast to others). Let's try the SA dataset we were looking at before. 

```{r}
sa <- d_trial |>
  filter(dataset_name == "swingley_aslin_2002") |>
  group_by(
    trial_id, subject_id, administration_id,
    target_label
  ) |>
  summarise(
    baseline = mean(correct[t_norm < 500], na.rm = TRUE),
    accuracy = mean(correct[t_norm > 500], na.rm = TRUE),
    bc_accuracy = accuracy - baseline
  ) |>
  filter(!is.na(accuracy))

ggplot(
  am,
  aes(x = accuracy)
) +
  geom_histogram()

ggplot(
  am,
  aes(x = bc_accuracy)
) +
  geom_histogram()

# disaggregated
get_icc(sa, column = "accuracy", object = "stimulus")
get_icc(sa, column = "accuracy", object = "administration")
get_icc(sa, column = "bc_accuracy", object = "stimulus")
get_icc(sa, column = "bc_accuracy", object = "administration")
```

Weirdly it looks like the reverse is happening. Let's get more systematic. 

```{r}
d_summary <- d_trial |>
  group_by(
    dataset_name, trial_id, subject_id, administration_id,
    target_label
  ) |>
  summarise(
    baseline = mean(correct[t_norm < 500], na.rm = TRUE),
    accuracy = mean(correct[t_norm > 500], na.rm = TRUE),
    bc_accuracy = accuracy - baseline,
    target = sum(correct[t_norm > 500], na.rm = TRUE),
    target_baseline = sum(correct[t_norm < 500], na.rm = TRUE),
    distractor = sum(!correct[t_norm > 500], na.rm = TRUE),
    distractor_baseline = sum(!correct[t_norm < 500], na.rm = TRUE),
    elogit_baseline = log((target_baseline + .5) /
      (distractor_baseline + .5)),
    elogit = log((target + .5) /
      (distractor + .5)),
    elogit_bc = elogit - elogit_baseline
  ) |>
  filter(!is.na(accuracy), !is.na(bc_accuracy), !is.na(elogit), !is.na(elogit_bc))

iccs <- d_summary |>
  group_by(dataset_name) |>
  nest() |>
  mutate(
    icc_stimulus_acc = unlist(map(data, ~ get_icc(.x,
      column = "accuracy",
      object = "stimulus"
    ))),
    icc_admin_acc = unlist(map(data, ~ get_icc(.x,
      column = "accuracy",
      object = "administration"
    ))),
    icc_stimulus_bc = unlist(map(data, ~ get_icc(.x,
      column = "bc_accuracy",
      object = "stimulus"
    ))),
    icc_admin_bc = unlist(map(data, ~ get_icc(.x,
      column = "bc_accuracy",
      object = "administration"
    ))),
    icc_stimulus_elogit = unlist(map(data, ~ get_icc(.x,
      column = "elogit",
      object = "stimulus"
    ))),
    icc_admin__elogit = unlist(map(data, ~ get_icc(.x,
      column = "elogit",
      object = "administration"
    ))),
    icc_stimulus_elogitbc = unlist(map(data, ~ get_icc(.x,
      column = "elogit_bc",
      object = "stimulus"
    ))),
    icc_admin_elogitbc = unlist(map(data, ~ get_icc(.x,
      column = "elogit_bc",
      object = "administration"
    )))
  ) |>
  select(-data) |>
  unnest(cols = c())
```

Let's plot these. 

```{r}
iccs_long <- iccs |>
  pivot_longer(-dataset_name, names_to = "measure", values_to = "icc") |>
  separate(measure, into = c("extra", "dimension", "measure")) |>
  select(-extra)

ggplot(
  iccs_long,
  aes(x = measure, y = icc, group = dataset_name)
) +
  geom_point() +
  geom_line(alpha = .5) +
  stat_summary(aes(group = 1), col = "red") +
  facet_wrap(~dimension) +
  ylim(0, 1) +
  ylab("ICC") +
  xlab("Measure")
```


Now trying with different baseline windows
```{r}
baseline_cutoffs <- seq(-500, 500, by = 50)

d_summary_cutoffs <- map(baseline_cutoffs, function(baseline_cutoff) {
  d_trial |>
    group_by(
      dataset_name, trial_id, subject_id, administration_id,
      target_label
    ) |>
    summarise(
      baseline = mean(correct[t_norm < baseline_cutoff], na.rm = TRUE),
      accuracy = mean(correct[t_norm > 500], na.rm = TRUE),
      bc_accuracy = accuracy - baseline,
      target = sum(correct[t_norm > 500], na.rm = TRUE),
      target_baseline = sum(correct[t_norm < baseline_cutoff], na.rm = TRUE),
      distractor = sum(!correct[t_norm > 500], na.rm = TRUE),
      distractor_baseline = sum(!correct[t_norm < baseline_cutoff], na.rm = TRUE),
      elogit_baseline = log((target_baseline + .5) /
        (distractor_baseline + .5)),
      elogit = log((target + .5) /
        (distractor + .5)),
      elogit_bc = elogit - elogit_baseline,
      baseline_cutoff = baseline_cutoff,
      .groups = "drop"
    ) |>
    filter(!is.na(accuracy), !is.na(bc_accuracy), !is.na(elogit), !is.na(elogit_bc))
}) |> list_rbind()
```

```{r eval=F}
iccs_cutoffs <- d_summary_cutoffs |>
  group_by(dataset_name, baseline_cutoff) |>
  nest() |>
  mutate(
    icc_stimulus_acc = unlist(map(data, ~ get_icc(.x,
      column = "accuracy",
      object = "stimulus"
    ))),
    icc_admin_acc = unlist(map(data, ~ get_icc(.x,
      column = "accuracy",
      object = "administration"
    ))),
    icc_stimulus_bc = unlist(map(data, ~ get_icc(.x,
      column = "bc_accuracy",
      object = "stimulus"
    ))),
    icc_admin_bc = unlist(map(data, ~ get_icc(.x,
      column = "bc_accuracy",
      object = "administration"
    ))),
    icc_stimulus_elogit = unlist(map(data, ~ get_icc(.x,
      column = "elogit",
      object = "stimulus"
    ))),
    icc_admin_elogit = unlist(map(data, ~ get_icc(.x,
      column = "elogit",
      object = "administration"
    ))),
    icc_stimulus_elogitbc = unlist(map(data, ~ get_icc(.x,
      column = "elogit_bc",
      object = "stimulus"
    ))),
    icc_admin_elogitbc = unlist(map(data, ~ get_icc(.x,
      column = "elogit_bc",
      object = "administration"
    )))
  ) |>
  select(-data) |>
  unnest(cols = c())

saveRDS(iccs_cutoffs, here("cached_intermediates", "7_iccs_cutoffs.Rds"))
```

```{r}
iccs_cutoffs <- readRDS(here("cached_intermediates", "7_iccs_cutoffs.Rds"))

iccs_long_cutoffs <- iccs_cutoffs |>
  pivot_longer(-c(dataset_name, baseline_cutoff),
    names_to = "measure", values_to = "icc"
  ) |>
  separate_wider_delim(measure,
    delim = "_",
    names = c("extra", "dimension", "measure")
  ) |>
  select(-extra) |>
  mutate(measure = case_when(
    measure == "acc" ~ "accuracy_raw",
    measure == "bc" ~ "accuracy_bc",
    measure == "elogit" ~ "elogit_raw",
    measure == "elogitbc" ~ "elogit_bc"
  )) |>
  separate_wider_delim(measure,
    delim = "_",
    names = c("measure", "correction")
  )

iccs_long_cutoffs |>
  ggplot(aes(
    x = baseline_cutoff, y = icc,
    group = interaction(dataset_name, measure, correction),
    col = measure, lty = correction, shape = correction
  )) +
  geom_line(alpha = .3) +
  geom_vline(xintercept = 0) +
  stat_summary(aes(group = interaction(measure, correction)),
    lty = "solid"
  ) +
  facet_wrap(~dimension) +
  ylim(0, 1) +
  scale_shape_manual(values = c(1, 16)) +
  labs(
    x = "Baseline window cutoff (ms)",
    y = "ICC",
    col = "Measure",
    lty = "Baseline correction",
    shape = "Baseline correction"
  ) +
  theme(legend.position = "bottom")
```


