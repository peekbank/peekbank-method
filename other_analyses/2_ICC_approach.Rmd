---
title: "Basic illustration of the ICC approach"
author: "Mike"
date: "2/19/2021"
output: 
  html_document:
    toc: true
    toc_depth: 2
    toc_float: true
    code_folding: hide
---

```{r}
source(here::here("helper/common.R"))
```


Here's the distribution of average looking across all trials.

```{r}
d_summary <- d_trial |>
  group_by(dataset_name, trial_id, dataset_id, subject_id, administration_id, 
           target_label) |>
  summarise(accuracy = mean(correct[t_norm > 500], na.rm=TRUE),
            prop_data = mean(!is.na(correct[t_norm > 500]))) |>
  filter(!is.na(accuracy))

ggplot(d_summary, aes(x = prop_data, y = accuracy)) +
  geom_point(alpha = .05)
```

We're going to use ICCs to measure reliability, using McGraw & Wong (1996). It seems like we want two-way random effects, no interaction (subjects and items are meaningful). This is type "2A." We want average agreement across units.

One big decision is whether to look across stimulus items, rather than across kids. Across stimulus items returns *much* higher values. This is in part because we typically have more kids than items, and kids are sort of like "raters." 


## Swingley and Aslin

Let's look at one dataset. Here are the stimulus and administration ICCs for Swingley & Aslin (2002).

```{r}
sa <- d_summary |> 
  filter(dataset_name == "swingley_aslin_2002")

sa_cleaned <- filter(sa, 
                     !(target_label %in% c("bird","duck","shoe","truck")))

ggplot(sa_cleaned, 
       aes(x = target_label, y = accuracy)) +
  geom_jitter(alpha = .5, width = .2) + 
  stat_summary(col = "red")
```

Now check ICCs.

```{r}
# disaggregated
get_icc(sa, object = "stimulus")
get_icc(sa, object = "administration")

# disaggregated and cleaned
get_icc(sa_cleaned, object = "stimulus")
get_icc(sa_cleaned, object = "administration")


# aggregated
sa_agg <- sa |>
  group_by(target_label, administration_id, trial_id) |>
  summarise(accuracy = mean(accuracy))
get_icc(sa_agg, object = "stimulus")
get_icc(sa_agg, object = "administration")
```

I don't understand the zero. Hypothesis - this is about not differentiating two different observations for each `target_label`. 

```{r}
dim_icc(sa, 
        model = "2A", 
        type = "agreement", 
        unit = "average",
        object = administration_id, 
        rater = target_label,
        trial = trial_id, 
        score = accuracy, 
        bootstrap = 1000)
```

```{r}
dim_icc(sa, 
        model = "2A", 
        type = "agreement", 
        unit = "average",
        object = target_label, 
        rater = administration_id,
        trial = trial_id, 
        score = accuracy, 
        bootstrap = 1000)
```

Last question: inter vs. intra-rater reliability. 

```{r}
dim_icc(sa, 
        model = "2A", 
        type = "agreement", 
        unit = "average",
        object = administration_id, 
        rater = target_label,
        trial = trial_id, 
        score = accuracy, 
        bootstrap = 0)
dim_icc(sa_agg, 
        model = "2A", 
        type = "agreement", 
        unit = "average",
        object = administration_id, 
        rater = target_label,
        score = accuracy, 
        bootstrap = 0)
```
We're not actually sure how the **intra**-rater reliabilities are computed when you have only one observation per rater (shouldn't they be zero then?). But we're pretty clear we want the **inter**-rater reliabilities.

Take-homes:
* can't have multiple observations without a disambiguating trial label
* average absolute inter-rater reliability is what we want
* averaging across multiple observations increases reliabilities


## Across datasets

Note that we need to remove NaNs to make the ICCs work. 

```{r}
iccs <- d_summary |>
  group_by(dataset_name) |> 
  nest() |>
  mutate(icc_stimulus_acc = unlist(map(data, ~get_icc(.x, object = "stimulus"))),
         icc_admin_acc = unlist(map(data, ~get_icc(.x, object = "administration")))) |>
  select(-data) |>
  unnest(cols = c())

knitr::kable(iccs, digits = 2)

save(iccs, file= here("cached_intermediates","2_iccs_accuracy.Rds"))
```

OK, to summarize, we think we understand the ICCs. They are:

* higher for administrations in datasets with lots of items (e.g., adams marchman)
* higher for stimuli in big datasets with few items (e.g., attword)
* reliably not NaN or 0 because we solved missing data (can't have) and repeated trials (need to mark trial id) issues. 
