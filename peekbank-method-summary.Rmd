---
title: "Peekbank method summary"
output: 
  html_document:
    toc: true
---

```{r, echo=F, include=F}
knitr::opts_chunk$set(warning=F, message=F, echo=F)
source(here::here("helper/common.R"))
library(metafor)
library(ggthemes)
```

Here are (draft) recommendations and (draft) supporting figures for the peekbank methods paper. Obviously still some work on how to phase the recommendations. 

Note that quality checks on the data may cause numbers to change, but we don't anticipate large changes in overall results. 

# Summary of approach

The goal of the peekbank-methods project is to give recommendations about the design and analysis of LWL studies. To do so, we need some criteria for what is a better outcome. Here, we take a "data-driven" approach to look at how different potential data-processing and study design factors affect reliability and validity. 

All of these analyses are done on a subset of the peekbank database limited to "vanilla" trials (real word target, real word distractor, plain carrier phrase) in English with target words that are nouns. 

Throughout we use three different metrics to assess reliabilty and validity. 

1. An intra-class correlation (ICC) as a measure of reliability. Specifically, we use a measure of inter-rater reliability that measures (within a dataset) how much different items (target labels) are consistent with each other in "rating" the subjects (kid-administrations). 

2. Test-retest reliability for measures on the same child at two different administrations within 1.5 months of each other. This measure is only possible on a subset of the datasets that have longitudinal data with some closely spaced repeat administrations. 

3. Correlation with CDI measure for validity. We can check how well our metrics correlate with children's CDI sumscores from CDI administrations at roughly the same time. In general, correlating with production and comprehension in the expected directions is desirable, although we also may expect that LWL measures are picking up on slightly different aspects of language learning. 

In order to tell when numerical differences in ICC or correlations could be meaningful, I use bootstrapping which generates per-dataset confidence intervals around the point estimate. I then use point estimates and the bootstrapped CIs to calculate a variance-weighted meta-analytic estimate (and CI) for the correlation across datasets.

This project has a non-standard aim with the statistics. We are neither trying to do just estimation nor are we trying to do "traditional" inferential (p<.05) statistics. We are trying to make inferences as to what data practices to recommend (and how strongly)! 

Often in our other statistical applications, we like to see differences. Here, we expect that some big changes in method should lead to differences in outcome (ex. if an accuracy window started really late at 1000 milliseconds, we expect it be bad), but we also expect that many small differences in approach might lead to minimal differences in outcome. It would be surprising and worrying if small differences in approach often dramatically altered results! 

Other factors may also influence what data practices to choose (and to recommend). In particular, we probably prefer parsimony -- given two processes with equivalent goodness of outcome, we should prefer the simpler one. There may also be theoretical commitments that lead to preferences over one measure or another (ex: log vs raw RT). 

# Data processing recommendations

## Recommendation 1: Use long accuracy windows. 

Longer accuracy windows that go to 4000ms post disambiguation, rather than to 2000ms, increase reliability (both ICC and test-retest). Short and long windows show comparable correlations with CDI production data, although shorter windows are somewhat more highly correlated with comprehension data. Together, this suggests that looking behaving between 2 and 4 seconds is a stable child-level property, although we cannot say how related it to to child language per se. 

Exactly when the window starts is not important, although we recommend starting around 500ms to balance between reliability is numerically highest starting at 600 or 700 milliseconds and correlation with production is highest at 500ms. 

While we are recommending accuracy be measured over the 500-4000ms window, we want to be clear that other accuracy windows also have reasonable reliability and validity properties. Strong effects are likely to show up in any reasonable window, but for eking the most reliability out of data, we recommend long windows. 

A heatmap showing ICC (a measure of reliability) for different start and end times for accuracy windows.
```{r}

accs <- readRDS(here("cached_intermediates/3_accs.Rds"))

accs_summary <- accs |>
  filter(object == "administration") |>
  group_by(t_start, t_end, object) |>
  summarize(
    N = n(),
    mean_icc = mean(icc, na.rm = TRUE)
  ) |>
  mutate(window_size = t_end - t_start)

ggplot(accs_summary, aes(x = t_start, y = t_end, fill = mean_icc)) +
  geom_tile(color = "white") +
  scale_fill_viridis(name = "Mean ICC", option = "inferno") +
  scale_x_continuous(breaks = c(-1000, -500, 0, 500, 1000, 1500)) +
  xlab("Window Start Time (in ms)") +
  ylab("Window End Time (in ms)")
```


```{r}
acc_boot <- readRDS(here("cached_intermediates", "3_acc_icc_boot.rds"))

acc_boot_summary <- acc_boot |>
  filter(!is.na(lower), !is.na(upper)) |>
  mutate(
    stdev = (upper - lower) / (1.96 * 2),
    var = stdev**2
  ) |>
  group_by(t_start, t_end) |>
  nest() |>
  mutate(icc = map(data, \(d){
    rma(d$est, d$var) |>
      summary() |>
      coef()
  })) |>
  select(-data) |>
  unnest(icc)



accs_cdi_kid <- readRDS(here("cached_intermediates", "3_accs_kid_cdi.rds")) |> mutate(
  prod_est = ifelse(dataset_name=="newman_genderdistractor", NA, prod_est),
  prod_lower = ifelse(dataset_name=="newman_genderdistractor", NA, prod_lower),
  prod_upper = ifelse(dataset_name=="newman_genderdistractor", NA, prod_upper),
)

library(metafor)

accs_cdi_kid_summ <-  accs_cdi_kid |> 
  mutate(
    comp_stdev = (comp_upper - comp_lower) / (1.96 * 2),
    comp_var = comp_stdev**2,
    prod_stdev = (prod_upper - prod_lower) / (1.96 * 2),
    prod_var = prod_stdev**2,
    age_stdev = (age_upper - age_lower) / (1.96 * 2),
    age_var = age_stdev**2
  ) |>
  group_by(t_start, t_end) |>
  nest() |>
  mutate(comp= map(data, \(d){
    rma(d$comp_est, d$comp_var) |>
      summary() |>
      coef()
  }),
  prod= map(data, \(d){
    rma(d$prod_est, d$prod_var) |>
      summary() |>
      coef()
  }),
  age= map(data, \(d){
    rma(d$age_est, d$age_var) |>
      summary() |>
      coef()
  })) |>
  select(-data) |>
  unnest(c(comp, prod, age), names_sep="_")

accs_kid_test_retest <- readRDS(here("cached_intermediates", "3_accs_boot_kid_test_retest.rds"))

accs_kid_test_retest_summ <-  accs_kid_test_retest |>
  mutate(
    stdev = (upper - lower) / (1.96 * 2),
    var = stdev**2,
  ) |>
  group_by(t_start, t_end) |>
  nest() |>
  mutate(corr= map(data, \(d){
    rma(d$est, d$var) |>
      summary() |>
      coef()
  })) |> 
  unnest(corr)
```

Reliability and validity measures for different accuracy windows. Ranges are bootstrapped 95% CIs.
```{r}
acc_all <- acc_boot_summary |> mutate(Type="ICC reliability") |> bind_rows(accs_cdi_kid_summ |> rename(estimate=prod_estimate, ci.lb=prod_ci.lb, ci.ub=prod_ci.ub) |> mutate(Type="Corr. with CDI Production")) |> bind_rows(accs_cdi_kid_summ |> rename(estimate=comp_estimate, ci.lb=comp_ci.lb, ci.ub=comp_ci.ub) |> mutate(Type="Corr. with CDI Comprehension")) |> 
  bind_rows(accs_kid_test_retest_summ |> mutate(Type="Test-retest reliability"))

acc_all |> ggplot(aes(x=t_start, col=as.character(t_end), y=estimate, ymin=ci.lb, group=t_end, ymax=ci.ub))+geom_pointrange(position=position_dodge(50))+coord_flip()+facet_wrap(~Type, scales="free_x", nrow=1)+theme(legend.position = "bottom", )+labs(y="Correlation", x="Window start time (ms)", color="Window end time (ms)")+scale_color_solarized()
```


## Recommendation 2: Don't do baseline correction. 

Different images have varying levels of saliency to infants, which can drive uneven looking patterns at baseline, before auditory stimulus. Sometimes, researchers address this by using baseline-corrected accuracy, a difference between the looking fraction after the stimulus and the looking fraction before the stimulus. In general, we found that baseline-corrected accuracy led to worse reliability and validity than accuracy. For datasets where there was looking data for a long pre-stimulus period (at least 3 seconds), baseline correction using a long pre-stimulus period was as reliable as accuracy without baseline-correction, but still had marginally worse validity.

[TODO we could therorize on why baseline-correction is worse -- difference measures are generally high variance]

As alternatives to baseline correction, researchers could account for item-level variability in their statistical models. Additionally, better matching of stimulus properties between the target and distractor pairs may reduce unequal looking before stimulus onset (see Recommendation TODO). 

```{r}
d_aoi <- readRDS(here("cached_intermediates", "1_d_aoi.Rds"))

bc_boot <- readRDS(here("cached_intermediates", "7_bc_accs_icc_boot.rds")) 
no_bc_boot <- readRDS(here("cached_intermediates", "3_acc_icc_boot.rds")) |> filter(t_start == 500, t_end == 4000)

baseline_lengths <- d_aoi |>
  group_by(dataset_name) |>
  summarise(t_min = min(t_norm))
boot_graph <- bc_boot |>
  bind_rows(no_bc_boot) |>   inner_join(baseline_lengths |> filter(t_min < 0)) |>
  mutate(b_start = ifelse(is.na(b_start), "none", as.character(b_start)))

library(metafor)
bc_boot_summ <- boot_graph |>
  inner_join(baseline_lengths |> filter(t_min < 0)) |>
  filter(!is.na(lower), !is.na(upper)) |>
  mutate(
    stdev = (upper - lower) / (1.96 * 2),
    var = stdev**2
  ) |>
  group_by(b_start, b_end) |>
  nest() |>
  mutate(icc = map(data, \(d){
    rma(d$est, d$var) |>
      summary() |>
      coef()
  })) |>
  select(-data) |>
  unnest(icc)

bc_boot_early_summ <- boot_graph |>
  inner_join(baseline_lengths |> filter(t_min <= -3000)) |>
  filter(!is.na(lower), !is.na(upper)) |>
  mutate(
    stdev = (upper - lower) / (1.96 * 2),
    var = stdev**2
  ) |>
  group_by(b_start, b_end) |>
  nest() |>
  mutate(icc = map(data, \(d){
    rma(d$est, d$var) |>
      summary() |>
      coef()
  })) |>
  select(-data) |>
  unnest(icc)

no_bc_accs_kid_cdi <- readRDS(here("cached_intermediates", "3_accs_kid_cdi.rds")) |> filter(t_start==500, t_end==4000)

bc_accs_kid_cdi <- readRDS(here("cached_intermediates", "7_bc_accs_kid_cdi.rds")) |>bind_rows(no_bc_accs_kid_cdi) |> 
  mutate(b_start=ifelse(is.na(b_start), "none", as.character(b_start)))

  baseline_lengths <- d_aoi |>
    group_by(dataset_name) |>
    summarise(t_min = min(t_norm))
  
bc_accs_kid_cdi_summ <-  bc_accs_kid_cdi |>
  left_join(baseline_lengths) |> filter(t_min< 0) |> 
  mutate(
    comp_stdev = (comp_upper - comp_lower) / (1.96 * 2),
    comp_var = comp_stdev**2,
    prod_stdev = (prod_upper - prod_lower) / (1.96 * 2),
    prod_var = prod_stdev**2,
    age_stdev = (age_upper - age_lower) / (1.96 * 2),
    age_var = age_stdev**2
  ) |>
  group_by(t_start, t_end, b_start, b_end) |>
  nest() |>
  mutate(comp= map(data, \(d){
    rma(d$comp_est, d$comp_var) |>
      summary() |>
      coef()
  }),
  prod= map(data, \(d){
    rma(d$prod_est, d$prod_var) |>
      summary() |>
      coef()
  }),
  age= map(data, \(d){
    rma(d$age_est, d$age_var) |>
      summary() |>
      coef()
  })) |>
  select(-data) |>
  unnest(c(comp, prod, age), names_sep="_")

  
  bc_accs_kid_cdi_summ_early <-  bc_accs_kid_cdi |>
    left_join(baseline_lengths) |> filter(t_min< -3000) |> 
  mutate(
    comp_stdev = (comp_upper - comp_lower) / (1.96 * 2),
    comp_var = comp_stdev**2,
    prod_stdev = (prod_upper - prod_lower) / (1.96 * 2),
    prod_var = prod_stdev**2,
    age_stdev = (age_upper - age_lower) / (1.96 * 2),
    age_var = age_stdev**2
  ) |>
  group_by(t_start, t_end, b_start, b_end) |>
  nest() |>
  mutate(comp= map(data, \(d){
    rma(d$comp_est, d$comp_var) |>
      summary() |>
      coef()
  }),
  prod= map(data, \(d){
    rma(d$prod_est, d$prod_var) |>
      summary() |>
      coef()
  }),
  age= map(data, \(d){
    rma(d$age_est, d$age_var) |>
      summary() |>
      coef()
  })) |>
  select(-data) |>
  unnest(c(comp, prod, age), names_sep="_")

no_bc_accs_kid_test_retest <- readRDS(here("cached_intermediates", "3_accs_boot_kid_test_retest.rds")) |> filter(t_start==500, t_end==4000)

bc_accs_kid_test_retest <- readRDS(here("cached_intermediates", "7_bc_accs_boot_kid_test_retest.rds")) |> bind_rows(no_bc_accs_kid_test_retest) |> 
  mutate(b_start=ifelse(is.na(b_start), "none", as.character(b_start)))

library(metafor)

  baseline_lengths <- d_aoi |>
    group_by(dataset_name) |>
    summarise(t_min = min(t_norm))

bc_accs_kid_test_retest_summ <-  bc_accs_kid_test_retest |>
    left_join(baseline_lengths) |> filter(t_min< 0) |> 
  mutate(
    stdev = (upper - lower) / (1.96 * 2),
    var = stdev**2,
  ) |>
  group_by(b_start, b_end) |>
  nest() |>
  mutate(corr= map(data, \(d){
    rma(d$est, d$var) |>
      summary() |>
      coef()
  })) |> 
  unnest(corr)
  

bc_accs_kid_test_retest_summ_early <-  bc_accs_kid_test_retest |>
    left_join(baseline_lengths) |> filter(t_min< -3000) |> 
  mutate(
    stdev = (upper - lower) / (1.96 * 2),
    var = stdev**2,
  ) |>
  group_by(b_start, b_end) |>
  nest() |>
  mutate(corr= map(data, \(d){
    rma(d$est, d$var) |>
      summary() |>
      coef()
  })) |> 
  unnest(corr)


```

Comparison between baseline-corrected accuracy with different baseline windows and accuracy with no baseline-correction. All datsets with data before time 0 were included.

```{r}
bc_acc_all <- bc_boot_summ |> mutate(Type="ICC reliability") |> 
  bind_rows(bc_accs_kid_cdi_summ |> rename(estimate=prod_estimate, ci.lb=prod_ci.lb, ci.ub=prod_ci.ub) |> mutate(Type="Corr. with CDI Production")) |> 
  bind_rows(bc_accs_kid_cdi_summ |> rename(estimate=comp_estimate, ci.lb=comp_ci.lb, ci.ub=comp_ci.ub) |> mutate(Type="Corr. with CDI Comprehension")) |> 
  bind_rows(bc_accs_kid_test_retest_summ |> mutate(Type="Test-retest reliability")) |> 
  mutate(b_end=ifelse(is.na(b_end), "no baseline", as.character(b_end)),
         b_start=ifelse(b_start=="none", "no baseline", b_start)) 

bc_acc_all |> ggplot(aes(x=b_start, col=b_end, y=estimate, ymin=ci.lb, group=t_end, ymax=ci.ub))+geom_pointrange(position=position_dodge2(width=.5))+coord_flip()+facet_wrap(~Type, scales="free_x", nrow=1)+theme(legend.position = "bottom", )+labs(y="Correlation", x="Baseline start time (ms)", color="Baseline end time (ms)")+scale_color_solarized()
```

Comparison between baseline-corrected accuracy with different baseline windows and accuracy with no baseline-correction on the subset of datasets with at least 3 seconds of pre-stimulus onset looking data.
```{r}
bc_acc_early <- bc_boot_early_summ |> mutate(Type="ICC reliability") |> 
  bind_rows(bc_accs_kid_cdi_summ_early |> rename(estimate=prod_estimate, ci.lb=prod_ci.lb, ci.ub=prod_ci.ub) |> mutate(Type="Corr. with CDI Production")) |> 
  bind_rows(bc_accs_kid_cdi_summ_early |> rename(estimate=comp_estimate, ci.lb=comp_ci.lb, ci.ub=comp_ci.ub) |> mutate(Type="Corr. with CDI Comprehension")) |> 
  mutate(b_end=ifelse(is.na(b_end), "no baseline", as.character(b_end)),
         b_start=ifelse(b_start=="none", "no baseline", b_start)) 

bc_acc_early |> ggplot(aes(x=b_start, col=b_end, y=estimate, ymin=ci.lb, group=t_end, ymax=ci.ub))+geom_pointrange(position=position_dodge2(width=.5))+coord_flip()+facet_wrap(~Type, scales="free_x", nrow=1)+theme(legend.position = "bottom", )+labs(y="Correlation", x="Baseline start time (ms)", color="Baseline end time (ms)")+scale_color_solarized()
```


## Recommendation 3: Use RT with a minimum RT of around 400 ms. 

While for accuracy there are relatively few dimensions of variability, RT can be measured in a number of ways. 

One choice is whether to measure RT based on when a child's eyes launch from the distractor or when a child's eyes land on the target. Launches could be a more precise index of the timing of intent to look at the target, but leaving from the distractor could also be done with intent to look elsewhere. Thus, launch based analyses sometimes exclude trials where it takes a child too long to go from distractor to target (shift-length cutoff).

Short RTs are often excluded because they were necessarily or likely to not be as a response to the stimulus. We explore a range of different potential cutoffs in at attempt to maximize at what point RTs shift from mostly being random to mostly being stimulus driven. 

RTs can be analysed either on a log scale or a raw RT scale.

Our initial exploration suggested that ICC reliability was maximized for log-scale landing-based RT without any shift-length cutoff with minimum RTs around 400-500ms.  

For the main validity and reliability measures, we compare a launch-based RT with a 600ms shift-length cutoff (as used in XX) and a land-based RT without a shift-length cutoff. We vary the minimum RT and whether the RTs are logged or not. 

```{r}

# we require 10 trials and 5 kids 
rt_dist <- readRDS(here("cached_intermediates", "4_rts.rds")) |> filter(shift_type=="D-T") |> 
  group_by(time_0, time_end, during, frac, dataset_name, window) |> summarize(trials=n(), admins=administration_id |> unique() |> length()) 
  
icc_cutoff <- rt_dist |> filter(trials>9, admins>4) |> select(dataset_name, time_0, time_end, during, frac, window)


rt_boot <- readRDS(here("cached_intermediates", "4_rt_iccs_boot.rds")) |> inner_join(icc_cutoff) |> 
    mutate(
    type = case_when(
      str_detect(measure, "first_launch_rt") ~ "first_launch",
      str_detect(measure, "last_launch_rt") ~ "last_launch",
      str_detect(measure, "land_rt") ~ "land"
    ),
    logged = case_when(
      str_detect(measure, "log") ~ "log",
      T ~ "raw"
    ),
    trimming = case_when(
      str_detect(measure, "trim_first") ~ "trim_first",
      str_detect(measure, "trim_last") ~ "trim_last",
      T ~ "untrimmed"
    )
  ) |>
  mutate(approach = case_when(
    type == "land" & trimming == "untrimmed" & frac == "0" ~ "generous",
    type == "land" & trimming == "untrimmed" & frac == "1" ~ "trad_land",
    type == "first_launch" & trimming == "trim_first" & frac == "1" ~ "trad_launch"
  )) |>
  filter(!is.na(approach)) |> 
  group_by(dataset_name) |> 
  mutate(options_count=n()) |> 
  filter(dataset_name!="ferguson_eyetrackingr") |> 
  select(-options_count) |> ungroup()

library(metafor)
rt_boot_summ <- rt_boot |>
  filter(!is.na(lower), !is.na(upper)) |>
  mutate(
    stdev = (upper - lower) / (1.96 * 2),
    var = stdev**2
  ) |>
  group_by(window, during, logged, approach) |>
  nest() |>
  mutate(icc = map(data, \(d){
    rma(d$est, d$var) |>
      summary() |>
      coef()
  })) |>
  select(-data) |>
  unnest(icc)

rt_kid_cdi <- readRDS(here("cached_intermediates", "4_rt_kid_cdi.rds")) |> filter(dataset_name!="ferguson_eyetrackingr")

library(metafor)

rt_kid_cdi_summ <-  rt_kid_cdi |>
  mutate(
    comp_stdev = (comp_upper - comp_lower) / (1.96 * 2),
    comp_var = comp_stdev**2,
    prod_stdev = (prod_upper - prod_lower) / (1.96 * 2),
    prod_var = prod_stdev**2,
    age_stdev = (age_upper - age_lower) / (1.96 * 2),
    age_var = age_stdev**2
  ) |>
  group_by(approach, window, logged) |>
  nest() |>
  mutate(comp= map(data, \(d){
    rma(d$comp_est, d$comp_var) |>
      summary() |>
      coef()
  }),
  prod= map(data, \(d){
    rma(d$prod_est, d$prod_var) |>
      summary() |>
      coef()
  }),
  age= map(data, \(d){
    rma(d$age_est, d$age_var) |>
      summary() |>
      coef()
  })) |>
  select(-data) |>
  unnest(c(comp, prod, age), names_sep="_")

rt_kid_test_retest <- readRDS(here("cached_intermediates", "4_rt_kid_test_retest.rds")) 

library(metafor)
rt_kid_test_retest_summ <-  rt_kid_test_retest |>
  mutate(
    stdev = (upper - lower) / (1.96 * 2),
    var = stdev**2,
  ) |>
  group_by(approach, window, logged) |>
  nest() |>
  mutate(corr= map(data, \(d){
    rma(d$est, d$var) |>
      summary() |>
      coef()
  })) |> 
  unnest(corr)
```

Reliability and validity measures for different accuracy windows. Ranges are bootstrapped 95% CIs.
```{r}
rt_all <- rt_boot_summ |> mutate(Type="ICC reliability") |> 
  bind_rows(rt_kid_cdi_summ |> rename(estimate=prod_estimate, ci.lb=prod_ci.lb, ci.ub=prod_ci.ub) |> mutate(Type="Corr. with CDI Production")) |> 
  bind_rows(rt_kid_cdi_summ |> rename(estimate=comp_estimate, ci.lb=comp_ci.lb, ci.ub=comp_ci.ub) |> mutate(Type="Corr. with CDI Comprehension")) |> 
  bind_rows(rt_kid_test_retest_summ |> mutate(Type="Test-retest reliability")) |> 
  filter(approach%in%c("trad_land", "trad_launch")) |> 
  mutate(approach=ifelse(approach=="trad_land", "land-based", "launch-based"))

#rt_all |> ggplot(aes(x=str_c(logged, " ", approach, " RT"), col=as.character(window), y=estimate, ymin=ci.lb, ymax=ci.ub))+geom_pointrange(position=position_dodge(.5))+coord_flip()+facet_wrap(~Type, scales="free_x", nrow=1)+theme(legend.position = "bottom", )+labs(y="Correlation", x="Type of RT", color="Minimum RT")+scale_color_viridis(discrete=T)

rt_all |> mutate(type=str_c(logged, " ", approach, " RT") |> factor(levels=c("raw launch-based RT", "log launch-based RT", "raw land-based RT", "log land-based RT"))) |> ggplot(aes(col=type, x=as.character(window), y=estimate, ymin=ci.lb, ymax=ci.ub))+geom_pointrange(position=position_dodge(.5))+coord_flip()+facet_wrap(~Type, scales="free_x", nrow=1)+theme(legend.position = "bottom", )+labs(y="Correlation", x="Minimum RT", color="Type of RT")+scale_color_brewer(palette="Paired")
```
Among the 4 types of RT we considered in more detail (log v raw x launch-based v land-based), all have similar correlations with CDI comprehension, and correlations are negative as smaller (faster) RT is associated with larger CDI scores. Log RT has a slightly stronger (more negative) correlation with CDI production than raw RT especially for minimum RTs in the 400-450 range. The highest ICC reliabilities occur around 400 ms. ICC reliability is consistently slightly higher for land-based than launch-based although the scale of difference varies with minimum RT. Test-retest reliability is reasonable for all measures. 

We recommend that researchers use minimum RT cutoffs around 400 ms as these increase reliability compared to shorter 200 ms cutoffs. While the differences between different methods are small, we recommend using log landing-based RT to maximize overall reliability and validity. 

Finally, trials are generally only included if the child is looking at the distractor at the point of disambiguation and continues looking at the distractor for the entire minimum RT period. We found that a more lax threshold of looking at the distractor and 0 and 400ms and not looking at the target between 0 and 400ms can be used for landing-based RT instead of requiring children to be continuously on the distractor, with no loss of reliability or validity. However, little bits of missing data were rare, and is likely to be very dataset specific depending on the tracking and imputation process. 

## Recommendation 4: Excluding trials or children for "zoning" or low numbers of trials is generally unnecessary.

Children may engage in various behavior that seems to not be responsive to the trials, including "zoning", contributing few valid looking points on a trial, or contributing few trials overall. One question is whether excluding these types of trials improves overall validity and reliability. 

On a trial-by-trial basis, excluding trials where kids don't look at both target and distractor (either during a pre-window, or ever) does not improve ICC reliability or the correlation with CDI measures. Additionally, excluding trials with looks away from the target and distractor also does not improve ICC reliability or correlation with CDI measures. We do not recommend any trial-level exclusions to accuracy data. 

On a administration basis, common exclusions are for cross-trial side-bias and minimum number of trials contributed. Very few children show strong side bias; in this dataset around 1% children who contributed .5% of trials looked to one side more than 90% of the time.

Thus, exclusions for side bias have minimum impact on reliability and we do not recommend them. We also do not find that setting a minimum number of trials for inclusion improves ICC or CDI validity. 

For RTs, exclusions based on number of trials does not improve ICC or correlations with CDI. 

## Recommendation 5: When measuring individual differences, excluding children with fewer than 2 RT measures or 3 accuracy measures may be reasonable. 
The one case where exclusions based on minimum trial count may be helpful is for individual difference measures where high test-retest reliability is desired. However, the children who contribute fewer trials might vary in other ways from kids who contribute more so exclusions may hurt generalizability in addition to decreasing effective sample size. 

While our results are uncertain, it appears that test-retest reliability is higher when children have to have 2 RTs from each testing session. Reliability may also increase slightly over the 2-5 RT minimum, but the data loss is substantial. For analysis of archival datasets, tradeoffs between better measurement of each child and fewer children must be considered. 

For accuracy, results were again uncertain, but a minimum of 3 accuracy trials had slightly better test-retest reliability than 2 trials (which in turn was better than 1). Most children had plenty of accuracy trials, so data loss is unlikely to be a concern. 

[TODO possible figure for these]

# Design recommendations 

## Recommendation 5: For individual differences, plan for at least 10 trials and ideally closer to 30 trials per child. 

While it is difficult to tell whether post-hoc exclusion of children who contributed too few data points makes sense for individual difference work, it is much clearer that it is better to design experiments to increase the likelihood of having more valid trials.  

To understand how different numbers of trials would impact test-retest reliability, we sampled trials from each administration with replacement. This allows for up-sampling of children with low numbers of trials, simulating what might have happened in a longer experiment. 

```{r}
admins <- d_aoi |>
  select(dataset_name, subject_id, administration_id, age) |>
  distinct()
repeated <- admins |>
  group_by(dataset_name, subject_id) |>
  tally() |>
  filter(n > 1)

repeated_subjects <- admins |> inner_join(repeated)

# identify pairs
# hmm, sometimes we have times with 3+ 1 month apart,
# for now allow all the pairs that result so like 14-15 and 15-16 for the same kid

pairs <- repeated_subjects |>
  group_by(dataset_name, subject_id) |>
  mutate(
    forward_age = lead(age),
    forward_diff = forward_age - age,
    test_num = case_when(
      forward_diff < 1.5 ~ 1,
    ),
    mean_age = case_when(
      test_num == 1 ~ (age + forward_age) / 2,
    ),
    second_admin = case_when(
      test_num == 1 ~ lead(administration_id)
    )
  ) |>
  filter(!is.na(test_num)) |>
  rename(first_admin = administration_id) |>
  select(-n, -age) |>
  left_join(repeated_subjects |> select(-age, -n), by = c("dataset_name", "subject_id", "second_admin" = "administration_id")) |>
  ungroup() |>
  mutate(pair_number = row_number()) |>
  filter(!(dataset_name=="adams_marchman_2018" & mean_age>28)) |> # these do have multiple sessions but with very different items banks for the two sessions!
  select(-forward_age, -forward_diff, -test_num)

pairs_long <- pairs |> pivot_longer(c("first_admin", "second_admin"), names_to = "session_num", values_to = "administration_id")


acc <- d_aoi |>
  inner_join(pairs_long |> select(dataset_name, administration_id, subject_id) |> distinct()) |>
  filter(t_norm >= 500) |>
  filter(t_norm <= 4000) |>
  group_by(dataset_name, subject_id, administration_id, trial_id, trial_order, target_label) |>
  summarize(acc = mean(correct, na.rm = T)) |>
  filter(!is.na(acc))

acc_mean <- acc |>
  group_by(dataset_name, subject_id, administration_id) |>
  summarize(acc_trials = sum(!is.na(acc)), acc = mean(acc, na.rm = T))

rt <- readRDS(here("cached_intermediates", "4_rt_canonical.rds")) |>
  inner_join(pairs_long |> select(dataset_name, administration_id, subject_id) |> distinct()) |>
  filter(shift_type == "D-T") |> filter(approach=="trad_launch") |> filter(logged=="raw") |> filter(!is.na(rt))

rt_mean <- rt |>
  group_by(dataset_name, subject_id, administration_id) |>
  summarize(rt_trials = n(), mean_rt = mean(rt))
```

```{r}
all <- pairs_long |>
  left_join(acc_mean) |>
  left_join(rt_mean)

all_wide <- all |>
  mutate(session_num = ifelse(session_num == "first_admin", 1, 2)) |>
  pivot_wider(values_from = c("administration_id", "acc", "acc_trials", "rt_trials", "mean_rt"), names_from = session_num) |>
  mutate(age_bin = case_when(
    mean_age < 18 ~ "12-17",
    mean_age < 21 ~ "18-20",
    mean_age < 28 ~ "21-27",
    mean_age < 33 ~ "28-32",
    mean_age >= 33 ~ "33+"
  ))
```

Simulations of test-retest reliability from down- and up-sampling accuracy trials. 

```{r}

eligible <- all_wide |>
  select(dataset_name, subject_id, mean_age, pair_number, administration_id_1, administration_id_2) |>
  pivot_longer(c("administration_id_1", "administration_id_2"), names_to = "session_num", values_to = "administration_id") |>
  select(dataset_name, subject_id, administration_id)

acc_resample <- expand_grid(num_samples = c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 18, 20, 25, 30), iter=1:10) |>
  mutate(accs = map(num_samples, \(num) {
    acc |>
      group_by(dataset_name, subject_id, administration_id) |>
      inner_join(eligible, by=c("dataset_name", "subject_id", "administration_id")) |>
      group_by(dataset_name, subject_id, administration_id) |>
      slice_sample(n=num,replace=T) |>
      summarize(acc_trials = n(), mean_acc = mean(acc))
  })) |>
  unnest(accs)


acc_corr_resample <- pairs_long |>
  left_join(acc_resample) |>
  mutate(session_num = ifelse(session_num == "first_admin", 1, 2)) |>
  filter(!is.na(num_samples)) |>
  pivot_wider(values_from = c("administration_id", "acc_trials", "mean_acc"), names_from = session_num) |>
  mutate(age_bin = case_when(
    mean_age < 18 ~ "12-17",
    mean_age < 21 ~ "18-20",
    mean_age < 28 ~ "21-27",
    mean_age < 33 ~ "28-32",
    mean_age >= 33 ~ "33+"
  )) |>
  filter(!is.na(mean_acc_1)) |>
  filter(!is.na(mean_acc_2)) |>
  group_by(dataset_name, age_bin, num_samples, iter) |>
  summarise(
    cor_acc = ifelse(sum(!is.na(mean_acc_1) & !is.na(mean_acc_2)) > 2, cor.test(mean_acc_1, mean_acc_2)$estimate, NA)  )

ggplot(acc_corr_resample, aes(x = num_samples, y = cor_acc)) +
  #scale_color_viridis(discrete = T) +
  stat_summary(position = position_dodge(.5)) +
  geom_smooth()+
  stat_summary(position = position_dodge(.5), geom = "line") +
  geom_hline(yintercept = 0)+geom_vline(xintercept=10, lty="dashed")+geom_vline(xintercept=15, lty="dashed")
```

Simulations of test-retest reliability from down- and up-sampling accuracy trials. 

```{r}

eligible <- all_wide |>
  select(dataset_name, subject_id, mean_age, pair_number, administration_id_1, administration_id_2) |>
  pivot_longer(c("administration_id_1", "administration_id_2"), names_to = "session_num", values_to = "administration_id") |>
  select(dataset_name, subject_id, administration_id)

rt_resample <- expand_grid(num_samples = c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15), iter=1:10) |>
  mutate(rts = map(num_samples, \(num) {
    rt |>
      group_by(dataset_name, subject_id, administration_id) |>
      inner_join(eligible, by=c("dataset_name", "subject_id", "administration_id")) |>
      group_by(dataset_name, subject_id, administration_id) |>
      slice_sample(n=num,replace=T) |>
      summarize(rt_trials = n(), mean_rt = mean(rt))
  })) |>
  unnest(rts)


rt_corr_resample <- pairs_long |>
  left_join(rt_resample) |>
  mutate(session_num = ifelse(session_num == "first_admin", 1, 2)) |>
  filter(!is.na(num_samples)) |>
  pivot_wider(values_from = c("administration_id", "rt_trials", "mean_rt"), names_from = session_num) |>
  mutate(age_bin = case_when(
    mean_age < 18 ~ "12-17",
    mean_age < 21 ~ "18-20",
    mean_age < 28 ~ "21-27",
    mean_age < 33 ~ "28-32",
    mean_age >= 33 ~ "33+"
  )) |>
  filter(!is.na(mean_rt_1)) |>
  filter(!is.na(mean_rt_2)) |>
  group_by(dataset_name, age_bin, num_samples, iter) |>
  summarise(
    cor_rt = ifelse(sum(!is.na(mean_rt_1) & !is.na(mean_rt_2)) > 2, cor.test(mean_rt_1, mean_rt_2)$estimate, NA)  )

ggplot(rt_corr_resample, aes(x = num_samples, y = cor_rt)) +
  scale_color_viridis(discrete = T) +
  stat_summary(position = position_dodge(.5)) +
  geom_smooth(se=F)+
  stat_summary(position = position_dodge(.5), geom = "line") +
  geom_hline(yintercept = 0)+geom_vline(xintercept=5, lty="dashed")+geom_vline(xintercept=10, lty="dashed")
```
For accuracy, the benefits of increasing numbers of trials seems to flatten out around 10-15 trials. As nearly all trials produce useable accuracy data, this suggests a target minimum of 10-15 trials. 

For RT, it seems much better to have 2 RTs rather than just 1 whenever possible. On average, 1/3 of trials result in useable RT data, but this varies per child, so having 10 trials is recommended to ensure that most children have at least 2 valid RT trials. Test-retest reliability for RT does seem to increase with more trials, with benefits diminishing after 10 trials. Thus, for individual-differences studies where consistent measures of individual language processing speed are desired, we recommend 30 trials, to end up with an average of 10 RT trials per child. 

10 trials should result in multiple RT measures for each child and a fairly reliable measure of accuracy; however, for individual differences in processing speed, 30 trials is recommended where possible. 

## Recommendation 6: Match target and distractor based on image saliency, in particular animacy. 
[TODO Tarun is leading the image properties analysis so I haven't looked at this part yet.]

Repeating the same image pairs does not have a substantial effect on looking patterns overall. 

# Paper outline

Introduction:

 * Early language is important.
 * LWL is a good method/measure for early language. (what LWL is)
 * LWL is widely used in various paradigms (both longitudinal indiv-differences & experimental paradigms)
 * Reliability and validity are important, but many early childhood measures have lower-than-desired reliability and validity. 
 * LWL has rich time course data that can be cleaned and analysed in different ways. We want to figure out the best practices to maximize reliability and validity to get the most out of the data. 
 * Two main measures -- accuracy and RT. 
 * We leverage peekbank to get broad coverage of vanilla trials often with corresponding CDI data, and look at 2 types of reliability and 2 types of validity. 
 * Contribution: data-driven recommendations based on peekbank for practices to get the most reliability and validity out of LWL data. 
 
Methods: (where are we submitting this -- is it a methods first paper)

 * dataset description of peekbank & of the subset of data used here
 * explanation of 4 reliability and validity metrics, general search strategy, bootstrapping/errorbars as needed
 
Results:

= recommendations above

Discussion:

* summary
* focused on "vanilla" trials, where it's clear what reliability and validity would mean; we don't know how practices would generalize to experimental settings
* limited by datasets which are heterogenous by age/length/items
* often a smooth, wide range of options with similar metrics -- we focus on "best" to inform future practice where one wants to know what is necessary or what (otherwise arbitrary) cutoff to choose, but robust findings will show up robustly over a range of data practices. 

 
 