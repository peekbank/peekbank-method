---
title: "Peekbank method summary"
output: 
  html_document:
    toc: true
---

```{r, echo=F, include=F}
knitr::opts_chunk$set(warning=F, message=F, echo=F)
source(here::here("helper/common.R"))
library(metafor)
library(ggthemes)
```

Here are (draft) recommendations and (draft) supporting figures for the peekbank methods paper. Obviously there is still some work to be done on how to phrase the recommendations. 

Note that quality checks on the data may cause numbers to change, but we don't anticipate large changes in overall results. 

# Summary of approach

The goal of the peekbank-methods project is to give recommendations about the design and analysis of LWL studies. To do so, we need some criteria for what is a better outcome. Here, we take a "data-driven" approach to look at how different potential data-processing and study design factors affect **reliability** and **validity**. 

All of these analyses are done on a subset of the peekbank database limited to "vanilla" trials (real word target, real word distractor, plain carrier phrase) in English with target words that are nouns. 

Throughout we use three different metrics to assess reliabilty and validity. 

1. An intra-class correlation (ICC) as a measure of reliability. Specifically, we use a measure of inter-rater reliability that measures (within a dataset) how much different items (target labels) are consistent with each other in "rating" the subjects (kid-administrations). 

2. Test-retest reliability for measures on the same child at two different administrations within 1.5 months of each other. This measure is only possible on a subset of the datasets that have longitudinal data with some closely spaced repeat administrations. 

3. Correlation with CDI measure for validity. We can check how well our metrics correlate with children's CDI sumscores from CDI administrations at roughly the same time. In general, correlating with production and comprehension in the expected directions is desirable, although we also may expect that LWL measures are picking up on slightly different aspects of language learning than is represented in CDI. 

In order to tell when numerical differences in ICC or correlations could be meaningful, I use bootstrapping to generate per-dataset confidence intervals around the point estimate. I then use the point estimates and the bootstrapped CIs to calculate a variance-weighted meta-analytic estimate (and CI) of the overall correlation across datasets.

This project has a non-standard aim with the statistics. We are neither trying to do just estimation nor are we trying to do "traditional" inferential (p<.05) statistics. We are trying to make inferences as to what data practices to recommend (and how strongly)! 

Often in our other statistical applications, we like to see differences. Here, we expect that some big changes in method should lead to differences in outcome (ex. if an accuracy window started really late at 1000 milliseconds, we expect it be bad), but we also expect that many small differences in approach might lead to minimal differences in outcome. It would be surprising and worrying if small differences in approach often dramatically altered results! 

Other factors may also influence what data practices to choose (and to recommend). In particular, we probably prefer parsimony -- given two processes with equivalent goodness of outcome, we should prefer the simpler one. There may also be theoretical commitments that lead to preferences over one measure or another (ex: log vs raw RT). 

# Data processing recommendations

## Recommendation 1: Measure accuracy over a long  time window, such as from 500 - 4000 ms after the point of disambiguation.

Longer accuracy windows that go to 4000ms post disambiguation, rather than to 2000ms, increase reliability (both ICC and test-retest). Short and long windows show comparable correlations with CDI production data, although shorter windows are somewhat more highly correlated with comprehension data. Together, this suggests that children's looking behaving between 2 and 4 seconds post disambiguation is a stable child-level property, although we cannot say how related it to to child language per se. 

Exactly when the accuracy window starts is not important, although we recommend starting around 500ms to balance between reliability is numerically highest starting at 600 or 700 milliseconds and correlation with production is highest at 500ms. 

While we are recommending accuracy be measured over the 500-4000ms window, we want to be clear that other accuracy windows also have reasonable reliability and validity properties. Strong effects are likely to show up in any reasonable window, but for eking the most reliability out of data, we recommend long windows. 

A heatmap showing ICC (a measure of reliability) for different start and end times for accuracy windows. Lighter colors indicate higher (better) reliability. 

```{r}

accs <- readRDS(here("cached_intermediates/3_accs.Rds"))

accs_summary <- accs |>
  filter(object == "administration") |>
  group_by(t_start, t_end, object) |>
  summarize(
    N = n(),
    mean_icc = mean(icc, na.rm = TRUE)
  ) |>
  mutate(window_size = t_end - t_start)

ggplot(accs_summary, aes(x = t_start, y = t_end, fill = mean_icc)) +
  geom_tile(color = "white") +
  scale_fill_viridis(name = "Mean ICC", option = "inferno") +
  scale_x_continuous(breaks = c(-1000, -500, 0, 500, 1000, 1500)) +
  xlab("Window Start Time (in ms)") +
  ylab("Window End Time (in ms)")

ggsave(here("heat_acc.png"), width=5, height=3)
```


```{r}
acc_boot <- readRDS(here("cached_intermediates", "3_acc_icc_boot.rds"))

acc_boot_summary <- acc_boot |>
  filter(!is.na(lower), !is.na(upper)) |>
  mutate(
    stdev = (upper - lower) / (1.96 * 2),
    var = stdev**2
  ) |>
  group_by(t_start, t_end) |>
  nest() |>
  mutate(icc = map(data, \(d){
    rma(d$est, d$var) |>
      summary() |>
      coef()
  })) |>
  select(-data) |>
  unnest(icc)



accs_cdi_kid <- readRDS(here("cached_intermediates", "3_accs_kid_cdi.rds")) |> mutate(
  prod_est = ifelse(dataset_name=="newman_genderdistractor", NA, prod_est),
  prod_lower = ifelse(dataset_name=="newman_genderdistractor", NA, prod_lower),
  prod_upper = ifelse(dataset_name=="newman_genderdistractor", NA, prod_upper),
)

library(metafor)

accs_cdi_kid_summ <-  accs_cdi_kid |> 
  mutate(
    comp_stdev = (comp_upper - comp_lower) / (1.96 * 2),
    comp_var = comp_stdev**2,
    prod_stdev = (prod_upper - prod_lower) / (1.96 * 2),
    prod_var = prod_stdev**2,
    age_stdev = (age_upper - age_lower) / (1.96 * 2),
    age_var = age_stdev**2
  ) |>
  group_by(t_start, t_end) |>
  nest() |>
  mutate(comp= map(data, \(d){
    rma(d$comp_est, d$comp_var) |>
      summary() |>
      coef()
  }),
  prod= map(data, \(d){
    rma(d$prod_est, d$prod_var) |>
      summary() |>
      coef()
  }),
  age= map(data, \(d){
    rma(d$age_est, d$age_var) |>
      summary() |>
      coef()
  })) |>
  select(-data) |>
  unnest(c(comp, prod, age), names_sep="_")

accs_kid_test_retest <- readRDS(here("cached_intermediates", "3_accs_boot_kid_test_retest.rds"))

accs_kid_test_retest_summ <-  accs_kid_test_retest |>
  mutate(
    stdev = (upper - lower) / (1.96 * 2),
    var = stdev**2,
  ) |>
  group_by(t_start, t_end) |>
  nest() |>
  mutate(corr= map(data, \(d){
    rma(d$est, d$var) |>
      summary() |>
      coef()
  })) |> 
  unnest(corr)
```

Reliability and validity measures for different accuracy windows. Ranges are bootstrapped 95% CIs.
```{r}
acc_all <- acc_boot_summary |> mutate(Type="ICC reliability") |> bind_rows(accs_cdi_kid_summ |> rename(estimate=prod_estimate, ci.lb=prod_ci.lb, ci.ub=prod_ci.ub) |> mutate(Type="Corr. with CDI Production")) |> bind_rows(accs_cdi_kid_summ |> rename(estimate=comp_estimate, ci.lb=comp_ci.lb, ci.ub=comp_ci.ub) |> mutate(Type="Corr. with CDI Comp.")) |> 
  bind_rows(accs_kid_test_retest_summ |> mutate(Type="Test-retest reliability"))

acc_all |> ggplot(aes(x=t_start, col=as.character(t_end), y=estimate, ymin=ci.lb, group=t_end, ymax=ci.ub))+geom_pointrange(position=position_dodge(50))+coord_flip()+facet_wrap(~Type, scales="free_x", nrow=1)+theme(legend.position = "bottom", )+labs(y="Correlation", x="Window start time (ms)", color="Window end time (ms)")+scale_color_solarized()

ggsave("acc.png")
```

Other approaches to aggregation!


```{r}
acc_boot <- readRDS(here("cached_intermediates", "3_acc_icc_boot.rds"))

acc_boot_summary_1 <- acc_boot |>
  filter(!is.na(lower), !is.na(upper)) |>
  mutate(
    stdev = (upper - lower) / (1.96 * 2),
    var = stdev**2
  ) |>
  group_by(t_start, t_end) |>
  nest() |>
  mutate(icc = map(data, \(d){
    rma(d$est, d$var) |>
      summary() |>
      predict() |> as_tibble()
  })) |>
  select(-data) |>
  unnest(icc) |> 
  mutate(type="1")

acc_boot_summary_2 <- acc_boot |>
  filter(!is.na(lower), !is.na(upper)) |>
  mutate(
    stdev = (upper - lower) / (1.96 * 2),
    var = stdev**2,
    condition=str_c(t_start, "_", t_end)
  ) |> select(dataset_name, condition, var, est) |> nest() |> 
  mutate(icc = map(data, \(d){
    rma(yi=est, vi=var, mods=~condition, data=d) |> #summary() |> coef() |> as_tibble(rownames="coef") |> View()
      
      predict() |> as_tibble()
  })) |>
  unnest(icc, data) |> 
  select(condition, pred, ci.lb, ci.ub) |> distinct() |> 
  separate(condition, sep="_", c("t_start", "t_end")) |> 
  mutate(t_start=as.numeric(t_start), t_end=as.numeric(t_end)) |> mutate(type="2")

acc_boot_summary_3 <- acc_boot |>
  filter(!is.na(lower), !is.na(upper)) |>
  mutate(
    stdev = (upper - lower) / (1.96 * 2),
    var = stdev**2,
    condition=str_c(t_start, "_", t_end)
  ) |> select(dataset_name, condition, var, est) |> nest() |> 
  mutate(icc = map(data, \(d){
    rma.mv(yi=est, V=var, mods=~condition, random=~1|dataset_name, data=d) |> #summary() |> coef() |> as_tibble(rownames="coef") |> View()
      
    
    predict() |> as_tibble()
  })) |>
  unnest(icc, data) |> 
  select(condition, pred, ci.lb, ci.ub) |> distinct() |> 
  separate(condition, sep="_", c("t_start", "t_end")) |> 
  mutate(t_start=as.numeric(t_start), t_end=as.numeric(t_end)) |> mutate(type="3")

acc_boot_summary_4 <- acc_boot |>
  filter(!is.na(lower), !is.na(upper)) |>
  mutate(
    stdev = (upper - lower) / (1.96 * 2),
    var = stdev**2,
    condition=str_c(t_start, "_", t_end)
  ) |> select(dataset_name, condition, var, est) |> nest() |> 
  mutate(icc = map(data, \(d){
    rma.mv(yi=est, V=var, mods=~condition, random=~1|dataset_name, data=d) |> summary() |> coef() |> as_tibble(rownames="coef")
    })) |>
  unnest(icc) |> 
  select(-data) |> 
  mutate(coef=str_replace(coef, "condition", "")) |> 
  separate(coef, sep="_", c( "t_start", "t_end")) |> 
  mutate(t_start=as.numeric(t_start), t_end=as.numeric(t_end)) |> mutate(type="4") |> filter(!is.na(t_start)) 

acc_boot_summary_5 <- acc_boot |> 
   filter(!is.na(lower), !is.na(upper)) |>
  mutate(
    stdev = (upper - lower) / (1.96 * 2),
    var = stdev**2,
    condition=str_c(t_start, "_", t_end)
  ) |> select(dataset_name, condition, var, est) |> 
  group_by(dataset_name) |> 
  mutate(dataset_mean=mean(est),
         centered_est=est-dataset_mean)|> ungroup() |>  nest() |> 
  mutate(icc = map(data, \(d){
    rma.mv(yi=centered_est, V=var, mods=~0+condition, random=~1|dataset_name, data=d) |> summary() |> coef() |> as_tibble(rownames="coef")
    })) |>
  unnest(icc) |> 
  select(-data) |> 
  mutate(coef=str_replace(coef, "condition", "")) |> 
  separate(coef, sep="_", c( "t_start", "t_end")) |> 
  mutate(t_start=as.numeric(t_start), t_end=as.numeric(t_end)) |> mutate(type="5") |> filter(!is.na(t_start)) 

acc_boot_summary_6 <- acc_boot |> 
   filter(!is.na(lower), !is.na(upper)) |>
  filter(t_end=="4000") |> 
  mutate(
    stdev = (upper - lower) / (1.96 * 2),
    var = stdev**2,
    condition=str_c(t_start, "_", t_end)
  ) |> select(dataset_name, condition, var, est) |> 
  group_by(dataset_name) |> 
  mutate(dataset_mean=mean(est),
         centered_est=est-dataset_mean)|> ungroup() |>  nest() |> 
  mutate(icc = map(data, \(d){
    rma.mv(yi=centered_est, V=var, mods=~0+condition, random=~1|dataset_name, data=d) |> summary() |> coef() |> as_tibble(rownames="coef")
    })) |>
  unnest(icc) |> 
  select(-data) |> 
  mutate(coef=str_replace(coef, "condition", "")) |> 
  separate(coef, sep="_", c( "t_start", "t_end")) |> 
  mutate(t_start=as.numeric(t_start), t_end=as.numeric(t_end)) |> mutate(type="6") |> filter(!is.na(t_start)) 
  
acc_boot_summary_3 |> bind_rows(acc_boot_summary_2)  |> bind_rows(acc_boot_summary_1)|>
  bind_rows(acc_boot_summary_4) |> ggplot(aes(x=t_start, col=type, y=pred, ymin=ci.lb, group=type, ymax=ci.ub))+geom_pointrange(position=position_dodge(70))+coord_flip()+theme(legend.position = "bottom", )+labs(y="Correlation", x="Window start time (ms)", color="Window end time (ms)")+scale_color_solarized()+facet_wrap(~t_end)


acc_boot_summary_3 |> bind_rows(acc_boot_summary_2) |> bind_rows(acc_boot_summary_1)|>
  bind_rows(acc_boot_summary_4) |> ggplot(aes(x=t_start, col=as.character(t_end), y=pred, ymin=ci.lb, group=t_end, ymax=ci.ub))+geom_pointrange(position=position_dodge(70))+coord_flip()+theme(legend.position = "bottom", )+labs(y="Correlation", x="Window start time (ms)", color="Window end time (ms)")+scale_color_solarized()+facet_wrap(~type)

acc_boot_summary_5|> bind_rows(acc_boot_summary_4) |> bind_rows(acc_boot_summary_6) |>  ggplot(aes(x=t_start, col=as.character(t_end), y=estimate, ymin=ci.lb, group=t_end, ymax=ci.ub))+geom_pointrange(position=position_dodge(70))+coord_flip()+theme(legend.position = "bottom", )+labs(y="Correlation", x="Window start time (ms)", color="Window end time (ms)")+scale_color_solarized()+ facet_wrap(~type)+geom_hline(yintercept=0)


acc_boot_summary_5|> bind_rows(acc_boot_summary_4) |> bind_rows(acc_boot_summary_6) |>  ggplot(aes(x=t_start, col=type, y=estimate, ymin=ci.lb, group=type, ymax=ci.ub))+geom_pointrange(position=position_dodge(70))+coord_flip()+theme(legend.position = "bottom", )+labs(y="Correlation", x="Window start time (ms)", color="Window end time (ms)")+scale_color_solarized()+ facet_wrap(~t_end)+geom_hline(yintercept=0)
```

```{r, eval=F}
acc_boot <- readRDS(here("cached_intermediates", "3_acc_icc_boot.rds"))

acc_boot |> filter(t_start==500, t_end==4000) |> ggplot(aes(x=dataset_name, y=est, ymin=lower, group=t_end, ymax=upper))+geom_pointrange(position=position_dodge(.3), color="purple")+coord_flip()+theme(legend.position = "bottom", )+labs(y="ICC", x="Dataset", color="Window end time (ms)")+scale_color_solarized()+geom_pointrange(data=acc_boot_summary |> filter(t_start==500, t_end==4000), aes(x=" Meta-analytic aggregate", y=estimate, ymin=ci.lb, ymax=ci.ub), color="black")


ggsave("single_boot.png", width=5, height=2.5)
acc_boot_summary|> ggplot(aes(x=t_start, col=as.character(t_end), y=estimate, ymin=ci.lb, group=t_end, ymax=ci.ub))+geom_pointrange(position=position_dodge(50))+coord_flip()+theme(legend.position = "bottom", )+labs(y="Correlation", x="Window start time (ms)", color="Window end time (ms)")+scale_color_solarized()

ggsave("acc_boot.png", width=5, height=2.5)

```


```{r}
acc_boot_age <- readRDS(here("cached_intermediates", "3_acc_icc_boot_age.rds"))

acc_boot_age_summary <- acc_boot_age |>
  filter(!is.na(lower), !is.na(upper)) |>
  mutate(
    stdev = (upper - lower) / (1.96 * 2),
    var = stdev**2
  ) |>
  group_by(t_start, t_end, younger) |>
  nest() |>
  mutate(icc = map(data, \(d){
    rma(d$est, d$var) |>
      summary() |>
      coef()
  })) |>
  select(-data) |>
  unnest(icc)

acc_boot_age_summary|>   mutate(age = ifelse(younger,  "<24 months", ">=24 months")) |> 
ggplot(aes(x=t_start, col=as.character(t_end), y=estimate, ymin=ci.lb, group=t_end, ymax=ci.ub))+geom_pointrange(position=position_dodge(50))+coord_flip()+theme(legend.position = "bottom", )+labs(y="Correlation", x="Window start time (ms)", color="Window end time (ms)")+scale_color_solarized()+facet_wrap(~age)
```



## Recommendation 2: Don't use baseline correction to control for variation in image saliency. 

Different images have varying levels of saliency to infants, which can drive uneven looking patterns at baseline, before auditory stimulus. Sometimes, researchers address this by using baseline-corrected accuracy, a difference between the looking fraction after the stimulus and the looking fraction before the stimulus. In general, we found that baseline-corrected accuracy led to worse reliability and validity than accuracy. For datasets where there was looking data for a long pre-stimulus period (at least 3 seconds), baseline correction using a long pre-stimulus period was as reliable as accuracy without baseline-correction, but still had marginally worse validity.

[TODO we could therorize on why baseline-correction is worse -- difference measures are generally high variance]

As alternatives to baseline correction, researchers could account for item-level variability in their statistical models. Additionally, better matching of stimulus properties between the target and distractor pairs may reduce unequal looking before stimulus onset (see Recommendation # YY). 

```{r}
d_aoi <- readRDS(here("cached_intermediates", "1_d_aoi.Rds"))

bc_boot <- readRDS(here("cached_intermediates", "7_bc_accs_icc_boot.rds")) 
no_bc_boot <- readRDS(here("cached_intermediates", "3_acc_icc_boot.rds")) |> filter(t_start == 500, t_end == 4000)

baseline_lengths <- d_aoi |>
  group_by(dataset_name) |>
  summarise(t_min = min(t_norm))
boot_graph <- bc_boot |>
  bind_rows(no_bc_boot) |>   inner_join(baseline_lengths |> filter(t_min < 0)) |>
  mutate(b_start = ifelse(is.na(b_start), "none", as.character(b_start)))

library(metafor)
bc_boot_summ <- boot_graph |>
  inner_join(baseline_lengths |> filter(t_min < 0)) |>
  filter(!is.na(lower), !is.na(upper)) |>
  mutate(
    stdev = (upper - lower) / (1.96 * 2),
    var = stdev**2
  ) |>
  group_by(b_start, b_end) |>
  nest() |>
  mutate(icc = map(data, \(d){
    rma(d$est, d$var) |>
      summary() |>
      coef()
  })) |>
  select(-data) |>
  unnest(icc)

bc_boot_early_summ <- boot_graph |>
  inner_join(baseline_lengths |> filter(t_min <= -3000)) |>
  filter(!is.na(lower), !is.na(upper)) |>
  mutate(
    stdev = (upper - lower) / (1.96 * 2),
    var = stdev**2
  ) |>
  group_by(b_start, b_end) |>
  nest() |>
  mutate(icc = map(data, \(d){
    rma(d$est, d$var) |>
      summary() |>
      coef()
  })) |>
  select(-data) |>
  unnest(icc)

no_bc_accs_kid_cdi <- readRDS(here("cached_intermediates", "3_accs_kid_cdi.rds")) |> filter(t_start==500, t_end==4000)

bc_accs_kid_cdi <- readRDS(here("cached_intermediates", "7_bc_accs_kid_cdi.rds")) |>bind_rows(no_bc_accs_kid_cdi) |> 
  mutate(b_start=ifelse(is.na(b_start), "none", as.character(b_start)))

  baseline_lengths <- d_aoi |>
    group_by(dataset_name) |>
    summarise(t_min = min(t_norm))
  
bc_accs_kid_cdi_summ <-  bc_accs_kid_cdi |>
  left_join(baseline_lengths) |> filter(t_min< 0) |> 
  mutate(
    comp_stdev = (comp_upper - comp_lower) / (1.96 * 2),
    comp_var = comp_stdev**2,
    prod_stdev = (prod_upper - prod_lower) / (1.96 * 2),
    prod_var = prod_stdev**2,
    age_stdev = (age_upper - age_lower) / (1.96 * 2),
    age_var = age_stdev**2
  ) |>
  group_by(t_start, t_end, b_start, b_end) |>
  nest() |>
  mutate(comp= map(data, \(d){
    rma(d$comp_est, d$comp_var) |>
      summary() |>
      coef()
  }),
  prod= map(data, \(d){
    rma(d$prod_est, d$prod_var) |>
      summary() |>
      coef()
  }),
  age= map(data, \(d){
    rma(d$age_est, d$age_var) |>
      summary() |>
      coef()
  })) |>
  select(-data) |>
  unnest(c(comp, prod, age), names_sep="_")

  
  bc_accs_kid_cdi_summ_early <-  bc_accs_kid_cdi |>
    left_join(baseline_lengths) |> filter(t_min< -3000) |> 
  mutate(
    comp_stdev = (comp_upper - comp_lower) / (1.96 * 2),
    comp_var = comp_stdev**2,
    prod_stdev = (prod_upper - prod_lower) / (1.96 * 2),
    prod_var = prod_stdev**2,
    age_stdev = (age_upper - age_lower) / (1.96 * 2),
    age_var = age_stdev**2
  ) |>
  group_by(t_start, t_end, b_start, b_end) |>
  nest() |>
  mutate(comp= map(data, \(d){
    rma(d$comp_est, d$comp_var) |>
      summary() |>
      coef()
  }),
  prod= map(data, \(d){
    rma(d$prod_est, d$prod_var) |>
      summary() |>
      coef()
  }),
  age= map(data, \(d){
    rma(d$age_est, d$age_var) |>
      summary() |>
      coef()
  })) |>
  select(-data) |>
  unnest(c(comp, prod, age), names_sep="_")

no_bc_accs_kid_test_retest <- readRDS(here("cached_intermediates", "3_accs_boot_kid_test_retest.rds")) |> filter(t_start==500, t_end==4000)

bc_accs_kid_test_retest <- readRDS(here("cached_intermediates", "7_bc_accs_boot_kid_test_retest.rds")) |> bind_rows(no_bc_accs_kid_test_retest) |> 
  mutate(b_start=ifelse(is.na(b_start), "none", as.character(b_start)))

library(metafor)

  baseline_lengths <- d_aoi |>
    group_by(dataset_name) |>
    summarise(t_min = min(t_norm))

bc_accs_kid_test_retest_summ <-  bc_accs_kid_test_retest |>
    left_join(baseline_lengths) |> filter(t_min< 0) |> 
  mutate(
    stdev = (upper - lower) / (1.96 * 2),
    var = stdev**2,
  ) |>
  group_by(b_start, b_end) |>
  nest() |>
  mutate(corr= map(data, \(d){
    rma(d$est, d$var) |>
      summary() |>
      coef()
  })) |> 
  unnest(corr)
  

bc_accs_kid_test_retest_summ_early <-  bc_accs_kid_test_retest |>
    left_join(baseline_lengths) |> filter(t_min< -3000) |> 
  mutate(
    stdev = (upper - lower) / (1.96 * 2),
    var = stdev**2,
  ) |>
  group_by(b_start, b_end) |>
  nest() |>
  mutate(corr= map(data, \(d){
    rma(d$est, d$var) |>
      summary() |>
      coef()
  })) |> 
  unnest(corr)


```

Comparison between baseline-corrected accuracy with different baseline windows and accuracy with no baseline-correction. All datsets with data before time 0 were included.

```{r}
bc_acc_all <- bc_boot_summ |> mutate(Type="ICC reliability") |> 
  bind_rows(bc_accs_kid_cdi_summ |> rename(estimate=prod_estimate, ci.lb=prod_ci.lb, ci.ub=prod_ci.ub) |> mutate(Type="Corr. with CDI Prod.")) |> 
  bind_rows(bc_accs_kid_cdi_summ |> rename(estimate=comp_estimate, ci.lb=comp_ci.lb, ci.ub=comp_ci.ub) |> mutate(Type="Corr. with CDI Comp.")) |> 
  bind_rows(bc_accs_kid_test_retest_summ |> mutate(Type="Test-retest reliability")) |> 
  mutate(b_end=ifelse(is.na(b_end), "no baseline", as.character(b_end)),
         b_start=ifelse(b_start=="none", "no baseline", b_start)) 

bc_acc_all |> ggplot(aes(x=b_start, col=b_end, y=estimate, ymin=ci.lb, group=t_end, ymax=ci.ub))+geom_pointrange(position=position_dodge2(width=.5))+coord_flip()+facet_wrap(~Type, scales="free_x", nrow=1)+theme(legend.position = "bottom", )+labs(y="Correlation", x="Baseline start time (ms)", color="Baseline end time (ms)")+scale_color_solarized()

ggsave("bc-all.png")
```

Comparison between baseline-corrected accuracy with different baseline windows and accuracy with no baseline-correction on the subset of datasets with at least 3 seconds of pre-stimulus onset looking data.
```{r}
bc_acc_early <- bc_boot_early_summ |> mutate(Type="ICC reliability") |> 
  bind_rows(bc_accs_kid_cdi_summ_early |> rename(estimate=prod_estimate, ci.lb=prod_ci.lb, ci.ub=prod_ci.ub) |> mutate(Type="Corr. with CDI Production")) |> 
  bind_rows(bc_accs_kid_cdi_summ_early |> rename(estimate=comp_estimate, ci.lb=comp_ci.lb, ci.ub=comp_ci.ub) |> mutate(Type="Corr. with CDI Comp.")) |> 
  mutate(b_end=ifelse(is.na(b_end), "no baseline", as.character(b_end)),
         b_start=ifelse(b_start=="none", "no baseline", b_start)) 

bc_acc_early |> ggplot(aes(x=b_start, col=b_end, y=estimate, ymin=ci.lb, group=t_end, ymax=ci.ub))+geom_pointrange(position=position_dodge2(width=.5))+coord_flip()+facet_wrap(~Type, scales="free_x", nrow=1)+theme(legend.position = "bottom", )+labs(y="Correlation", x="Baseline start time (ms)", color="Baseline end time (ms)")+scale_color_solarized()

ggsave("bc-some.png")

```


## Recommendation 3: Measure reaction times with a minimum reaction time of around 400 ms. 

While for accuracy there are relatively few dimensions of variability, reaction time (RT) can be measured in a number of ways. 

One choice is whether to measure RT based on when a child's eyes launch from the distractor or when a child's eyes land on the target. Launches could be a more precise index of the timing of intent to look at the target, but leaving from the distractor could also be done with intent to look elsewhere. Thus, launch based analyses sometimes exclude trials where it takes a child too long to go from distractor to target (shift-length cutoff).

Short RTs are often excluded because they were necessarily or likely to not be as a response to the stimulus. We explore a range of different potential cutoffs in at attempt to maximize at what point RTs shift from mostly being random to mostly being stimulus driven. 

RTs can be analysed either on a log scale or a raw RT scale.

Our initial exploration suggested that ICC reliability was maximized for log-scale landing-based RT without any shift-length cutoff with minimum RTs around 400-500ms.  

For the main validity and reliability measures, we compare a launch-based RT with a 600ms shift-length cutoff (as used in ...TODO) and a land-based RT without a shift-length cutoff (as used in ...TODO). We vary the minimum RT and whether the RTs are on a log or raw scale. 

Among the 4 types of RT we considered in more detail (log v raw x launch-based v land-based), all have similar correlations with CDI comprehension. Note that correlations between RT and CDI scores are expected to be negative as smaller (faster) RT is associated with better language skills and thus higher CDI scores. Log RT has a slightly stronger (more negative) correlation with CDI production than raw RT especially for minimum RT cutoffs in the 400-450ms range. 

The highest ICC reliabilities occur around 400 ms. ICC reliability is consistently slightly higher for land-based than launch-based although the scale of difference varies with minimum RT. Test-retest reliability is reasonable for all measures. 

We recommend that researchers use minimum RT cutoffs around 400 ms as these increase reliability compared to shorter cutoffs like 200 ms. While the differences between different methods are small, we recommend using landing-based RT on the log scale to maximize overall reliability and validity. 

As an additional note, trials are generally only included in RT analyses if the child is looking at the distractor at the point of disambiguation and continues looking at the distractor for the entire minimum RT period. We found that a more lax threshold of looking at the distractor at time 0ms  and 400ms and not looking at the target in between  can be used with no loss of reliability or validity. However, little bits of missing data were rare, and is likely to be very dataset specific depending on the tracking and imputation process. 

```{r}

# we require 10 trials and 5 kids 
rt_dist <- readRDS(here("cached_intermediates", "4_rts.rds")) |> filter(shift_type=="D-T") |> 
  group_by(time_0, time_end, during, frac, dataset_name, window) |> summarize(trials=n(), admins=administration_id |> unique() |> length()) 
  
icc_cutoff <- rt_dist |> filter(trials>9, admins>4) |> select(dataset_name, time_0, time_end, during, frac, window)


rt_boot <- readRDS(here("cached_intermediates", "4_rt_iccs_boot.rds")) |> inner_join(icc_cutoff) |> 
    mutate(
    type = case_when(
      str_detect(measure, "first_launch_rt") ~ "first_launch",
      str_detect(measure, "last_launch_rt") ~ "last_launch",
      str_detect(measure, "land_rt") ~ "land"
    ),
    logged = case_when(
      str_detect(measure, "log") ~ "log",
      T ~ "raw"
    ),
    trimming = case_when(
      str_detect(measure, "trim_first") ~ "trim_first",
      str_detect(measure, "trim_last") ~ "trim_last",
      T ~ "untrimmed"
    )
  ) |>
  mutate(approach = case_when(
    type == "land" & trimming == "untrimmed" & frac == "0" ~ "generous",
    type == "land" & trimming == "untrimmed" & frac == "1" ~ "trad_land",
    type == "first_launch" & trimming == "trim_first" & frac == "1" ~ "trad_launch"
  )) |>
  filter(!is.na(approach)) |> 
  group_by(dataset_name) |> 
  mutate(options_count=n()) |> 
  filter(dataset_name!="ferguson_eyetrackingr") |> 
  select(-options_count) |> ungroup()

library(metafor)
rt_boot_summ <- rt_boot |>
  filter(!is.na(lower), !is.na(upper)) |>
  mutate(
    stdev = (upper - lower) / (1.96 * 2),
    var = stdev**2
  ) |>
  group_by(window, during, logged, approach) |>
  nest() |>
  mutate(icc = map(data, \(d){
    rma(d$est, d$var) |>
      summary() |>
      coef()
  })) |>
  select(-data) |>
  unnest(icc)

rt_kid_cdi <- readRDS(here("cached_intermediates", "4_rt_kid_cdi.rds")) |> filter(dataset_name!="ferguson_eyetrackingr")

library(metafor)

rt_kid_cdi_summ <-  rt_kid_cdi |>
  mutate(
    comp_stdev = (comp_upper - comp_lower) / (1.96 * 2),
    comp_var = comp_stdev**2,
    prod_stdev = (prod_upper - prod_lower) / (1.96 * 2),
    prod_var = prod_stdev**2,
    age_stdev = (age_upper - age_lower) / (1.96 * 2),
    age_var = age_stdev**2
  ) |>
  group_by(approach, window, logged) |>
  nest() |>
  mutate(comp= map(data, \(d){
    rma(d$comp_est, d$comp_var) |>
      summary() |>
      coef()
  }),
  prod= map(data, \(d){
    rma(d$prod_est, d$prod_var) |>
      summary() |>
      coef()
  }),
  age= map(data, \(d){
    rma(d$age_est, d$age_var) |>
      summary() |>
      coef()
  })) |>
  select(-data) |>
  unnest(c(comp, prod, age), names_sep="_")

rt_kid_test_retest <- readRDS(here("cached_intermediates", "4_rt_kid_test_retest.rds")) 

library(metafor)
rt_kid_test_retest_summ <-  rt_kid_test_retest |>
  mutate(
    stdev = (upper - lower) / (1.96 * 2),
    var = stdev**2,
  ) |>
  group_by(approach, window, logged) |>
  nest() |>
  mutate(corr= map(data, \(d){
    rma(d$est, d$var) |>
      summary() |>
      coef()
  })) |> 
  unnest(corr)
```

Reliability and validity measures for different accuracy windows. Ranges are bootstrapped 95% CIs.
```{r}
rt_all <- rt_boot_summ |> mutate(Type="ICC reliability") |> 
  bind_rows(rt_kid_cdi_summ |> rename(estimate=prod_estimate, ci.lb=prod_ci.lb, ci.ub=prod_ci.ub) |> mutate(Type="Corr. with CDI Production")) |> 
  bind_rows(rt_kid_cdi_summ |> rename(estimate=comp_estimate, ci.lb=comp_ci.lb, ci.ub=comp_ci.ub) |> mutate(Type="Corr. with CDI Comp.")) |> 
  bind_rows(rt_kid_test_retest_summ |> mutate(Type="Test-retest reliability")) |> 
  filter(approach%in%c("trad_land", "trad_launch")) |> 
  mutate(approach=ifelse(approach=="trad_land", "land-based", "launch-based"))

#rt_all |> ggplot(aes(x=str_c(logged, " ", approach, " RT"), col=as.character(window), y=estimate, ymin=ci.lb, ymax=ci.ub))+geom_pointrange(position=position_dodge(.5))+coord_flip()+facet_wrap(~Type, scales="free_x", nrow=1)+theme(legend.position = "bottom", )+labs(y="Correlation", x="Type of RT", color="Minimum RT")+scale_color_viridis(discrete=T)

rt_all |> mutate(type=str_c(logged, " ", approach, " RT") |> factor(levels=c("raw launch-based RT", "log launch-based RT", "raw land-based RT", "log land-based RT"))) |> ggplot(aes(col=type, x=as.character(window), y=estimate, ymin=ci.lb, ymax=ci.ub))+geom_pointrange(position=position_dodge(.5))+coord_flip()+facet_wrap(~Type, scales="free_x", nrow=1)+theme(legend.position = "bottom", )+labs(y="Correlation", x="Minimum RT", color="Type of RT")+scale_color_brewer(palette="Paired")

ggsave("rt.png", width=7, height=5)
```

```{r}
# we require 10 trials and 5 kids 
rt_dist <- readRDS(here("cached_intermediates", "4_rts.rds")) |> filter(shift_type=="D-T") |> 
  group_by(time_0, time_end, during, frac, dataset_name, window) |> summarize(trials=n(), admins=administration_id |> unique() |> length()) 
  
icc_cutoff <- rt_dist |> filter(trials>9, admins>4) |> select(dataset_name, time_0, time_end, during, frac, window)


rt_boot_age <- readRDS(here("cached_intermediates", "4_rt_iccs_age_boot.rds")) |> inner_join(icc_cutoff) |> 
    mutate(
    type = case_when(
      str_detect(measure, "first_launch_rt") ~ "first_launch",
      str_detect(measure, "last_launch_rt") ~ "last_launch",
      str_detect(measure, "land_rt") ~ "land"
    ),
    logged = case_when(
      str_detect(measure, "log") ~ "log",
      T ~ "raw"
    ),
    trimming = case_when(
      str_detect(measure, "trim_first") ~ "trim_first",
      str_detect(measure, "trim_last") ~ "trim_last",
      T ~ "untrimmed"
    )
  ) |>
  mutate(approach = case_when(
    type == "land" & trimming == "untrimmed" & frac == "0" ~ "generous",
    type == "land" & trimming == "untrimmed" & frac == "1" ~ "land-based",
    type == "first_launch" & trimming == "trim_first" & frac == "1" ~ "launch-based"
  )) |>
  filter(!is.na(approach)) |> 
  group_by(dataset_name) |> 
  mutate(options_count=n()) |> 
  filter(dataset_name!="ferguson_eyetrackingr") |> 
  select(-options_count) |> ungroup()

library(metafor)
rt_boot_age_summ <- rt_boot_age |>
  filter(upper!=0) |> 
  filter(!is.na(lower), !is.na(upper)) |>  
  mutate(
    stdev = (upper - lower) / (1.96 * 2),
    var = stdev**2
  ) |>
  group_by(window, logged, approach, younger) |>
  nest() |>
  mutate(icc = map(data, \(d){
    rma(d$est, d$var, method="REML", verbose=F,control=list(stepadj=0.5, maxiter=10000)) |>
      summary() |>
      coef()
  })) |>
  select(-data) |>
  unnest(icc)

rt_boot_age_summ |>mutate(age = ifelse(younger,  "<24 months", ">=24 months")) |> filter(approach!="generous") |>  mutate(type=str_c(logged, " ", approach, " RT") |> factor(levels=c("raw launch-based RT", "log launch-based RT", "raw land-based RT", "log land-based RT"))) |> ggplot(aes(col=type, x=as.character(window), y=estimate, ymin=ci.lb, ymax=ci.ub))+geom_pointrange(position=position_dodge(.5))+coord_flip()+facet_wrap(~age)+theme(legend.position = "bottom", )+labs(y="Correlation", x="Minimum RT", color="Type of RT")+scale_color_brewer(palette="Paired")

rt_boot_age |>mutate(age = ifelse(younger,  "<24 months", ">=24 months")) |> filter(approach!="generous") |>  mutate(type=str_c(logged, " ", approach, " RT") |> factor(levels=c("raw launch-based RT", "log launch-based RT", "raw land-based RT", "log land-based RT"))) |> ggplot(aes(col=type, x=as.character(window), y=est))+stat_summary(position=position_dodge(.5))+coord_flip()+facet_wrap(~age)+theme(legend.position = "bottom", )+labs(y="Correlation", x="Minimum RT", color="Type of RT")+scale_color_brewer(palette="Paired")
```


## Recommendation 4: Excluding trials or children for side-bias or low numbers of trials is generally unnecessary.

Children may engage in various behavior that seems to not be responsive to the trials, including "zoning", contributing few valid looking points on a trial, or contributing few trials overall. One question is whether excluding these types of trials improves overall validity and reliability. 

On a trial-by-trial basis, excluding trials where kids don't look at both target and distractor (either during a pre-window, or ever) does not improve ICC reliability or the correlation with CDI measures. Additionally, excluding trials with looks away from the target and distractor also does not improve ICC reliability or correlation with CDI measures. We do not recommend any trial-level exclusions to accuracy data. 

On a administration basis, children who show a cross-trial side-bias or who did not contribute some minimum number of trials are often excluded. We found that very few children show strong side bias; in this dataset around 1% children who contributed .5% of trials looked to one side more than 90% of the time.

Thus, exclusions for side bias have minimum impact on reliability and we do not recommend them. We also do not find that setting a minimum number of trials for inclusion improves ICC or CDI validity. 

For RTs, exclusions based on number of trials does not improve ICC or correlations with CDI. 

## Recommendation 5: When measuring individual differences, excluding children with fewer than 2 RT measures or 3 accuracy measures may be reasonable. 

[may want to combine this with rec 4]
The one case where exclusions based on minimum trial count may be helpful is for individual difference measures where high test-retest reliability is desired. However, the children who contribute fewer trials might vary in other ways from kids who contribute more so exclusions may hurt generalizability in addition to decreasing effective sample size. 

While our results are uncertain, it appears that test-retest reliability is higher when children have to have 2 RTs from each testing session. Reliability may also increase slightly over the 2-5 RT minimum, but the data loss is substantial. For analysis of archival datasets, tradeoffs between better measurement of each child and fewer children must be considered. 

For accuracy, results were again uncertain, but a minimum of 3 accuracy trials had slightly better test-retest reliability than 2 trials (which in turn was better than 1). Most children had plenty of accuracy trials, so data loss is unlikely to be a concern. 

Effects of different trial-level accuracy exclusions on data validity and reliability. (If we include the graph, bootstrapped error bars can be produced)
```{r}
excl1 <- readRDS(here("cached_intermediates/5_acc_trial_exclusions.rds"))

excl1_loss <- readRDS(here("cached_intermediates/5_acc_trial_exclusions_loss.rds"))


excl1_cdi <- readRDS(here("cached_intermediates/5_acc_trial_exclusion_cdi.rds"))

excl1_test_retest <- readRDS(here("cached_intermediates/5_acc_trial_exclusions_testretest.rds"))

acc_trial_all <- excl1 |> mutate(Type="ICC reliability") |> rename(estimate=icc) |> 
  bind_rows(excl1_loss |> rename(estimate=prop_trials_retained) |> mutate(Type="Frac data retained")) |> 
  bind_rows(excl1_cdi |> rename(estimate=cor_prod) |> mutate(Type="Corr. with CDI Prod.")) |> 
    bind_rows(excl1_cdi |> rename(estimate=cor_comp) |> mutate(Type="Corr. with CDI Comp.")) |> 
    bind_rows(excl1_test_retest |> rename(estimate=cor_test_retest) |> mutate(Type="Test-retest Reliability")) |> 
  mutate(zoners_included=case_when(
    zoners_included=="all" ~ "All included",
    zoners_included=="no pre" ~ "Only if look at both in pre-window",
    zoners_included=="none" ~ "Only if look at both ever"
  ))


  

acc_trial_all |> ggplot(aes(x=exclude_less_than, col=zoners_included, y=estimate, group=zoners_included))+stat_summary(position=position_dodge2(.05), geom="point")+coord_flip()+facet_wrap(~Type, scales="free_x", nrow=1)+theme(legend.position = "bottom", )+labs(y="", x="Minimum Fraction Looking", color="Zoner inclusion")+scale_color_solarized()

ggsave("excl1.png", width=8, height=4)
```
Effects of different minimum trial cut-offs on data validity and reliability. (If we include the graph, bootstrapped error bars can be produced, as could data-loss rates)
```{r}
excl2 <- readRDS(here("cached_intermediates", "5_acc_kid_exclusions_icc.rds"))

excl2_cdi <- readRDS(here("cached_intermediates", "5_acc_kid_exclusions_cdi.rds"))

excl2_test_retest <- readRDS(here("cached_intermediates", "5_acc_kid_exclusions_testretest.rds"))

acc_kid_all <- excl2 |> mutate(Type="ICC reliability") |> rename(estimate=icc_values) |> 
  bind_rows(excl2_cdi |> rename(estimate=cor_prod) |> mutate(Type="Corr. with CDI Production")) |> 
    bind_rows(excl2_cdi |> rename(estimate=cor_comp) |> mutate(Type="Corr. with CDI Comp.")) |> 
    bind_rows(excl2_test_retest |> rename(estimate=cor_test_retest) |> mutate(Type="Test-retest Reliability")) |> 
  mutate(Measure="accuracy")

excl3 <- readRDS(here("cached_intermediates", "5_kid_rt_exclusions.rds"))

excl3_cdi <- readRDS(here("cached_intermediates", "5_kid_rt_exclusions_cdi.rds"))

excl3_test_retest <- readRDS(here("cached_intermediates", "5_kid_rt_exclusions_testretest.rds"))

rt_kid_all <-   excl3 |> mutate(Type="ICC reliability") |> rename(estimate=icc_values) |> 
  bind_rows(excl3_cdi |> rename(estimate=cor_prod) |> mutate(Type="Corr. with CDI Production")) |> 
    bind_rows(excl3_cdi |> rename(estimate=cor_comp) |> mutate(Type="Corr. with CDI Comp.")) |> 
    bind_rows(excl3_test_retest |> rename(estimate=cor_test_retest) |> mutate(Type="Test-retest Reliability")) |> 
  mutate(Measure="RT")

acc_plot <- acc_kid_all |> ggplot(aes(x=min_trials, y=estimate, color=Measure, group=Measure))+stat_summary(position=position_dodge2(.05), geom="point")+coord_flip()+facet_wrap(~Type, scales="free_x", nrow=1)+theme(legend.position = "bottom", )+labs(y="Correlation", x="Minimum Trials", color="")+scale_color_solarized()

rt_plot <- rt_kid_all |> ggplot(aes(x=min_trials, y=estimate, color=Measure, group=Measure))+stat_summary(position=position_dodge2(.05), geom="point")+coord_flip()+facet_wrap(~Type, scales="free_x", nrow=1)+theme(legend.position = "bottom", )+labs(y="Correlation", x="Minimum Trials", color="")+scale_color_solarized(accent="red")

library(cowplot)
plot_grid(acc_plot, rt_plot, rows=2)

ggsave("excl2.png")
```

# Design recommendations 

## Recommendation 6: Design studies to have a minimum of 10 trials per kid. Studies targeting individual differences in processing speed should target 30 trials if possible.

While it is difficult to determine whether to do post-hoc exclusion of children who contributed too few data points, it is much clearer that it is better to design experiments to increase the likelihood of having more valid trials.  

To understand how different numbers of trials would impact test-retest reliability, we sampled trials from each administration with replacement. This allows for up-sampling of children with low numbers of trials, simulating what might have happened in a longer experiment. 

For accuracy, the benefits of increasing numbers of trials seems to flatten out around 10-15 trials. As nearly all trials produce useable accuracy data, this suggests a target minimum of 10-15 trials. 

For RT, it seems much better to have 2 RTs rather than just 1 whenever possible. On average, 1/3 of trials result in useable RT data, but this varies per child, so having 10 trials is recommended to ensure that most children have at least 2 valid RT trials. Test-retest reliability for RT does seem to increase with more trials, with benefits diminishing after 10 trials. Thus, for individual-differences studies where consistent measures of individual language processing speed are desired, we recommend 30 trials, to end up with an average of 10 RT trials per child. 

10 trials should result in multiple RT measures for each child and a fairly reliable measure of accuracy; however, for individual differences in processing speed, 30 trials is recommended where possible. 

(note for discussion: Alvin brings up that up-sampling might be artificially inflating reliability. may want to consider alternatives or make sure results are consistent with downsample only results??)
```{r}
admins <- d_aoi |>
  select(dataset_name, subject_id, administration_id, age) |>
  distinct()
repeated <- admins |>
  group_by(dataset_name, subject_id) |>
  tally() |>
  filter(n > 1)

repeated_subjects <- admins |> inner_join(repeated)

# identify pairs
# hmm, sometimes we have times with 3+ 1 month apart,
# for now allow all the pairs that result so like 14-15 and 15-16 for the same kid

pairs <- repeated_subjects |>
  group_by(dataset_name, subject_id) |>
  mutate(
    forward_age = lead(age),
    forward_diff = forward_age - age,
    test_num = case_when(
      forward_diff < 1.5 ~ 1,
    ),
    mean_age = case_when(
      test_num == 1 ~ (age + forward_age) / 2,
    ),
    second_admin = case_when(
      test_num == 1 ~ lead(administration_id)
    )
  ) |>
  filter(!is.na(test_num)) |>
  rename(first_admin = administration_id) |>
  select(-n, -age) |>
  left_join(repeated_subjects |> select(-age, -n), by = c("dataset_name", "subject_id", "second_admin" = "administration_id")) |>
  ungroup() |>
  mutate(pair_number = row_number()) |>
  filter(!(dataset_name=="adams_marchman_2018" & mean_age>28)) |> # these do have multiple sessions but with very different items banks for the two sessions!
  select(-forward_age, -forward_diff, -test_num)

pairs_long <- pairs |> pivot_longer(c("first_admin", "second_admin"), names_to = "session_num", values_to = "administration_id")


acc <- d_aoi |>
  inner_join(pairs_long |> select(dataset_name, administration_id, subject_id) |> distinct()) |>
  filter(t_norm >= 500) |>
  filter(t_norm <= 4000) |>
  group_by(dataset_name, subject_id, administration_id, trial_id, trial_order, target_label) |>
  summarize(acc = mean(correct, na.rm = T)) |>
  filter(!is.na(acc))

acc_mean <- acc |>
  group_by(dataset_name, subject_id, administration_id) |>
  summarize(acc_trials = sum(!is.na(acc)), acc = mean(acc, na.rm = T))

rt <- readRDS(here("cached_intermediates", "4_rt_canonical.rds")) |>
  inner_join(pairs_long |> select(dataset_name, administration_id, subject_id) |> distinct()) |>
  filter(shift_type == "D-T") |> filter(approach=="trad_launch") |> filter(logged=="raw") |> filter(!is.na(rt))

rt_mean <- rt |>
  group_by(dataset_name, subject_id, administration_id) |>
  summarize(rt_trials = n(), mean_rt = mean(rt))
```

```{r}
all <- pairs_long |>
  left_join(acc_mean) |>
  left_join(rt_mean)

all_wide <- all |>
  mutate(session_num = ifelse(session_num == "first_admin", 1, 2)) |>
  pivot_wider(values_from = c("administration_id", "acc", "acc_trials", "rt_trials", "mean_rt"), names_from = session_num) |>
  mutate(age_bin = case_when(
    mean_age < 18 ~ "12-17",
    mean_age < 21 ~ "18-20",
    mean_age < 28 ~ "21-27",
    mean_age < 33 ~ "28-32",
    mean_age >= 33 ~ "33+"
  ))
```

Simulations of test-retest reliability from down- and up-sampling accuracy trials. 

```{r}

eligible <- all_wide |>
  select(dataset_name, subject_id, mean_age, pair_number, administration_id_1, administration_id_2) |>
  pivot_longer(c("administration_id_1", "administration_id_2"), names_to = "session_num", values_to = "administration_id") |>
  select(dataset_name, subject_id, administration_id)

acc_resample <- expand_grid(num_samples = c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 18, 20, 25, 30), iter=1:10) |>
  mutate(accs = map(num_samples, \(num) {
    acc |>
      group_by(dataset_name, subject_id, administration_id) |>
      inner_join(eligible, by=c("dataset_name", "subject_id", "administration_id")) |>
      group_by(dataset_name, subject_id, administration_id) |>
      slice_sample(n=num,replace=T) |>
      summarize(acc_trials = n(), mean_acc = mean(acc))
  })) |>
  unnest(accs)


acc_corr_resample <- pairs_long |>
  left_join(acc_resample) |>
  mutate(session_num = ifelse(session_num == "first_admin", 1, 2)) |>
  filter(!is.na(num_samples)) |>
  pivot_wider(values_from = c("administration_id", "acc_trials", "mean_acc"), names_from = session_num) |>
  mutate(age_bin = case_when(
    mean_age < 18 ~ "12-17",
    mean_age < 21 ~ "18-20",
    mean_age < 28 ~ "21-27",
    mean_age < 33 ~ "28-32",
    mean_age >= 33 ~ "33+"
  )) |>
  filter(!is.na(mean_acc_1)) |>
  filter(!is.na(mean_acc_2)) |>
  group_by(dataset_name, age_bin, num_samples, iter) |>
  summarise(
    cor_acc = ifelse(sum(!is.na(mean_acc_1) & !is.na(mean_acc_2)) > 2, cor.test(mean_acc_1, mean_acc_2)$estimate, NA)  )

ggplot(acc_corr_resample, aes(x = num_samples, y = cor_acc)) +
  #scale_color_viridis(discrete = T) +
  stat_summary(position = position_dodge(.5)) +
  geom_smooth()+
  stat_summary(position = position_dodge(.5), geom = "line") +
  geom_hline(yintercept = 0)+geom_vline(xintercept=10, lty="dashed")+geom_vline(xintercept=15, lty="dashed")+
  labs(y="Test-retest correlation", x="Simulated trial count (accuracy)")

ggsave("trials-acc.png")
```

Simulations of test-retest reliability from down- and up-sampling RT trials. 

```{r}

eligible <- all_wide |>
  select(dataset_name, subject_id, mean_age, pair_number, administration_id_1, administration_id_2) |>
  pivot_longer(c("administration_id_1", "administration_id_2"), names_to = "session_num", values_to = "administration_id") |>
  select(dataset_name, subject_id, administration_id)

rt_resample <- expand_grid(num_samples = c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15), iter=1:10) |>
  mutate(rts = map(num_samples, \(num) {
    rt |>
      group_by(dataset_name, subject_id, administration_id) |>
      inner_join(eligible, by=c("dataset_name", "subject_id", "administration_id")) |>
      group_by(dataset_name, subject_id, administration_id) |>
      slice_sample(n=num,replace=T) |>
      summarize(rt_trials = n(), mean_rt = mean(rt))
  })) |>
  unnest(rts)


rt_corr_resample <- pairs_long |>
  left_join(rt_resample) |>
  mutate(session_num = ifelse(session_num == "first_admin", 1, 2)) |>
  filter(!is.na(num_samples)) |>
  pivot_wider(values_from = c("administration_id", "rt_trials", "mean_rt"), names_from = session_num) |>
  mutate(age_bin = case_when(
    mean_age < 18 ~ "12-17",
    mean_age < 21 ~ "18-20",
    mean_age < 28 ~ "21-27",
    mean_age < 33 ~ "28-32",
    mean_age >= 33 ~ "33+"
  )) |>
  filter(!is.na(mean_rt_1)) |>
  filter(!is.na(mean_rt_2)) |>
  group_by(dataset_name, age_bin, num_samples, iter) |>
  summarise(
    cor_rt = ifelse(sum(!is.na(mean_rt_1) & !is.na(mean_rt_2)) > 2, cor.test(mean_rt_1, mean_rt_2)$estimate, NA)  )

ggplot(rt_corr_resample, aes(x = num_samples, y = cor_rt)) +
  scale_color_viridis(discrete = T) +
  stat_summary(position = position_dodge(.5)) +
  geom_smooth()+
  stat_summary(position = position_dodge(.5), geom = "line") +
  geom_hline(yintercept = 0)+geom_vline(xintercept=5, lty="dashed")+geom_vline(xintercept=10, lty="dashed")+
    labs(y="Test-retest correlation", x="Simulated trial count (RT)")

ggsave("trials-rt.png")
```


## Recommendation 7: Match target and distractor images based on animacy. 

When designing stimuli and setting up target-distractor pairs, researchers want images that children (in the absense of auditory stimuli) have equivalent preference for. One large driver of child looking preference is a preference to look at animates, that is people (or parts of people) and animals. For mixed-animacy trials, children overall look substantially more to the animate option in the pre-stimulus-response window, and are slower to look to inanimates in response to the stimulus. In contrast, children's response to the stimulus shows a similar trajectory between animates and inanimates in trials where the distractor matches the target in animacy. 

[Not sure if there's anything else we want to say about image saliency measures here]

We found that repeating the same image pairs does not have a substantial effect on looking patterns overall, so it is acceptable to repeat target labels (and target and distractor images) during the experiment. 


Plot of accuracy over time based on target and distractor animacy. Dashed horizontal line indicates chance and dashed vertical lines indicate the point of disambiguation and start of the recommended accuracy window. 

[TODO could bootstrap for CI around the mean at each time point and get error bands on these]
```{r}

readRDS(here("cached_intermediates", "13_animacy_timecourse.rds")) |> 
  ggplot(aes(x = t_norm, y = acc, color = type)) +
  geom_line() +
  geom_hline(yintercept = .5, lty = "dashed") +
  geom_vline(xintercept = 0, lty="dashed") +
      geom_vline(xintercept = 500, lty = "dashed") +
  labs(x="Time point (ms)", y="Fraction looks to target", color="")+
  coord_cartesian(xlim=c(-1000,2000), ylim=c(.4, .8))+
  scale_color_brewer(palette="Dark2")

ggsave("animacy.png")
```
To check for possible differences in behavior when viewing images for a first or repeated time, we compared time courses for items viewed for the first time with items viewed for a second time. All items had the same distractor-target pairings; some datasets had counterbalancing where what was initially the target was later the distractor, and others kept the same distractor. Here we show items for the first time compared to the same items a second time, based on their prior role. 

```{r}
prev_dist <- readRDS(here("cached_intermediates", "13_prev_dist_timecourse.rds")) |> mutate(Panel="prev_dist")
prev_target <- readRDS(here("cached_intermediates", "13_prev_target_time_course.rds")) |> mutate(Panel="prev_target")

prev_dist |> bind_rows(prev_target) |> ggplot(aes(x = t_norm, y = acc, color = type)) +
  geom_line() +
  geom_hline(yintercept = .5, lty = "dashed") +
  geom_vline(xintercept = 0, lty="dashed") +
      geom_vline(xintercept = 500, lty = "dashed") +
  labs(x="Time point (ms)", y="Fraction looks to target", color="")+
  scale_color_brewer(palette="Dark2")+
    coord_cartesian(xlim=c(-1000,2000), ylim=c(.4, .8))+
  facet_wrap(~Panel)

ggsave("order.png")
```

# Paper outline

Introduction:

 * Early language is important.
 * LWL is a good method/measure for early language. (what LWL is)
 * LWL is widely used in various paradigms (both longitudinal indiv-differences & experimental paradigms)
 * Reliability and validity are important, but many early childhood measures have lower-than-desired reliability and validity. 
 * LWL has rich time course data that can be cleaned and analysed in different ways. We want to figure out the best practices to maximize reliability and validity to get the most out of the data. 
 * Two main measures -- accuracy and RT. 
 * We leverage peekbank to get broad coverage of vanilla trials often with corresponding CDI data, and look at 2 types of reliability and 2 types of validity. 
 * Contribution: data-driven recommendations based on peekbank for practices to get the most reliability and validity out of LWL data. 
 
Methods: (where are we submitting this -- is it a methods first paper)

 * dataset description of peekbank & of the subset of data used here
 * explanation of 4 reliability and validity metrics, general search strategy, bootstrapping/errorbars as needed
 
Results:

= recommendations above (header wording suggestions appreciated!)

Discussion:

* summary
* focused on "vanilla" trials, where it's clear what reliability and validity would mean; we don't know how practices would generalize to experimental settings
* limited by datasets which are heterogenous by age/length/items
* often a smooth, wide range of options with similar metrics -- we focus on "best" to inform future practice where one wants to know what is necessary or what (otherwise arbitrary) cutoff to choose, but robust findings will show up robustly over a range of data practices. 

 
 